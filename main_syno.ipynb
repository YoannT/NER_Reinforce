{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.dataloaders.medic import get_raw_medic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:09,614: INFO: train_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_train.conll\n",
      "2021-03-01 15:46:09,618: INFO: Loading Training Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_train.conll\n",
      "2021-03-01 15:46:09,621: INFO: dataset_reader.type = WeakConll2003DatasetReader\n",
      "2021-03-01 15:46:09,624: INFO: dataset_reader.token_indexers.bert.type = bert-pretrained\n",
      "2021-03-01 15:46:09,626: INFO: dataset_reader.token_indexers.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-01 15:46:09,627: INFO: dataset_reader.token_indexers.bert.use_starting_offsets = True\n",
      "2021-03-01 15:46:09,628: INFO: dataset_reader.token_indexers.bert.do_lowercase = False\n",
      "2021-03-01 15:46:09,629: INFO: dataset_reader.token_indexers.bert.never_lowercase = None\n",
      "2021-03-01 15:46:09,631: INFO: dataset_reader.token_indexers.bert.max_pieces = 512\n",
      "2021-03-01 15:46:09,635: INFO: loading vocabulary file ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-01 15:46:09,778: INFO: dataset_reader.tag_label = ner\n",
      "2021-03-01 15:46:09,779: INFO: dataset_reader.feature_labels = ['chunk']\n",
      "2021-03-01 15:46:09,780: INFO: dataset_reader.lazy = False\n",
      "2021-03-01 15:46:09,781: INFO: dataset_reader.coding_scheme = IOB1\n",
      "2021-03-01 15:46:09,782: INFO: dataset_reader.label_indexer.label_namespace = labels\n",
      "2021-03-01 15:46:09,783: INFO: dataset_reader.label_indexer.tags = ['Disease']\n",
      "2021-03-01 15:46:09,784: INFO: dataset_reader.convert_numbers = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:09,789: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_train.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1803it [00:00, 2344.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:10,559: INFO: Length of Training Data: 1803\n",
      "2021-03-01 15:46:10,560: INFO: validation_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_dev.conll\n",
      "2021-03-01 15:46:10,562: INFO: Loading Validation Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:10,565: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319it [00:00, 2970.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:10,673: INFO: Length of Validation Data: 319\n",
      "2021-03-01 15:46:10,674: INFO: test_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_test.conll\n",
      "2021-03-01 15:46:10,675: INFO: Loading Test Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:10,679: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_1class_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [00:00, 1477.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:10,892: INFO: Length of Testing Data: 316\n",
      "2021-03-01 15:46:10,893: INFO: max_vocab_size = -1\n",
      "2021-03-01 15:46:10,894: INFO: Constructing Vocab of size: -1\n",
      "2021-03-01 15:46:10,895: INFO: Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1803/1803 [00:00<00:00, 37386.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:10,958: INFO: Saving vocab to ./trained_models/NCBI-BERT-realFT-PS/run-150/vocab\n",
      "2021-03-01 15:46:10,959: INFO: Vocab Construction Done\n",
      "2021-03-01 15:46:10,961: INFO: Constructing Data Iterators\n",
      "2021-03-01 15:46:10,962: INFO: iterator.type = bucket\n",
      "2021-03-01 15:46:10,963: INFO: iterator.sorting_keys = [['tokens', 'bert']]\n",
      "2021-03-01 15:46:10,964: INFO: iterator.padding_noise = 0.1\n",
      "2021-03-01 15:46:10,965: INFO: iterator.biggest_batch_first = False\n",
      "2021-03-01 15:46:10,966: INFO: iterator.batch_size = 32\n",
      "2021-03-01 15:46:10,967: INFO: iterator.instances_per_epoch = None\n",
      "2021-03-01 15:46:10,967: INFO: iterator.max_instances_in_memory = None\n",
      "2021-03-01 15:46:10,968: INFO: Data Iterators Done\n",
      "2021-03-01 15:46:10,969: INFO: Constructing The model\n",
      "2021-03-01 15:46:10,970: INFO: model.type = MultiClassifier\n",
      "2021-03-01 15:46:10,971: INFO: model.method = binary\n",
      "2021-03-01 15:46:10,972: INFO: model.text_field_embedder.type = basic\n",
      "2021-03-01 15:46:10,973: INFO: model.text_field_embedder.allow_unmatched_keys = True\n",
      "2021-03-01 15:46:10,974: INFO: model.text_field_embedder.token_embedders.bert.type = bert-pretrained\n",
      "2021-03-01 15:46:10,975: INFO: model.text_field_embedder.token_embedders.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased.tar.gz\n",
      "2021-03-01 15:46:10,975: INFO: model.text_field_embedder.token_embedders.bert.requires_grad = [10, 11]\n",
      "2021-03-01 15:46:10,976: INFO: model.text_field_embedder.token_embedders.bert.top_layer_only = False\n",
      "2021-03-01 15:46:10,979: INFO: loading archive file ./Data/embeddings/bert-base-multilingual-cased.tar.gz\n",
      "2021-03-01 15:46:10,981: INFO: extracting archive file ./Data/embeddings/bert-base-multilingual-cased.tar.gz to temp dir /tmp/tmp0rn0iho7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:16,093: INFO: Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2021-03-01 15:46:24,445: INFO: Layer encoder.layer.10.attention.self.query.weight is finetuned\n",
      "2021-03-01 15:46:24,446: INFO: Layer encoder.layer.10.attention.self.query.bias is finetuned\n",
      "2021-03-01 15:46:24,447: INFO: Layer encoder.layer.10.attention.self.key.weight is finetuned\n",
      "2021-03-01 15:46:24,448: INFO: Layer encoder.layer.10.attention.self.key.bias is finetuned\n",
      "2021-03-01 15:46:24,449: INFO: Layer encoder.layer.10.attention.self.value.weight is finetuned\n",
      "2021-03-01 15:46:24,449: INFO: Layer encoder.layer.10.attention.self.value.bias is finetuned\n",
      "2021-03-01 15:46:24,450: INFO: Layer encoder.layer.10.attention.output.dense.weight is finetuned\n",
      "2021-03-01 15:46:24,451: INFO: Layer encoder.layer.10.attention.output.dense.bias is finetuned\n",
      "2021-03-01 15:46:24,452: INFO: Layer encoder.layer.10.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:46:24,453: INFO: Layer encoder.layer.10.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:46:24,454: INFO: Layer encoder.layer.10.intermediate.dense.weight is finetuned\n",
      "2021-03-01 15:46:24,455: INFO: Layer encoder.layer.10.intermediate.dense.bias is finetuned\n",
      "2021-03-01 15:46:24,456: INFO: Layer encoder.layer.10.output.dense.weight is finetuned\n",
      "2021-03-01 15:46:24,457: INFO: Layer encoder.layer.10.output.dense.bias is finetuned\n",
      "2021-03-01 15:46:24,458: INFO: Layer encoder.layer.10.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:46:24,458: INFO: Layer encoder.layer.10.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:46:24,459: INFO: Layer encoder.layer.11.attention.self.query.weight is finetuned\n",
      "2021-03-01 15:46:24,460: INFO: Layer encoder.layer.11.attention.self.query.bias is finetuned\n",
      "2021-03-01 15:46:24,461: INFO: Layer encoder.layer.11.attention.self.key.weight is finetuned\n",
      "2021-03-01 15:46:24,461: INFO: Layer encoder.layer.11.attention.self.key.bias is finetuned\n",
      "2021-03-01 15:46:24,462: INFO: Layer encoder.layer.11.attention.self.value.weight is finetuned\n",
      "2021-03-01 15:46:24,463: INFO: Layer encoder.layer.11.attention.self.value.bias is finetuned\n",
      "2021-03-01 15:46:24,464: INFO: Layer encoder.layer.11.attention.output.dense.weight is finetuned\n",
      "2021-03-01 15:46:24,465: INFO: Layer encoder.layer.11.attention.output.dense.bias is finetuned\n",
      "2021-03-01 15:46:24,465: INFO: Layer encoder.layer.11.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:46:24,466: INFO: Layer encoder.layer.11.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:46:24,467: INFO: Layer encoder.layer.11.intermediate.dense.weight is finetuned\n",
      "2021-03-01 15:46:24,467: INFO: Layer encoder.layer.11.intermediate.dense.bias is finetuned\n",
      "2021-03-01 15:46:24,468: INFO: Layer encoder.layer.11.output.dense.weight is finetuned\n",
      "2021-03-01 15:46:24,468: INFO: Layer encoder.layer.11.output.dense.bias is finetuned\n",
      "2021-03-01 15:46:24,469: INFO: Layer encoder.layer.11.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:46:24,469: INFO: Layer encoder.layer.11.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:46:24,474: INFO: Layer pooler.dense.weight is finetuned\n",
      "2021-03-01 15:46:24,475: INFO: Layer pooler.dense.bias is finetuned\n",
      "2021-03-01 15:46:24,477: INFO: model.encoder_word.type = gru\n",
      "2021-03-01 15:46:24,478: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-01 15:46:24,478: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:46:24,478: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:46:24,479: INFO: model.encoder_word.bidirectional = True\n",
      "2021-03-01 15:46:24,480: INFO: model.encoder_word.dropout = 0.5\n",
      "2021-03-01 15:46:24,481: INFO: model.encoder_word.hidden_size = 150\n",
      "2021-03-01 15:46:24,481: INFO: model.encoder_word.input_size = 768\n",
      "2021-03-01 15:46:24,481: INFO: model.encoder_word.num_layers = 1\n",
      "2021-03-01 15:46:24,482: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-01 15:46:24,492: INFO: model.attention_word.type = KeyedAttention\n",
      "2021-03-01 15:46:24,492: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-01 15:46:24,493: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-01 15:46:24,493: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-01 15:46:24,494: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-01 15:46:24,494: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-01 15:46:24,496: INFO: model.threshold = 0.2\n",
      "2021-03-01 15:46:24,497: INFO: model.initializer = []\n",
      "2021-03-01 15:46:24,497: INFO: model.regularizer = []\n",
      "INIT ClassificationMetrics\n",
      "2021-03-01 15:46:24,498: INFO: Initializing parameters\n",
      "2021-03-01 15:46:24,499: INFO: Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2021-03-01 15:46:24,500: INFO:    attn_Disease.key\n",
      "2021-03-01 15:46:24,501: INFO:    attn_Disease.proj_ctxt.bias\n",
      "2021-03-01 15:46:24,501: INFO:    attn_Disease.proj_ctxt.weight\n",
      "2021-03-01 15:46:24,501: INFO:    attn_Disease.proj_ctxt_key_matrix.bias\n",
      "2021-03-01 15:46:24,502: INFO:    attn_Disease.proj_ctxt_key_matrix.weight\n",
      "2021-03-01 15:46:24,502: INFO:    encoder_word._module.bias_hh_l0\n",
      "2021-03-01 15:46:24,503: INFO:    encoder_word._module.bias_hh_l0_reverse\n",
      "2021-03-01 15:46:24,503: INFO:    encoder_word._module.bias_ih_l0\n",
      "2021-03-01 15:46:24,503: INFO:    encoder_word._module.bias_ih_l0_reverse\n",
      "2021-03-01 15:46:24,504: INFO:    encoder_word._module.weight_hh_l0\n",
      "2021-03-01 15:46:24,504: INFO:    encoder_word._module.weight_hh_l0_reverse\n",
      "2021-03-01 15:46:24,505: INFO:    encoder_word._module.weight_ih_l0\n",
      "2021-03-01 15:46:24,505: INFO:    encoder_word._module.weight_ih_l0_reverse\n",
      "2021-03-01 15:46:24,505: INFO:    logits_layer_Disease.bias\n",
      "2021-03-01 15:46:24,506: INFO:    logits_layer_Disease.weight\n",
      "2021-03-01 15:46:24,506: INFO:    logits_layer_O.bias\n",
      "2021-03-01 15:46:24,507: INFO:    logits_layer_O.weight\n",
      "2021-03-01 15:46:24,507: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.gamma\n",
      "2021-03-01 15:46:24,507: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
      "2021-03-01 15:46:24,508: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
      "2021-03-01 15:46:24,508: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
      "2021-03-01 15:46:24,508: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
      "2021-03-01 15:46:24,509: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
      "2021-03-01 15:46:24,509: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
      "2021-03-01 15:46:24,509: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
      "2021-03-01 15:46:24,510: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
      "2021-03-01 15:46:24,510: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
      "2021-03-01 15:46:24,510: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
      "2021-03-01 15:46:24,511: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
      "2021-03-01 15:46:24,514: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
      "2021-03-01 15:46:24,514: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
      "2021-03-01 15:46:24,514: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
      "2021-03-01 15:46:24,515: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
      "2021-03-01 15:46:24,515: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
      "2021-03-01 15:46:24,515: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
      "2021-03-01 15:46:24,516: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,516: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,516: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,517: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,517: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
      "2021-03-01 15:46:24,517: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
      "2021-03-01 15:46:24,518: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
      "2021-03-01 15:46:24,518: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
      "2021-03-01 15:46:24,520: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
      "2021-03-01 15:46:24,520: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
      "2021-03-01 15:46:24,520: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,521: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,521: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,521: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,522: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
      "2021-03-01 15:46:24,522: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
      "2021-03-01 15:46:24,522: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,523: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,523: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,523: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,524: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
      "2021-03-01 15:46:24,524: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
      "2021-03-01 15:46:24,524: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
      "2021-03-01 15:46:24,525: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
      "2021-03-01 15:46:24,525: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
      "2021-03-01 15:46:24,525: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
      "2021-03-01 15:46:24,526: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,526: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,526: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,527: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,527: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
      "2021-03-01 15:46:24,527: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
      "2021-03-01 15:46:24,528: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,528: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,528: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,529: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,529: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
      "2021-03-01 15:46:24,529: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
      "2021-03-01 15:46:24,530: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
      "2021-03-01 15:46:24,530: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
      "2021-03-01 15:46:24,530: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
      "2021-03-01 15:46:24,531: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
      "2021-03-01 15:46:24,531: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,531: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,532: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,532: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,532: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
      "2021-03-01 15:46:24,533: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
      "2021-03-01 15:46:24,533: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,533: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,534: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,534: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,534: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
      "2021-03-01 15:46:24,535: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
      "2021-03-01 15:46:24,535: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
      "2021-03-01 15:46:24,535: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
      "2021-03-01 15:46:24,536: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
      "2021-03-01 15:46:24,536: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
      "2021-03-01 15:46:24,536: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,537: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,537: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,537: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,538: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
      "2021-03-01 15:46:24,538: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
      "2021-03-01 15:46:24,538: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,539: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,539: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,539: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,539: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
      "2021-03-01 15:46:24,540: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
      "2021-03-01 15:46:24,540: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
      "2021-03-01 15:46:24,540: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
      "2021-03-01 15:46:24,541: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
      "2021-03-01 15:46:24,541: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
      "2021-03-01 15:46:24,541: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,542: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,542: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,542: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,543: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
      "2021-03-01 15:46:24,543: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
      "2021-03-01 15:46:24,543: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,544: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,544: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,544: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,545: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
      "2021-03-01 15:46:24,545: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
      "2021-03-01 15:46:24,545: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
      "2021-03-01 15:46:24,546: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
      "2021-03-01 15:46:24,546: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
      "2021-03-01 15:46:24,546: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
      "2021-03-01 15:46:24,547: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,547: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,547: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,548: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,549: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
      "2021-03-01 15:46:24,549: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
      "2021-03-01 15:46:24,550: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,550: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,550: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,551: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,551: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
      "2021-03-01 15:46:24,551: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
      "2021-03-01 15:46:24,552: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
      "2021-03-01 15:46:24,552: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
      "2021-03-01 15:46:24,552: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
      "2021-03-01 15:46:24,553: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
      "2021-03-01 15:46:24,553: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,553: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,554: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,554: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,554: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
      "2021-03-01 15:46:24,555: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
      "2021-03-01 15:46:24,555: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,555: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,556: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,556: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,556: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
      "2021-03-01 15:46:24,557: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
      "2021-03-01 15:46:24,557: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
      "2021-03-01 15:46:24,557: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
      "2021-03-01 15:46:24,558: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
      "2021-03-01 15:46:24,558: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
      "2021-03-01 15:46:24,558: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,559: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,569: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,569: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,570: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
      "2021-03-01 15:46:24,570: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
      "2021-03-01 15:46:24,570: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,571: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,571: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,571: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,572: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
      "2021-03-01 15:46:24,572: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
      "2021-03-01 15:46:24,572: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
      "2021-03-01 15:46:24,573: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
      "2021-03-01 15:46:24,573: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
      "2021-03-01 15:46:24,573: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
      "2021-03-01 15:46:24,574: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,574: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,574: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,575: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,577: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
      "2021-03-01 15:46:24,577: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
      "2021-03-01 15:46:24,578: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,578: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,578: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,578: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,579: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
      "2021-03-01 15:46:24,580: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
      "2021-03-01 15:46:24,580: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
      "2021-03-01 15:46:24,580: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
      "2021-03-01 15:46:24,581: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
      "2021-03-01 15:46:24,581: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
      "2021-03-01 15:46:24,581: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,582: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,582: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,582: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,583: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
      "2021-03-01 15:46:24,583: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
      "2021-03-01 15:46:24,583: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,584: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,584: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,584: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,585: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
      "2021-03-01 15:46:24,585: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
      "2021-03-01 15:46:24,585: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
      "2021-03-01 15:46:24,586: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
      "2021-03-01 15:46:24,586: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
      "2021-03-01 15:46:24,586: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
      "2021-03-01 15:46:24,587: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,587: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,587: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,588: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,588: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
      "2021-03-01 15:46:24,588: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
      "2021-03-01 15:46:24,589: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,589: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,589: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "2021-03-01 15:46:24,590: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "2021-03-01 15:46:24,590: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
      "2021-03-01 15:46:24,590: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n",
      "2021-03-01 15:46:24,591: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
      "2021-03-01 15:46:24,591: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
      "2021-03-01 15:46:24,591: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
      "2021-03-01 15:46:24,591: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
      "2021-03-01 15:46:24,592: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "2021-03-01 15:46:24,592: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "2021-03-01 15:46:24,592: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2021-03-01 15:46:24,593: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2021-03-01 15:46:24,593: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
      "2021-03-01 15:46:24,598: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
      "2021-03-01 15:46:24,598: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
      "2021-03-01 15:46:24,598: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n",
      "INIT MULTICLASSIFIER\n",
      "2021-03-01 15:46:24,599: INFO: Model Construction done\n",
      "2021-03-01 15:46:24,600: INFO: segmentation.type = SymbolStopwordFilteredMultiPredictions\n",
      "2021-03-01 15:46:24,600: INFO: segmentation.tol = 0.05\n",
      "2021-03-01 15:46:24,600: INFO: segmentation.visualize = True\n",
      "2021-03-01 15:46:24,601: INFO: segmentation.use_probs = True\n",
      "INIT BASE PRED CLASS\n",
      "INIT BASIC MULTI PREDICTIONS\n",
      "INIT SymbolStopwordFilteredMultiPredictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pdb\n",
    "\n",
    "sys.path.insert(0,'/home/ytaille/AttentionSegmentation')\n",
    "\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.iterators import DataIterator\n",
    "# import allennlp.data.dataset_readers as Readers\n",
    "import AttentionSegmentation.reader as Readers\n",
    "\n",
    "# import model as Models\n",
    "import AttentionSegmentation.model.classifiers as Models\n",
    "\n",
    "from AttentionSegmentation.commons.utils import \\\n",
    "    setup_output_dir, read_from_config_file\n",
    "from AttentionSegmentation.commons.model_utils import \\\n",
    "    construct_vocab, load_model_from_existing\n",
    "# from AttentionSegmentation.visualization.visualize_attns import \\\n",
    "#     html_visualizer\n",
    "import AttentionSegmentation.model.attn2labels as SegmentationModels\n",
    "\n",
    "\"\"\"The main entry point\n",
    "\n",
    "This is the main entry point for training HAN SOLO models.\n",
    "\n",
    "Usage::\n",
    "\n",
    "    ${PYTHONPATH} -m AttentionSegmentation/main\n",
    "        --config_file ${CONFIG_FILE}\n",
    "\n",
    "\"\"\"\n",
    "args = type('MyClass', (object,), {'content':{}})()\n",
    "args.config_file = 'Configs/config_ncbi_1class.json'\n",
    "args.log = 'INFO'\n",
    "args.loglevel = 'INFO'\n",
    "args.seed = 1\n",
    "\n",
    "# Setup Experiment Directory\n",
    "config = read_from_config_file(args.config_file)\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if config.get('trainer', None) is not None and \\\n",
    "       config.get('trainer', None).get('cuda_device', -1) > 0:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "serial_dir, config = setup_output_dir(config, args.loglevel)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Training Data\n",
    "TRAIN_PATH = config.pop(\"train_data_path\")\n",
    "logger.info(\"Loading Training Data from {0}\".format(TRAIN_PATH))\n",
    "dataset_reader_params = config.pop(\"dataset_reader\")\n",
    "reader_type = dataset_reader_params.pop(\"type\", None)\n",
    "assert reader_type is not None and hasattr(Readers, reader_type),\\\n",
    "    f\"Cannot find reader {reader_type}\"\n",
    "reader = getattr(Readers, reader_type).from_params(dataset_reader_params)\n",
    "instances_train = reader.read(file_path=TRAIN_PATH)\n",
    "instances_train = instances_train\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Training Data\", len(instances_train)))\n",
    "\n",
    "# Load Validation Data\n",
    "VAL_PATH = config.pop(\"validation_data_path\")\n",
    "logger.info(\"Loading Validation Data from {0}\".format(VAL_PATH))\n",
    "instances_val = reader.read(VAL_PATH)\n",
    "instances_val = instances_val\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Validation Data\", len(instances_val)))\n",
    "\n",
    "# Load Test Data\n",
    "TEST_PATH = config.pop(\"test_data_path\", None)\n",
    "instances_test = None\n",
    "if TEST_PATH is not None:\n",
    "    logger.info(\"Loading Test Data from {0}\".format(TEST_PATH))\n",
    "    instances_test = reader.read(TEST_PATH)\n",
    "    instances_test = instances_test\n",
    "    logger.info(\"Length of {0}: {1}\".format(\n",
    "        \"Testing Data\", len(instances_test)))\n",
    "\n",
    "# # Load Pretrained Existing Model\n",
    "# load_config = config.pop(\"load_from\", None)\n",
    "\n",
    "# # Construct Vocabulary\n",
    "vocab_size = config.pop(\"max_vocab_size\", -1)\n",
    "logger.info(\"Constructing Vocab of size: {0}\".format(vocab_size))\n",
    "vocab_size = None if vocab_size == -1 else vocab_size\n",
    "vocab = Vocabulary.from_instances(instances_train,\n",
    "                                  max_vocab_size=vocab_size)\n",
    "vocab_dir = os.path.join(serial_dir, \"vocab\")\n",
    "assert os.path.exists(vocab_dir), \"Couldn't find the vocab directory\"\n",
    "vocab.save_to_files(vocab_dir)\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # modify the vocab from the source model vocab\n",
    "#     src_vocab_path = load_config.pop(\"vocab_path\", None)\n",
    "#     if src_vocab_path is not None:\n",
    "#         vocab = construct_vocab(src_vocab_path, vocab_dir)\n",
    "#         # Delete the old vocab\n",
    "#         for file in os.listdir(vocab_dir):\n",
    "#             os.remove(os.path.join(vocab_dir, file))\n",
    "#         # save the new vocab\n",
    "#         vocab.save_to_files(vocab_dir)\n",
    "logger.info(\"Saving vocab to {0}\".format(vocab_dir))\n",
    "logger.info(\"Vocab Construction Done\")\n",
    "\n",
    "# # Construct the data iterators\n",
    "logger.info(\"Constructing Data Iterators\")\n",
    "data_iterator = DataIterator.from_params(config.pop(\"iterator\"))\n",
    "data_iterator.index_with(vocab)\n",
    "\n",
    "logger.info(\"Data Iterators Done\")\n",
    "\n",
    "# Create the model\n",
    "logger.info(\"Constructing The model\")\n",
    "model_params = config.pop(\"model\")\n",
    "model_type = model_params.pop(\"type\")\n",
    "assert model_type is not None and hasattr(Models, model_type),\\\n",
    "    f\"Cannot find reader {model_type}\"\n",
    "model = getattr(Models, model_type).from_params(\n",
    "    vocab=vocab,\n",
    "    params=model_params,\n",
    "    label_indexer=reader.get_label_indexer()\n",
    ")\n",
    "logger.info(\"Model Construction done\")\n",
    "\n",
    "# visualize = config.pop(\"visualize\", False)\n",
    "# visualizer = None\n",
    "# if visualize:\n",
    "#     visualizer = html_visualizer(vocab, reader)\n",
    "segmenter_params = config.pop(\"segmentation\")\n",
    "segment_class = segmenter_params.pop(\"type\")\n",
    "segmenter = getattr(SegmentationModels, segment_class).from_params(\n",
    "    vocab=vocab,\n",
    "    reader=reader,\n",
    "    params=segmenter_params\n",
    ")\n",
    "\n",
    "# logger.info(\"Segmenter Done\")\n",
    "\n",
    "# print(\"##################################\\nAYYYYYYYYYYYYYYYYYYYYYYYY\\n\\n\\n\\n\\n\\n\\n\\n###########################\")\n",
    "\n",
    "# exit()\n",
    "\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # Load the weights, as specified by the load_config\n",
    "#     model_path = load_config.pop(\"model_path\", None)\n",
    "#     layers = load_config.pop(\"layers\", None)\n",
    "#     load_config.assert_empty(\"Load Config\")\n",
    "#     assert model_path is not None,\\\n",
    "#         \"You need to specify model path to load from\"\n",
    "#     model = load_model_from_existing(model_path, model, layers)\n",
    "#     logger.info(\"Pretrained weights loaded\")\n",
    "\n",
    "# logger.info(\"Starting the training process\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary to add unknown tag to dictionnary to avoid errors later\n",
    "data_iterator.vocab.add_token_to_namespace(\"@@UNKNOWN@@\", \"chunk_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_from_config_file(args.config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:46:24,887: INFO: PyTorch version 1.5.1 available.\n",
      "2021-03-01 15:46:28,254: INFO: TensorFlow version 2.3.1 available.\n",
      "2021-03-01 15:46:28,744: INFO: Loading faiss with AVX2 support.\n",
      "2021-03-01 15:46:28,747: INFO: Loading faiss.\n",
      "2021-03-01 15:46:29,191: INFO: trainer.patience = 10\n",
      "2021-03-01 15:46:29,192: INFO: trainer.validation_metric = +accuracy\n",
      "2021-03-01 15:46:29,193: INFO: trainer.num_epochs = 50\n",
      "2021-03-01 15:46:29,194: INFO: trainer.cuda_device = 0\n",
      "2021-03-01 15:46:29,195: INFO: trainer.grad_norm = None\n",
      "2021-03-01 15:46:29,196: INFO: trainer.grad_clipping = None\n",
      "2021-03-01 15:46:29,197: INFO: trainer.num_serialized_models_to_keep = 1\n",
      "2021-03-01 15:46:31,784: INFO: trainer.optimizer.type = adam\n",
      "2021-03-01 15:46:31,786: INFO: trainer.optimizer.parameter_groups = [[['.*bert.*'], ConfigTree([('lr', 2e-07)])], [['.*encoder_word.*', '.*attn.*', '.*logit.*'], ConfigTree([('lr', 0.001)])]]\n",
      "2021-03-01 15:46:31,787: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:46:31,788: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:46:31,789: INFO: trainer.optimizer.parameter_groups.list.list.lr = 2e-07\n",
      "2021-03-01 15:46:31,790: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:46:31,790: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:46:31,791: INFO: trainer.optimizer.parameter_groups.list.list.lr = 0.001\n",
      "2021-03-01 15:46:31,795: INFO: Done constructing parameter groups.\n",
      "2021-03-01 15:46:31,795: INFO: Group 0: ['text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6', 'text_field_embedder.token_embedder_bert._scalar_mix.gamma', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias'], {'lr': 2e-07}\n",
      "2021-03-01 15:46:31,796: INFO: Group 1: ['logits_layer_O.bias', 'attn_Disease.key', 'encoder_word._module.bias_hh_l0', 'attn_Disease.proj_ctxt.weight', 'attn_Disease.proj_ctxt_key_matrix.weight', 'logits_layer_O.weight', 'encoder_word._module.weight_hh_l0_reverse', 'encoder_word._module.bias_hh_l0_reverse', 'attn_Disease.proj_ctxt_key_matrix.bias', 'encoder_word._module.bias_ih_l0', 'encoder_word._module.bias_ih_l0_reverse', 'encoder_word._module.weight_ih_l0', 'encoder_word._module.weight_hh_l0', 'logits_layer_Disease.bias', 'attn_Disease.proj_ctxt.bias', 'logits_layer_Disease.weight', 'encoder_word._module.weight_ih_l0_reverse'], {'lr': 0.001}\n",
      "2021-03-01 15:46:31,797: INFO: Group 2: [], {}\n",
      "2021-03-01 15:46:31,797: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:46:31,798: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:46:31,799: INFO: trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
      "2021-03-01 15:46:31,799: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:46:31,800: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:46:31,800: INFO: trainer.learning_rate_scheduler.factor = 0.5\n",
      "2021-03-01 15:46:31,801: INFO: trainer.learning_rate_scheduler.patience = 5\n",
      "2021-03-01 15:46:31,814: INFO: Using cache /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f\n",
      "2021-03-01 15:46:31,816: INFO: Loading /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f/output.pkl... \n",
      "2021-03-01 15:46:50,786: INFO: Quaero mentions: 16283\n",
      "2021-03-01 15:47:05,976: INFO: Using cache /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23\n",
      "2021-03-01 15:47:05,982: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23/history.yaml... \n",
      "2021-03-01 15:47:06,193: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23/checkpoint-15.pt... \n",
      "2021-03-01 15:47:07,148: INFO: Model restored to its best state: 15\n"
     ]
    }
   ],
   "source": [
    "from AttentionSegmentation.trainer import Trainer\n",
    "\n",
    "from nlstruct.utils import  torch_global as tg\n",
    "\n",
    "trainer = Trainer.from_params(\n",
    "    model=model,\n",
    "    base_dir=serial_dir,\n",
    "    iterator=data_iterator,\n",
    "    train_data=instances_train,\n",
    "    validation_data=instances_val,\n",
    "    segmenter=segmenter,\n",
    "    params=config.pop(\"trainer\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.Tensor([[0,1,0,1,0], [0,0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0, 0, 1]]), tensor([[1, 3, 2]]))\n",
      "(tensor([[0, 0, 0, 0, 1, 1]]), tensor([[0, 2, 2, 4, 1, 3]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 1.0000, 0.1000, 1.0000, 0.1000],\n",
       "        [0.0000, 0.1000, 1.0000, 0.1000, 0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_boosted = attn.clone()\n",
    "nnz = (attn>0).nonzero().t().chunk(chunks=2,dim=0)\n",
    "\n",
    "print(nnz)\n",
    "\n",
    "new_nnz = [[], []]\n",
    "\n",
    "for nz0, nz1 in zip(nnz[0][0].numpy(), nnz[1][0].numpy()):\n",
    "    new_nnz[0].extend([nz0,nz0])\n",
    "    new_nnz[1].extend([nz1-1,nz1+1])\n",
    "    \n",
    "new_nnz[0] = torch.Tensor([new_nnz[0]]).long()\n",
    "new_nnz[1] = torch.Tensor([new_nnz[1]]).long()\n",
    "new_nnz = (new_nnz[0], new_nnz[1])\n",
    "print(new_nnz)\n",
    "attn_boosted[new_nnz] += 0.1\n",
    "\n",
    "attn_boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE BIO BERT\n",
    "# TRAIN STEP 1 ONLY ON MEDIC LABELS (+ NCBI MENTIONS)\n",
    "# PREPROCESS / TRAIN / ATTEINDRE BONS SCORES\n",
    "# GET MEDIC ALTERNATIVE LABELS DANS NLSTRUCT -> TRADUIRE LABELS NCBI VERS MEDIC\n",
    "\n",
    "# USE ENTROPY INSTEAD OF CROSS ENTROPY -> not rely on labelled data only (rely on model certainty)\n",
    "\n",
    "# GROUPS : TYPE SEMANTIQUE À LA MENTION (pas utiliser)\n",
    "\n",
    "# NGRAMS FOR ENTITIES -> not possible with discontinued entities\n",
    "\n",
    "# Use \"separation token\" in phrases ?\n",
    "\n",
    "# Use a limited number of attention heads (not one per class)\n",
    "\n",
    "# Use same method as Perceval for trajectories (draw closest ones, reduce list, repeat) -> prédiction itérative\n",
    "\n",
    "# Maybe remove weakly supervised completely?\n",
    "\n",
    "# Test with Reinforce only after a few epochs\n",
    "\n",
    "# Facteur de représentation pour pondérer loss de Perceval ?\n",
    "\n",
    "# Plusieurs facteurs pour constituer la reward\n",
    "\n",
    "# Facteur de similarité mention extraite / synonyme plutôt que similarité mention / label ?\n",
    "\n",
    "# Make sure that every trajectory is different -> draw first then use Perceval\n",
    "\n",
    "# Métrique finale : Est-ce qu'on arrive à choper les CUI ? -> parce que frontières entités dures à déterminer \n",
    "\n",
    "# Use only one class ? -> simpler because all mentions are diseases -> MAKE SURE THAT SEVERAL MENTIONS ARE PREDICTABLE\n",
    "\n",
    "# maybe problem with reinforce comes from hyperparameters??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddddddddddddddddddddddddddddddqsssssssssssssssssxw<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$pppppppppoooooooo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxbffffffffffffffffqeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:12:27,685: INFO: Beginning training.\n",
      "2021-03-01 16:12:27,687: INFO: ==================================================\n",
      "2021-03-01 16:12:27,688: INFO: Starting Training Epoch 1/50\n",
      "2021-03-01 16:12:27,689: INFO: Peak CPU memory usage MB: 6866.448\n",
      "2021-03-01 16:12:27,785: INFO: GPU 0 memory usage MB: 13726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "Disease: 1.0000, accuracy: 1.0000, loss: 0.7777 ||: 100%|██████████| 57/57 [11:44<00:00, 12.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:24:13,217: INFO: Starting with Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Disease: 0.9718, accuracy: 0.9718, loss: 0.3557 ||: 100%|██████████| 10/10 [00:01<00:00,  7.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:24:14,530: INFO: Validation done. (60.0 / 319) zero predicted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:24:22,793: INFO: Best validation performance so far. Copying weights to './trained_models/NCBI-BERT-realFT-PS/run-150/models/best.th'.\n",
      "2021-03-01 16:24:29,893: INFO: Metrics:\n",
      "                        Training accuracy          : 1.000  Validation accuracy          : 0.972\n",
      "                        Training Disease           : 1.000  Validation Disease           : 0.972\n",
      "                        Training loss              : 0.778  Validation loss              : 0.356\n",
      "\n",
      "2021-03-01 16:24:29,894: INFO: Writing validation visualization at ./trained_models/NCBI-BERT-realFT-PS/run-150/visualization/validation.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:24:32,751: INFO: Tag: Disease, Acc: 97.18\n",
      "2021-03-01 16:24:32,753: INFO: Average ACC: 97.18\n",
      "2021-03-01 16:24:32,807: INFO: processed 22501 tokens with 536 phrases; \n",
      "2021-03-01 16:24:32,808: INFO: found: 756 phrases; correct: 303.\n",
      "\n",
      "2021-03-01 16:24:32,809: INFO: accuracy:  95.63%; \n",
      "2021-03-01 16:24:32,810: INFO: precision:  40.08%; recall:  56.53%; FB1:  46.90\n",
      "2021-03-01 16:24:32,811: INFO:               TAG  precision   recall      FB1\n",
      "2021-03-01 16:24:32,812: INFO:           Disease     40.08%   56.53%   46.90%\n",
      "2021-03-01 16:24:32,814: INFO: Writing predictions to ./trained_models/NCBI-BERT-realFT-PS/run-150/predictions.json\n",
      "2021-03-01 16:24:32,816: INFO: Reducing LR: 1.25e-08 -> 1.25e-08\n",
      "2021-03-01 16:24:32,816: INFO: Epoch duration: 00:12:05\n",
      "2021-03-01 16:24:32,820: INFO: Estimated training time remaining: 09:52:11\n",
      "2021-03-01 16:24:32,821: INFO: ==================================================\n",
      "2021-03-01 16:24:32,822: INFO: ==================================================\n",
      "2021-03-01 16:24:32,823: INFO: Starting Training Epoch 2/50\n",
      "2021-03-01 16:24:32,824: INFO: Peak CPU memory usage MB: 6866.448\n",
      "2021-03-01 16:24:32,924: INFO: GPU 0 memory usage MB: 13726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "Disease: 0.9972, accuracy: 0.9972, loss: 0.7945 ||: 100%|██████████| 57/57 [11:43<00:00, 12.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:36:17,156: INFO: Starting with Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Disease: 0.9718, accuracy: 0.9718, loss: 0.3558 ||: 100%|██████████| 10/10 [00:01<00:00,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:36:18,429: INFO: Validation done. (60.0 / 319) zero predicted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:36:26,698: INFO: Best validation performance so far. Copying weights to './trained_models/NCBI-BERT-realFT-PS/run-150/models/best.th'.\n",
      "2021-03-01 16:36:33,792: INFO: Metrics:\n",
      "                        Training accuracy          : 0.997  Validation accuracy          : 0.972\n",
      "                        Training Disease           : 0.997  Validation Disease           : 0.972\n",
      "                        Training loss              : 0.794  Validation loss              : 0.356\n",
      "\n",
      "2021-03-01 16:36:33,794: INFO: Writing validation visualization at ./trained_models/NCBI-BERT-realFT-PS/run-150/visualization/validation.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:36:36,633: INFO: Tag: Disease, Acc: 97.18\n",
      "2021-03-01 16:36:36,635: INFO: Average ACC: 97.18\n",
      "2021-03-01 16:36:36,686: INFO: processed 22501 tokens with 535 phrases; \n",
      "2021-03-01 16:36:36,687: INFO: found: 756 phrases; correct: 303.\n",
      "\n",
      "2021-03-01 16:36:36,687: INFO: accuracy:  95.64%; \n",
      "2021-03-01 16:36:36,688: INFO: precision:  40.08%; recall:  56.64%; FB1:  46.94\n",
      "2021-03-01 16:36:36,688: INFO:               TAG  precision   recall      FB1\n",
      "2021-03-01 16:36:36,689: INFO:           Disease     40.08%   56.64%   46.94%\n",
      "2021-03-01 16:36:36,691: INFO: Writing predictions to ./trained_models/NCBI-BERT-realFT-PS/run-150/predictions.json\n",
      "2021-03-01 16:36:36,692: INFO: Reducing LR: 1.25e-08 -> 1.25e-08\n",
      "2021-03-01 16:36:36,693: INFO: Epoch duration: 00:12:03\n",
      "2021-03-01 16:36:36,693: INFO: Estimated training time remaining: 09:39:36\n",
      "2021-03-01 16:36:36,693: INFO: ==================================================\n",
      "2021-03-01 16:36:36,694: INFO: ==================================================\n",
      "2021-03-01 16:36:36,694: INFO: Starting Training Epoch 3/50\n",
      "2021-03-01 16:36:36,695: INFO: Peak CPU memory usage MB: 6866.448\n",
      "2021-03-01 16:36:36,794: INFO: GPU 0 memory usage MB: 13726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "Disease: 0.9972, accuracy: 0.9972, loss: 0.7935 ||: 100%|██████████| 57/57 [11:51<00:00, 12.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:48:29,581: INFO: Starting with Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Disease: 0.9718, accuracy: 0.9718, loss: 0.3562 ||: 100%|██████████| 10/10 [00:01<00:00,  7.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:48:30,878: INFO: Validation done. (63.0 / 319) zero predicted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:48:39,084: INFO: Best validation performance so far. Copying weights to './trained_models/NCBI-BERT-realFT-PS/run-150/models/best.th'.\n",
      "2021-03-01 16:48:46,104: INFO: Metrics:\n",
      "                        Training accuracy          : 0.997  Validation accuracy          : 0.972\n",
      "                        Training Disease           : 0.997  Validation Disease           : 0.972\n",
      "                        Training loss              : 0.793  Validation loss              : 0.356\n",
      "\n",
      "2021-03-01 16:48:46,106: INFO: Writing validation visualization at ./trained_models/NCBI-BERT-realFT-PS/run-150/visualization/validation.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 16:48:48,936: INFO: Tag: Disease, Acc: 97.18\n",
      "2021-03-01 16:48:48,938: INFO: Average ACC: 97.18\n",
      "2021-03-01 16:48:48,990: INFO: processed 22501 tokens with 532 phrases; \n",
      "2021-03-01 16:48:48,991: INFO: found: 756 phrases; correct: 303.\n",
      "\n",
      "2021-03-01 16:48:48,992: INFO: accuracy:  95.64%; \n",
      "2021-03-01 16:48:48,993: INFO: precision:  40.08%; recall:  56.95%; FB1:  47.05\n",
      "2021-03-01 16:48:48,994: INFO:               TAG  precision   recall      FB1\n",
      "2021-03-01 16:48:48,994: INFO:           Disease     40.08%   56.95%   47.05%\n",
      "2021-03-01 16:48:48,997: INFO: Writing predictions to ./trained_models/NCBI-BERT-realFT-PS/run-150/predictions.json\n",
      "2021-03-01 16:48:48,998: INFO: Reducing LR: 1.25e-08 -> 1.25e-08\n",
      "2021-03-01 16:48:48,998: INFO: Epoch duration: 00:12:12\n",
      "2021-03-01 16:48:48,999: INFO: Estimated training time remaining: 09:29:33\n",
      "2021-03-01 16:48:49,000: INFO: ==================================================\n",
      "2021-03-01 16:48:49,000: INFO: ==================================================\n",
      "2021-03-01 16:48:49,001: INFO: Starting Training Epoch 4/50\n",
      "2021-03-01 16:48:49,002: INFO: Peak CPU memory usage MB: 6866.448\n",
      "2021-03-01 16:48:49,110: INFO: GPU 0 memory usage MB: 13726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "Disease: 0.9977, accuracy: 0.9977, loss: 0.7726 ||: 100%|██████████| 57/57 [11:34<00:00, 12.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 17:00:24,383: INFO: Starting with Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Disease: 0.9718, accuracy: 0.9718, loss: 0.3564 ||: 100%|██████████| 10/10 [00:01<00:00,  7.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 17:00:25,663: INFO: Validation done. (63.0 / 319) zero predicted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 17:00:33,864: INFO: Best validation performance so far. Copying weights to './trained_models/NCBI-BERT-realFT-PS/run-150/models/best.th'.\n",
      "2021-03-01 17:00:40,879: INFO: Metrics:\n",
      "                        Training accuracy          : 0.998  Validation accuracy          : 0.972\n",
      "                        Training Disease           : 0.998  Validation Disease           : 0.972\n",
      "                        Training loss              : 0.773  Validation loss              : 0.356\n",
      "\n",
      "2021-03-01 17:00:40,880: INFO: Writing validation visualization at ./trained_models/NCBI-BERT-realFT-PS/run-150/visualization/validation.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 17:00:43,720: INFO: Tag: Disease, Acc: 97.18\n",
      "2021-03-01 17:00:43,722: INFO: Average ACC: 97.18\n",
      "2021-03-01 17:00:43,776: INFO: processed 22501 tokens with 531 phrases; \n",
      "2021-03-01 17:00:43,777: INFO: found: 756 phrases; correct: 303.\n",
      "\n",
      "2021-03-01 17:00:43,778: INFO: accuracy:  95.63%; \n",
      "2021-03-01 17:00:43,779: INFO: precision:  40.08%; recall:  57.06%; FB1:  47.09\n",
      "2021-03-01 17:00:43,780: INFO:               TAG  precision   recall      FB1\n",
      "2021-03-01 17:00:43,783: INFO:           Disease     40.08%   57.06%   47.09%\n",
      "2021-03-01 17:00:43,785: INFO: Writing predictions to ./trained_models/NCBI-BERT-realFT-PS/run-150/predictions.json\n",
      "2021-03-01 17:00:43,786: INFO: Reducing LR: 1.25e-08 -> 1.25e-08\n",
      "2021-03-01 17:00:43,787: INFO: Epoch duration: 00:11:54\n",
      "2021-03-01 17:00:43,788: INFO: Estimated training time remaining: 09:15:05\n",
      "2021-03-01 17:00:43,788: INFO: ==================================================\n",
      "2021-03-01 17:00:43,789: INFO: ==================================================\n",
      "2021-03-01 17:00:43,789: INFO: Starting Training Epoch 5/50\n",
      "2021-03-01 17:00:43,790: INFO: Peak CPU memory usage MB: 6866.448\n",
      "2021-03-01 17:00:43,891: INFO: GPU 0 memory usage MB: 13726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "  0%|          | 0/57 [00:11<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, continue_training)\u001b[0m\n\u001b[1;32m    586\u001b[0m                         epoch + 1, self._num_epochs)\n\u001b[1;32m    587\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validation_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-9619ef975d32>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    220\u001b[0m                             \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                             \u001b[0mvocabularies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabularies1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                             \u001b[0mbert_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m                         )\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentionSegmentation/notebook_utils.py\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(dataset, bert_name, vocabularies, max_length, apply_unidecode, prepend_labels)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuggingface_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_token_spans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munidecode_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"bert-base-japanese\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \"\"\"\n\u001b[1;32m    191\u001b[0m         config_dict, _ = PretrainedConfig.get_config_dict(\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mALL_PRETRAINED_CONFIG_ARCHIVE_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             )\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# Load config dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         )\n\u001b[1;32m    269\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    372\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m                 \u001b[0metag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ETag\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    540\u001b[0m         }\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    704\u001b[0m                 \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n\u001b[1;32m    708\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0mssl_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         )\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data, tls_in_tls)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msend_sni\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         ssl_sock = _ssl_wrap_socket_impl(\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtls_in_tls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         )\n\u001b[1;32m    431\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36m_ssl_wrap_socket_impl\u001b[0;34m(sock, ssl_context, tls_in_tls, server_hostname)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mssl_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    405\u001b[0m                          \u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m                          \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m                          _context=self, _session=session)\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     def wrap_bio(self, incoming, outgoing, server_side=False,\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, sock, keyfile, certfile, server_side, cert_reqs, ssl_version, ca_certs, do_handshake_on_connect, family, type, proto, fileno, suppress_ragged_eofs, npn_protocols, ciphers, server_hostname, _context, _session)\u001b[0m\n\u001b[1;32m    812\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"\"\"Start the SSL/TLS handshake.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Training Done.\")\n",
    "if instances_test is not None:\n",
    "    logger.info(\"Computing final Test Accuracy\")\n",
    "    trainer.test(instances_test)\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_mult_norm",
   "language": "python",
   "name": "deep_mult_norm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
