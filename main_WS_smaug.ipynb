{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.dataloaders.medic import get_raw_medic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:51,007: INFO: train_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-12 17:23:51,011: INFO: Loading Training Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-12 17:23:51,014: INFO: dataset_reader.type = WeakConll2003DatasetReader\n",
      "2021-03-12 17:23:51,018: INFO: dataset_reader.token_indexers.bert.type = bert-pretrained\n",
      "2021-03-12 17:23:51,019: INFO: dataset_reader.token_indexers.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-12 17:23:51,020: INFO: dataset_reader.token_indexers.bert.use_starting_offsets = True\n",
      "2021-03-12 17:23:51,022: INFO: dataset_reader.token_indexers.bert.do_lowercase = False\n",
      "2021-03-12 17:23:51,023: INFO: dataset_reader.token_indexers.bert.never_lowercase = None\n",
      "2021-03-12 17:23:51,024: INFO: dataset_reader.token_indexers.bert.max_pieces = 512\n",
      "2021-03-12 17:23:51,026: INFO: loading vocabulary file ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-12 17:23:51,160: INFO: dataset_reader.tag_label = ner\n",
      "2021-03-12 17:23:51,161: INFO: dataset_reader.feature_labels = ['chunk']\n",
      "2021-03-12 17:23:51,162: INFO: dataset_reader.lazy = False\n",
      "2021-03-12 17:23:51,163: INFO: dataset_reader.coding_scheme = IOB1\n",
      "2021-03-12 17:23:51,165: INFO: dataset_reader.label_indexer.label_namespace = labels\n",
      "2021-03-12 17:23:51,166: INFO: dataset_reader.label_indexer.tags = ['DiseaseClass', 'SpecificDisease', 'CompositeMention', 'Modifier']\n",
      "2021-03-12 17:23:51,167: INFO: dataset_reader.convert_numbers = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:51,171: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1803it [00:01, 1416.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:52,443: INFO: Length of Training Data: 1803\n",
      "2021-03-12 17:23:52,444: INFO: validation_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n",
      "2021-03-12 17:23:52,446: INFO: Loading Validation Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:52,449: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319it [00:00, 1854.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:52,621: INFO: Length of Validation Data: 319\n",
      "2021-03-12 17:23:52,622: INFO: test_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n",
      "2021-03-12 17:23:52,623: INFO: Loading Test Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:52,627: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [00:00, 1863.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:52,796: INFO: Length of Testing Data: 316\n",
      "2021-03-12 17:23:52,797: INFO: max_vocab_size = -1\n",
      "2021-03-12 17:23:52,798: INFO: Constructing Vocab of size: -1\n",
      "2021-03-12 17:23:52,799: INFO: Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1803/1803 [00:00<00:00, 36822.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:52,860: INFO: Saving vocab to ./trained_models/NCBI-BERT-realFT-PS/run-225/vocab\n",
      "2021-03-12 17:23:52,861: INFO: Vocab Construction Done\n",
      "2021-03-12 17:23:52,862: INFO: Constructing Data Iterators\n",
      "2021-03-12 17:23:52,864: INFO: iterator.type = bucket\n",
      "2021-03-12 17:23:52,865: INFO: iterator.sorting_keys = [['tokens', 'bert']]\n",
      "2021-03-12 17:23:52,866: INFO: iterator.padding_noise = 0.1\n",
      "2021-03-12 17:23:52,867: INFO: iterator.biggest_batch_first = False\n",
      "2021-03-12 17:23:52,868: INFO: iterator.batch_size = 32\n",
      "2021-03-12 17:23:52,869: INFO: iterator.instances_per_epoch = None\n",
      "2021-03-12 17:23:52,870: INFO: iterator.max_instances_in_memory = None\n",
      "2021-03-12 17:23:52,871: INFO: Data Iterators Done\n",
      "2021-03-12 17:23:52,872: INFO: Constructing The model\n",
      "2021-03-12 17:23:52,873: INFO: model.type = MultiClassifier\n",
      "2021-03-12 17:23:52,874: INFO: model.method = binary\n",
      "2021-03-12 17:23:52,876: INFO: model.text_field_embedder.type = basic\n",
      "2021-03-12 17:23:52,877: INFO: model.text_field_embedder.allow_unmatched_keys = True\n",
      "2021-03-12 17:23:52,878: INFO: model.text_field_embedder.token_embedders.bert.type = bert-pretrained\n",
      "2021-03-12 17:23:52,879: INFO: model.text_field_embedder.token_embedders.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased.tar.gz\n",
      "2021-03-12 17:23:52,879: INFO: model.text_field_embedder.token_embedders.bert.requires_grad = [10, 11]\n",
      "2021-03-12 17:23:52,883: INFO: model.text_field_embedder.token_embedders.bert.top_layer_only = False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:23:53,268: INFO: https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz not found in cache, downloading to /tmp/tmp75s0f13b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662804195/662804195 [00:18<00:00, 35400184.83B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:24:12,475: INFO: copying /tmp/tmp75s0f13b to cache at /home/ytaille/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:24:18,807: INFO: creating metadata file for /home/ytaille/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n",
      "2021-03-12 17:24:18,811: INFO: removing temp file /tmp/tmp75s0f13b\n",
      "2021-03-12 17:24:18,859: INFO: loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz from cache at /home/ytaille/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9\n",
      "2021-03-12 17:24:18,861: INFO: extracting archive file /home/ytaille/.pytorch_pretrained_bert/731c19ddf94e294e00ec1ba9a930c69cc2a0fd489b25d3d691373fae4c0986bd.4e367b0d0155d801930846bb6ed98f8a7c23e0ded37888b29caa37009a40c7b9 to temp dir /tmp/tmpn801xqy0\n",
      "2021-03-12 17:24:24,032: INFO: Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2021-03-12 17:24:33,617: INFO: Layer encoder.layer.10.attention.self.query.weight is finetuned\n",
      "2021-03-12 17:24:33,618: INFO: Layer encoder.layer.10.attention.self.query.bias is finetuned\n",
      "2021-03-12 17:24:33,619: INFO: Layer encoder.layer.10.attention.self.key.weight is finetuned\n",
      "2021-03-12 17:24:33,619: INFO: Layer encoder.layer.10.attention.self.key.bias is finetuned\n",
      "2021-03-12 17:24:33,620: INFO: Layer encoder.layer.10.attention.self.value.weight is finetuned\n",
      "2021-03-12 17:24:33,620: INFO: Layer encoder.layer.10.attention.self.value.bias is finetuned\n",
      "2021-03-12 17:24:33,621: INFO: Layer encoder.layer.10.attention.output.dense.weight is finetuned\n",
      "2021-03-12 17:24:33,621: INFO: Layer encoder.layer.10.attention.output.dense.bias is finetuned\n",
      "2021-03-12 17:24:33,622: INFO: Layer encoder.layer.10.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-12 17:24:33,622: INFO: Layer encoder.layer.10.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-12 17:24:33,623: INFO: Layer encoder.layer.10.intermediate.dense.weight is finetuned\n",
      "2021-03-12 17:24:33,623: INFO: Layer encoder.layer.10.intermediate.dense.bias is finetuned\n",
      "2021-03-12 17:24:33,623: INFO: Layer encoder.layer.10.output.dense.weight is finetuned\n",
      "2021-03-12 17:24:33,624: INFO: Layer encoder.layer.10.output.dense.bias is finetuned\n",
      "2021-03-12 17:24:33,624: INFO: Layer encoder.layer.10.output.LayerNorm.weight is finetuned\n",
      "2021-03-12 17:24:33,625: INFO: Layer encoder.layer.10.output.LayerNorm.bias is finetuned\n",
      "2021-03-12 17:24:33,625: INFO: Layer encoder.layer.11.attention.self.query.weight is finetuned\n",
      "2021-03-12 17:24:33,626: INFO: Layer encoder.layer.11.attention.self.query.bias is finetuned\n",
      "2021-03-12 17:24:33,626: INFO: Layer encoder.layer.11.attention.self.key.weight is finetuned\n",
      "2021-03-12 17:24:33,627: INFO: Layer encoder.layer.11.attention.self.key.bias is finetuned\n",
      "2021-03-12 17:24:33,627: INFO: Layer encoder.layer.11.attention.self.value.weight is finetuned\n",
      "2021-03-12 17:24:33,628: INFO: Layer encoder.layer.11.attention.self.value.bias is finetuned\n",
      "2021-03-12 17:24:33,628: INFO: Layer encoder.layer.11.attention.output.dense.weight is finetuned\n",
      "2021-03-12 17:24:33,629: INFO: Layer encoder.layer.11.attention.output.dense.bias is finetuned\n",
      "2021-03-12 17:24:33,631: INFO: Layer encoder.layer.11.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-12 17:24:33,631: INFO: Layer encoder.layer.11.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-12 17:24:33,631: INFO: Layer encoder.layer.11.intermediate.dense.weight is finetuned\n",
      "2021-03-12 17:24:33,632: INFO: Layer encoder.layer.11.intermediate.dense.bias is finetuned\n",
      "2021-03-12 17:24:33,632: INFO: Layer encoder.layer.11.output.dense.weight is finetuned\n",
      "2021-03-12 17:24:33,633: INFO: Layer encoder.layer.11.output.dense.bias is finetuned\n",
      "2021-03-12 17:24:33,633: INFO: Layer encoder.layer.11.output.LayerNorm.weight is finetuned\n",
      "2021-03-12 17:24:33,633: INFO: Layer encoder.layer.11.output.LayerNorm.bias is finetuned\n",
      "2021-03-12 17:24:33,634: INFO: Layer pooler.dense.weight is finetuned\n",
      "2021-03-12 17:24:33,634: INFO: Layer pooler.dense.bias is finetuned\n",
      "2021-03-12 17:24:33,636: INFO: model.encoder_word.type = gru\n",
      "2021-03-12 17:24:33,637: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-12 17:24:33,637: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-12 17:24:33,638: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-12 17:24:33,638: INFO: model.encoder_word.bidirectional = True\n",
      "2021-03-12 17:24:33,639: INFO: model.encoder_word.dropout = 0.5\n",
      "2021-03-12 17:24:33,639: INFO: model.encoder_word.hidden_size = 150\n",
      "2021-03-12 17:24:33,640: INFO: model.encoder_word.input_size = 768\n",
      "2021-03-12 17:24:33,640: INFO: model.encoder_word.num_layers = 1\n",
      "2021-03-12 17:24:33,641: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-12 17:24:33,651: INFO: model.attention_word.type = KeyedAttention\n",
      "2021-03-12 17:24:33,652: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-12 17:24:33,653: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-12 17:24:33,653: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-12 17:24:33,654: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-12 17:24:33,654: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-12 17:24:33,657: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-12 17:24:33,657: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-12 17:24:33,658: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-12 17:24:33,658: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-12 17:24:33,659: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-12 17:24:33,662: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-12 17:24:33,662: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-12 17:24:33,663: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-12 17:24:33,663: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-12 17:24:33,664: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-12 17:24:33,667: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-12 17:24:33,667: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-12 17:24:33,668: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-12 17:24:33,668: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-12 17:24:33,669: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-12 17:24:33,672: INFO: model.threshold = 0.2\n",
      "2021-03-12 17:24:33,672: INFO: model.initializer = []\n",
      "2021-03-12 17:24:33,673: INFO: model.regularizer = []\n",
      "INIT ClassificationMetrics\n",
      "2021-03-12 17:24:33,675: INFO: Initializing parameters\n",
      "2021-03-12 17:24:33,676: INFO: Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2021-03-12 17:24:33,677: INFO:    attn_CompositeMention.key\n",
      "2021-03-12 17:24:33,677: INFO:    attn_CompositeMention.proj_ctxt.bias\n",
      "2021-03-12 17:24:33,678: INFO:    attn_CompositeMention.proj_ctxt.weight\n",
      "2021-03-12 17:24:33,679: INFO:    attn_CompositeMention.proj_ctxt_key_matrix.bias\n",
      "2021-03-12 17:24:33,679: INFO:    attn_CompositeMention.proj_ctxt_key_matrix.weight\n",
      "2021-03-12 17:24:33,679: INFO:    attn_DiseaseClass.key\n",
      "2021-03-12 17:24:33,680: INFO:    attn_DiseaseClass.proj_ctxt.bias\n",
      "2021-03-12 17:24:33,681: INFO:    attn_DiseaseClass.proj_ctxt.weight\n",
      "2021-03-12 17:24:33,681: INFO:    attn_DiseaseClass.proj_ctxt_key_matrix.bias\n",
      "2021-03-12 17:24:33,681: INFO:    attn_DiseaseClass.proj_ctxt_key_matrix.weight\n",
      "2021-03-12 17:24:33,682: INFO:    attn_Modifier.key\n",
      "2021-03-12 17:24:33,682: INFO:    attn_Modifier.proj_ctxt.bias\n",
      "2021-03-12 17:24:33,683: INFO:    attn_Modifier.proj_ctxt.weight\n",
      "2021-03-12 17:24:33,683: INFO:    attn_Modifier.proj_ctxt_key_matrix.bias\n",
      "2021-03-12 17:24:33,684: INFO:    attn_Modifier.proj_ctxt_key_matrix.weight\n",
      "2021-03-12 17:24:33,684: INFO:    attn_SpecificDisease.key\n",
      "2021-03-12 17:24:33,684: INFO:    attn_SpecificDisease.proj_ctxt.bias\n",
      "2021-03-12 17:24:33,685: INFO:    attn_SpecificDisease.proj_ctxt.weight\n",
      "2021-03-12 17:24:33,685: INFO:    attn_SpecificDisease.proj_ctxt_key_matrix.bias\n",
      "2021-03-12 17:24:33,686: INFO:    attn_SpecificDisease.proj_ctxt_key_matrix.weight\n",
      "2021-03-12 17:24:33,686: INFO:    encoder_word._module.bias_hh_l0\n",
      "2021-03-12 17:24:33,687: INFO:    encoder_word._module.bias_hh_l0_reverse\n",
      "2021-03-12 17:24:33,687: INFO:    encoder_word._module.bias_ih_l0\n",
      "2021-03-12 17:24:33,687: INFO:    encoder_word._module.bias_ih_l0_reverse\n",
      "2021-03-12 17:24:33,688: INFO:    encoder_word._module.weight_hh_l0\n",
      "2021-03-12 17:24:33,688: INFO:    encoder_word._module.weight_hh_l0_reverse\n",
      "2021-03-12 17:24:33,689: INFO:    encoder_word._module.weight_ih_l0\n",
      "2021-03-12 17:24:33,689: INFO:    encoder_word._module.weight_ih_l0_reverse\n",
      "2021-03-12 17:24:33,689: INFO:    logits_layer_CompositeMention.bias\n",
      "2021-03-12 17:24:33,690: INFO:    logits_layer_CompositeMention.weight\n",
      "2021-03-12 17:24:33,690: INFO:    logits_layer_DiseaseClass.bias\n",
      "2021-03-12 17:24:33,691: INFO:    logits_layer_DiseaseClass.weight\n",
      "2021-03-12 17:24:33,691: INFO:    logits_layer_Modifier.bias\n",
      "2021-03-12 17:24:33,691: INFO:    logits_layer_Modifier.weight\n",
      "2021-03-12 17:24:33,692: INFO:    logits_layer_O.bias\n",
      "2021-03-12 17:24:33,692: INFO:    logits_layer_O.weight\n",
      "2021-03-12 17:24:33,692: INFO:    logits_layer_SpecificDisease.bias\n",
      "2021-03-12 17:24:33,693: INFO:    logits_layer_SpecificDisease.weight\n",
      "2021-03-12 17:24:33,693: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.gamma\n",
      "2021-03-12 17:24:33,694: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
      "2021-03-12 17:24:33,694: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
      "2021-03-12 17:24:33,694: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
      "2021-03-12 17:24:33,695: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
      "2021-03-12 17:24:33,695: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
      "2021-03-12 17:24:33,695: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
      "2021-03-12 17:24:33,696: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
      "2021-03-12 17:24:33,696: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
      "2021-03-12 17:24:33,697: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
      "2021-03-12 17:24:33,697: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
      "2021-03-12 17:24:33,700: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
      "2021-03-12 17:24:33,701: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
      "2021-03-12 17:24:33,701: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
      "2021-03-12 17:24:33,701: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
      "2021-03-12 17:24:33,702: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
      "2021-03-12 17:24:33,702: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
      "2021-03-12 17:24:33,702: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
      "2021-03-12 17:24:33,703: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,704: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,704: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,705: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,705: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
      "2021-03-12 17:24:33,705: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
      "2021-03-12 17:24:33,706: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
      "2021-03-12 17:24:33,706: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
      "2021-03-12 17:24:33,707: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
      "2021-03-12 17:24:33,707: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
      "2021-03-12 17:24:33,707: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,708: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,708: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,708: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,709: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
      "2021-03-12 17:24:33,709: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
      "2021-03-12 17:24:33,710: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,710: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,710: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,711: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,711: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
      "2021-03-12 17:24:33,711: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
      "2021-03-12 17:24:33,713: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
      "2021-03-12 17:24:33,714: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
      "2021-03-12 17:24:33,714: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
      "2021-03-12 17:24:33,714: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
      "2021-03-12 17:24:33,715: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,715: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,716: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,716: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,717: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
      "2021-03-12 17:24:33,717: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
      "2021-03-12 17:24:33,717: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,718: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,718: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,719: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,719: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
      "2021-03-12 17:24:33,719: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
      "2021-03-12 17:24:33,720: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
      "2021-03-12 17:24:33,720: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
      "2021-03-12 17:24:33,720: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
      "2021-03-12 17:24:33,721: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
      "2021-03-12 17:24:33,721: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,722: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,722: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,722: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,723: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
      "2021-03-12 17:24:33,723: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
      "2021-03-12 17:24:33,723: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,724: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,724: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,726: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,727: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
      "2021-03-12 17:24:33,727: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
      "2021-03-12 17:24:33,727: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
      "2021-03-12 17:24:33,728: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
      "2021-03-12 17:24:33,728: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
      "2021-03-12 17:24:33,728: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
      "2021-03-12 17:24:33,729: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,729: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,730: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,730: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,730: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
      "2021-03-12 17:24:33,731: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
      "2021-03-12 17:24:33,731: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,731: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,732: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,732: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,733: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
      "2021-03-12 17:24:33,733: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
      "2021-03-12 17:24:33,733: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
      "2021-03-12 17:24:33,734: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
      "2021-03-12 17:24:33,734: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
      "2021-03-12 17:24:33,734: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
      "2021-03-12 17:24:33,735: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,735: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,737: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,738: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,738: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
      "2021-03-12 17:24:33,738: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
      "2021-03-12 17:24:33,739: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,739: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,740: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,740: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,740: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
      "2021-03-12 17:24:33,741: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
      "2021-03-12 17:24:33,741: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
      "2021-03-12 17:24:33,741: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
      "2021-03-12 17:24:33,742: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
      "2021-03-12 17:24:33,742: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
      "2021-03-12 17:24:33,743: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,743: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,743: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,744: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,744: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
      "2021-03-12 17:24:33,744: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
      "2021-03-12 17:24:33,745: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,745: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,747: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,747: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,748: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
      "2021-03-12 17:24:33,749: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
      "2021-03-12 17:24:33,750: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
      "2021-03-12 17:24:33,751: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
      "2021-03-12 17:24:33,752: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
      "2021-03-12 17:24:33,752: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
      "2021-03-12 17:24:33,753: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,754: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,755: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,756: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,757: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
      "2021-03-12 17:24:33,757: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
      "2021-03-12 17:24:33,758: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,758: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,759: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,759: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,760: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
      "2021-03-12 17:24:33,760: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
      "2021-03-12 17:24:33,761: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
      "2021-03-12 17:24:33,761: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
      "2021-03-12 17:24:33,762: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
      "2021-03-12 17:24:33,762: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
      "2021-03-12 17:24:33,763: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,763: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,764: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,764: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,765: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
      "2021-03-12 17:24:33,765: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
      "2021-03-12 17:24:33,765: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,766: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,766: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,767: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,767: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
      "2021-03-12 17:24:33,768: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
      "2021-03-12 17:24:33,768: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
      "2021-03-12 17:24:33,769: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
      "2021-03-12 17:24:33,769: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
      "2021-03-12 17:24:33,769: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
      "2021-03-12 17:24:33,770: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,770: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,770: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,771: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,771: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
      "2021-03-12 17:24:33,771: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
      "2021-03-12 17:24:33,772: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,772: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,772: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,773: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,773: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
      "2021-03-12 17:24:33,774: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
      "2021-03-12 17:24:33,774: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
      "2021-03-12 17:24:33,774: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
      "2021-03-12 17:24:33,775: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
      "2021-03-12 17:24:33,775: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
      "2021-03-12 17:24:33,775: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,776: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,776: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,776: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,777: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
      "2021-03-12 17:24:33,777: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
      "2021-03-12 17:24:33,778: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,782: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,783: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,783: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,783: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
      "2021-03-12 17:24:33,784: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
      "2021-03-12 17:24:33,784: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
      "2021-03-12 17:24:33,784: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
      "2021-03-12 17:24:33,785: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
      "2021-03-12 17:24:33,785: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
      "2021-03-12 17:24:33,785: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,786: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,786: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,786: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,787: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
      "2021-03-12 17:24:33,787: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
      "2021-03-12 17:24:33,787: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,788: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,788: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "2021-03-12 17:24:33,788: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "2021-03-12 17:24:33,789: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
      "2021-03-12 17:24:33,789: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n",
      "2021-03-12 17:24:33,789: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
      "2021-03-12 17:24:33,790: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
      "2021-03-12 17:24:33,790: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
      "2021-03-12 17:24:33,790: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
      "2021-03-12 17:24:33,791: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "2021-03-12 17:24:33,791: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "2021-03-12 17:24:33,791: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2021-03-12 17:24:33,792: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2021-03-12 17:24:33,792: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
      "2021-03-12 17:24:33,792: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
      "2021-03-12 17:24:33,793: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
      "2021-03-12 17:24:33,793: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n",
      "INIT MULTICLASSIFIER\n",
      "2021-03-12 17:24:33,794: INFO: Model Construction done\n",
      "2021-03-12 17:24:33,794: INFO: segmentation.type = SymbolStopwordFilteredMultiPredictions\n",
      "2021-03-12 17:24:33,795: INFO: segmentation.tol = 0.05\n",
      "2021-03-12 17:24:33,795: INFO: segmentation.visualize = True\n",
      "2021-03-12 17:24:33,796: INFO: segmentation.use_probs = True\n",
      "INIT BASE PRED CLASS\n",
      "INIT BASIC MULTI PREDICTIONS\n",
      "INIT SymbolStopwordFilteredMultiPredictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pdb\n",
    "\n",
    "sys.path.insert(0,'/home/ytaille/AttentionSegmentation')\n",
    "\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.iterators import DataIterator\n",
    "# import allennlp.data.dataset_readers as Readers\n",
    "import AttentionSegmentation.reader as Readers\n",
    "\n",
    "# import model as Models\n",
    "import AttentionSegmentation.model.classifiers as Models\n",
    "\n",
    "from AttentionSegmentation.commons.utils import \\\n",
    "    setup_output_dir, read_from_config_file\n",
    "from AttentionSegmentation.commons.model_utils import \\\n",
    "    construct_vocab, load_model_from_existing\n",
    "# from AttentionSegmentation.visualization.visualize_attns import \\\n",
    "#     html_visualizer\n",
    "import AttentionSegmentation.model.attn2labels as SegmentationModels\n",
    "\n",
    "\"\"\"The main entry point\n",
    "\n",
    "This is the main entry point for training HAN SOLO models.\n",
    "\n",
    "Usage::\n",
    "\n",
    "    ${PYTHONPATH} -m AttentionSegmentation/main\n",
    "        --config_file ${CONFIG_FILE}\n",
    "\n",
    "\"\"\"\n",
    "args = type('MyClass', (object,), {'content':{}})()\n",
    "args.config_file = 'Configs/config_ncbi.json'\n",
    "args.log = 'INFO'\n",
    "args.loglevel = 'INFO'\n",
    "args.seed = 1\n",
    "\n",
    "# Setup Experiment Directory\n",
    "config = read_from_config_file(args.config_file)\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if config.get('trainer', None) is not None and \\\n",
    "       config.get('trainer', None).get('cuda_device', -1) > 0:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "serial_dir, config = setup_output_dir(config, args.loglevel)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Training Data\n",
    "TRAIN_PATH = config.pop(\"train_data_path\")\n",
    "logger.info(\"Loading Training Data from {0}\".format(TRAIN_PATH))\n",
    "dataset_reader_params = config.pop(\"dataset_reader\")\n",
    "reader_type = dataset_reader_params.pop(\"type\", None)\n",
    "assert reader_type is not None and hasattr(Readers, reader_type),\\\n",
    "    f\"Cannot find reader {reader_type}\"\n",
    "reader = getattr(Readers, reader_type).from_params(dataset_reader_params)\n",
    "instances_train = reader.read(file_path=TRAIN_PATH)\n",
    "instances_train = instances_train\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Training Data\", len(instances_train)))\n",
    "\n",
    "# Load Validation Data\n",
    "VAL_PATH = config.pop(\"validation_data_path\")\n",
    "logger.info(\"Loading Validation Data from {0}\".format(VAL_PATH))\n",
    "instances_val = reader.read(VAL_PATH)\n",
    "instances_val = instances_val\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Validation Data\", len(instances_val)))\n",
    "\n",
    "# Load Test Data\n",
    "TEST_PATH = config.pop(\"test_data_path\", None)\n",
    "instances_test = None\n",
    "if TEST_PATH is not None:\n",
    "    logger.info(\"Loading Test Data from {0}\".format(TEST_PATH))\n",
    "    instances_test = reader.read(TEST_PATH)\n",
    "    instances_test = instances_test\n",
    "    logger.info(\"Length of {0}: {1}\".format(\n",
    "        \"Testing Data\", len(instances_test)))\n",
    "\n",
    "# # Load Pretrained Existing Model\n",
    "# load_config = config.pop(\"load_from\", None)\n",
    "\n",
    "# # Construct Vocabulary\n",
    "vocab_size = config.pop(\"max_vocab_size\", -1)\n",
    "logger.info(\"Constructing Vocab of size: {0}\".format(vocab_size))\n",
    "vocab_size = None if vocab_size == -1 else vocab_size\n",
    "vocab = Vocabulary.from_instances(instances_train,\n",
    "                                  max_vocab_size=vocab_size)\n",
    "vocab_dir = os.path.join(serial_dir, \"vocab\")\n",
    "assert os.path.exists(vocab_dir), \"Couldn't find the vocab directory\"\n",
    "vocab.save_to_files(vocab_dir)\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # modify the vocab from the source model vocab\n",
    "#     src_vocab_path = load_config.pop(\"vocab_path\", None)\n",
    "#     if src_vocab_path is not None:\n",
    "#         vocab = construct_vocab(src_vocab_path, vocab_dir)\n",
    "#         # Delete the old vocab\n",
    "#         for file in os.listdir(vocab_dir):\n",
    "#             os.remove(os.path.join(vocab_dir, file))\n",
    "#         # save the new vocab\n",
    "#         vocab.save_to_files(vocab_dir)\n",
    "logger.info(\"Saving vocab to {0}\".format(vocab_dir))\n",
    "logger.info(\"Vocab Construction Done\")\n",
    "\n",
    "# # Construct the data iterators\n",
    "logger.info(\"Constructing Data Iterators\")\n",
    "data_iterator = DataIterator.from_params(config.pop(\"iterator\"))\n",
    "data_iterator.index_with(vocab)\n",
    "\n",
    "logger.info(\"Data Iterators Done\")\n",
    "\n",
    "# Create the model\n",
    "logger.info(\"Constructing The model\")\n",
    "model_params = config.pop(\"model\")\n",
    "model_type = model_params.pop(\"type\")\n",
    "assert model_type is not None and hasattr(Models, model_type),\\\n",
    "    f\"Cannot find reader {model_type}\"\n",
    "model = getattr(Models, model_type).from_params(\n",
    "    vocab=vocab,\n",
    "    params=model_params,\n",
    "    label_indexer=reader.get_label_indexer()\n",
    ")\n",
    "logger.info(\"Model Construction done\")\n",
    "\n",
    "# visualize = config.pop(\"visualize\", False)\n",
    "# visualizer = None\n",
    "# if visualize:\n",
    "#     visualizer = html_visualizer(vocab, reader)\n",
    "segmenter_params = config.pop(\"segmentation\")\n",
    "segment_class = segmenter_params.pop(\"type\")\n",
    "segmenter = getattr(SegmentationModels, segment_class).from_params(\n",
    "    vocab=vocab,\n",
    "    reader=reader,\n",
    "    params=segmenter_params\n",
    ")\n",
    "\n",
    "# logger.info(\"Segmenter Done\")\n",
    "\n",
    "# print(\"##################################\\nAYYYYYYYYYYYYYYYYYYYYYYYY\\n\\n\\n\\n\\n\\n\\n\\n###########################\")\n",
    "\n",
    "# exit()\n",
    "\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # Load the weights, as specified by the load_config\n",
    "#     model_path = load_config.pop(\"model_path\", None)\n",
    "#     layers = load_config.pop(\"layers\", None)\n",
    "#     load_config.assert_empty(\"Load Config\")\n",
    "#     assert model_path is not None,\\\n",
    "#         \"You need to specify model path to load from\"\n",
    "#     model = load_model_from_existing(model_path, model, layers)\n",
    "#     logger.info(\"Pretrained weights loaded\")\n",
    "\n",
    "# logger.info(\"Starting the training process\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary to add unknown tag to dictionnary to avoid errors later\n",
    "data_iterator.vocab.add_token_to_namespace(\"@@UNKNOWN@@\", \"chunk_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_from_config_file(args.config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:24:34,581: INFO: trainer.patience = 10\n",
      "2021-03-12 17:24:34,582: INFO: trainer.validation_metric = +accuracy\n",
      "2021-03-12 17:24:34,584: INFO: trainer.num_epochs = 50\n",
      "2021-03-12 17:24:34,585: INFO: trainer.cuda_device = 0\n",
      "2021-03-12 17:24:34,586: INFO: trainer.grad_norm = None\n",
      "2021-03-12 17:24:34,586: INFO: trainer.grad_clipping = None\n",
      "2021-03-12 17:24:34,588: INFO: trainer.num_serialized_models_to_keep = 1\n",
      "2021-03-12 17:24:37,055: INFO: trainer.optimizer.type = adam\n",
      "2021-03-12 17:24:37,056: INFO: trainer.optimizer.parameter_groups = [[['.*bert.*'], ConfigTree([('lr', 2e-07)])], [['.*encoder_word.*', '.*attn.*', '.*logit.*'], ConfigTree([('lr', 0.001)])]]\n",
      "2021-03-12 17:24:37,057: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-12 17:24:37,058: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-12 17:24:37,059: INFO: trainer.optimizer.parameter_groups.list.list.lr = 2e-07\n",
      "2021-03-12 17:24:37,060: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-12 17:24:37,061: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-12 17:24:37,063: INFO: trainer.optimizer.parameter_groups.list.list.lr = 0.001\n",
      "2021-03-12 17:24:37,067: INFO: Done constructing parameter groups.\n",
      "2021-03-12 17:24:37,068: INFO: Group 0: ['text_field_embedder.token_embedder_bert._scalar_mix.gamma', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7'], {'lr': 2e-07}\n",
      "2021-03-12 17:24:37,068: INFO: Group 1: ['attn_DiseaseClass.key', 'encoder_word._module.bias_ih_l0', 'encoder_word._module.weight_hh_l0_reverse', 'attn_CompositeMention.proj_ctxt.weight', 'encoder_word._module.bias_ih_l0_reverse', 'logits_layer_O.weight', 'encoder_word._module.bias_hh_l0', 'logits_layer_Modifier.weight', 'attn_SpecificDisease.proj_ctxt.bias', 'logits_layer_SpecificDisease.weight', 'logits_layer_DiseaseClass.bias', 'attn_CompositeMention.proj_ctxt.bias', 'attn_DiseaseClass.proj_ctxt.bias', 'logits_layer_Modifier.bias', 'encoder_word._module.weight_hh_l0', 'attn_CompositeMention.proj_ctxt_key_matrix.weight', 'logits_layer_SpecificDisease.bias', 'attn_Modifier.proj_ctxt_key_matrix.weight', 'attn_Modifier.proj_ctxt.bias', 'attn_DiseaseClass.proj_ctxt.weight', 'attn_SpecificDisease.proj_ctxt_key_matrix.bias', 'attn_Modifier.proj_ctxt.weight', 'encoder_word._module.bias_hh_l0_reverse', 'logits_layer_DiseaseClass.weight', 'encoder_word._module.weight_ih_l0_reverse', 'logits_layer_CompositeMention.bias', 'attn_DiseaseClass.proj_ctxt_key_matrix.bias', 'logits_layer_O.bias', 'attn_Modifier.key', 'attn_CompositeMention.proj_ctxt_key_matrix.bias', 'attn_Modifier.proj_ctxt_key_matrix.bias', 'attn_DiseaseClass.proj_ctxt_key_matrix.weight', 'attn_SpecificDisease.key', 'attn_CompositeMention.key', 'attn_SpecificDisease.proj_ctxt_key_matrix.weight', 'attn_SpecificDisease.proj_ctxt.weight', 'encoder_word._module.weight_ih_l0', 'logits_layer_CompositeMention.weight'], {'lr': 0.001}\n",
      "2021-03-12 17:24:37,069: INFO: Group 2: [], {}\n",
      "2021-03-12 17:24:37,070: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-12 17:24:37,070: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-12 17:24:37,071: INFO: trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
      "2021-03-12 17:24:37,071: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-12 17:24:37,072: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-12 17:24:37,072: INFO: trainer.learning_rate_scheduler.factor = 0.5\n",
      "2021-03-12 17:24:37,073: INFO: trainer.learning_rate_scheduler.patience = 5\n",
      "2021-03-12 17:24:37,079: INFO: Using cache /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f\n",
      "2021-03-12 17:24:37,081: INFO: Loading /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f/output.pkl... \n",
      "2021-03-12 17:24:55,083: INFO: Quaero mentions: 16283\n",
      "2021-03-12 17:25:11,846: INFO: Using cache /home/ytaille/data/cache/norm/paper/train_step1/415526e277077a3a\n",
      "2021-03-12 17:25:11,852: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/415526e277077a3a/history.yaml... \n",
      "2021-03-12 17:25:12,049: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/415526e277077a3a/checkpoint-15.pt... \n",
      "2021-03-12 17:25:12,950: INFO: Model restored to its best state: 15\n"
     ]
    }
   ],
   "source": [
    "from AttentionSegmentation.trainer import Trainer\n",
    "\n",
    "from nlstruct.utils import  torch_global as tg\n",
    "\n",
    "trainer = Trainer.from_params(\n",
    "    model=model,\n",
    "    base_dir=serial_dir,\n",
    "    iterator=data_iterator,\n",
    "    train_data=instances_train,\n",
    "    validation_data=instances_val,\n",
    "    segmenter=segmenter,\n",
    "    params=config.pop(\"trainer\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIT FOR BOOSTING SURROUNDING ATTENTIONS\n",
    "\n",
    "# attn = torch.Tensor([[0,1,0,1,0], [0,0,1,0,0]])\n",
    "\n",
    "# attn_boosted = attn.clone()\n",
    "# nnz = (attn>0).nonzero().t().chunk(chunks=2,dim=0)\n",
    "\n",
    "# print(nnz)\n",
    "\n",
    "# new_nnz = [[], []]\n",
    "\n",
    "# for nz0, nz1 in zip(nnz[0][0].numpy(), nnz[1][0].numpy()):\n",
    "#     new_nnz[0].extend([nz0,nz0])\n",
    "#     new_nnz[1].extend([nz1-1,nz1+1])\n",
    "    \n",
    "# new_nnz[0] = torch.Tensor([new_nnz[0]]).long()\n",
    "# new_nnz[1] = torch.Tensor([new_nnz[1]]).long()\n",
    "# new_nnz = (new_nnz[0], new_nnz[1])\n",
    "# print(new_nnz)\n",
    "# attn_boosted[new_nnz] += 0.1\n",
    "\n",
    "# attn_boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE BIO BERT\n",
    "# TRAIN STEP 1 ONLY ON MEDIC LABELS (+ NCBI MENTIONS)\n",
    "# PREPROCESS / TRAIN / ATTEINDRE BONS SCORES\n",
    "# GET MEDIC ALTERNATIVE LABELS DANS NLSTRUCT -> TRADUIRE LABELS NCBI VERS MEDIC\n",
    "\n",
    "# USE ENTROPY INSTEAD OF CROSS ENTROPY -> not rely on labelled data only (rely on model certainty)\n",
    "\n",
    "# GROUPS : TYPE SEMANTIQUE À LA MENTION (pas utiliser)\n",
    "\n",
    "# NGRAMS FOR ENTITIES -> not possible with discontinued entities\n",
    "\n",
    "# Use \"separation token\" in phrases ?\n",
    "\n",
    "# Use a limited number of attention heads (not one per class)\n",
    "\n",
    "# Use same method as Perceval for trajectories (draw closest ones, reduce list, repeat) -> prédiction itérative\n",
    "\n",
    "# Maybe remove weakly supervised completely?\n",
    "\n",
    "# Test with Reinforce only after a few epochs\n",
    "\n",
    "# Facteur de représentation pour pondérer loss de Perceval ?\n",
    "\n",
    "# Plusieurs facteurs pour constituer la reward\n",
    "\n",
    "# Facteur de similarité mention extraite / synonyme plutôt que similarité mention / label ?\n",
    "\n",
    "# Make sure that every trajectory is different -> draw first then use Perceval\n",
    "\n",
    "# Métrique finale : Est-ce qu'on arrive à choper les CUI ? -> parce que frontières entités dures à déterminer \n",
    "\n",
    "# Use only one class ? -> simpler because all mentions are diseases -> MAKE SURE THAT SEVERAL MENTIONS ARE PREDICTABLE\n",
    "\n",
    "# maybe problem with reinforce algo comes from hyperparameters?? -> USE OPTIMIZER PARAMETER SPECIFICATION\n",
    "\n",
    "# change objective: instead of WL use RL metrics -> measure on CUI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PERCEVAL WAY OF PREDICTING:\n",
    "# Entrée: Embeddings tokens + embeddings labels \n",
    "# Pour un CUI prédit: récupérer loss Perceval, comparer avec CUI le plus proche?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_labels,\n",
    "                 hidden_dim,\n",
    "                 dropout,\n",
    "                 n_tokens=None,\n",
    "                 token_dim=None,\n",
    "                 embeddings=None,\n",
    "                 tag_scheme=\"bio\",\n",
    "                 metric='linear',\n",
    "                 metric_fc_kwargs=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if embeddings is not None:\n",
    "            self.embeddings = embeddings\n",
    "            if n_tokens is None or token_dim is None:\n",
    "                if hasattr(embeddings, 'weight'):\n",
    "                    n_tokens, token_dim = embeddings.weight.shape\n",
    "                else:\n",
    "                    n_tokens, token_dim = embeddings.embeddings.weight.shape\n",
    "        else:\n",
    "            self.embeddings = torch.nn.Embedding(n_tokens, token_dim) if n_tokens > 0 else None\n",
    "        assert token_dim is not None, \"Provide token_dim or embeddings\"\n",
    "        assert self.embeddings is not None\n",
    "\n",
    "        dim = (token_dim if n_tokens > 0 else 0)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        if tag_scheme == \"bio\":\n",
    "            self.crf = BIODecoder(n_labels)\n",
    "        elif tag_scheme == \"bioul\":\n",
    "            self.crf = BIOULDecoder(n_labels)\n",
    "        else:\n",
    "            raise Exception()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "        self.linear = torch.nn.Linear(dim, hidden_dim)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        n_tags = self.crf.num_tags\n",
    "        metric_fc_kwargs = metric_fc_kwargs if metric_fc_kwargs is not None else {}\n",
    "        if metric == \"linear\":\n",
    "            self.metric_fc = torch.nn.Linear(dim, n_tags)\n",
    "        elif metric == \"cosine\":\n",
    "            self.metric_fc = CosineSimilarity(dim, n_tags, rescale=rescale, **metric_fc_kwargs)\n",
    "        elif metric == \"ema_cosine\":\n",
    "            self.metric_fc = EMACosineSimilarity(dim, n_tags, rescale=rescale, **metric_fc_kwargs)\n",
    "        else:\n",
    "            raise Exception()\n",
    "    \n",
    "    def extended_embeddings(self, tokens, mask, **kwargs):\n",
    "        # Default case here, size <= 512\n",
    "        # Small ugly check to see if self.embeddings is Bert-like, then we need to pass a mask\n",
    "        if hasattr(self.embeddings, 'encoder') or hasattr(self.embeddings, 'transformer'):\n",
    "            return self.embeddings(tokens, mask, **kwargs)[0]\n",
    "        else:\n",
    "            return self.embeddings(tokens)\n",
    "\n",
    "    def forward(self, tokens, mask, tag_embeds=None, return_embeddings=False):\n",
    "        # Embed the tokens\n",
    "        scores = None\n",
    "        # shape: n_batch * sequence * 768\n",
    "        embeds = self.extended_embeddings(tokens, mask, custom_embeds=tag_embeds)\n",
    "        state = embeds.masked_fill(~mask.unsqueeze(-1), 0)\n",
    "        state = torch.relu(self.linear(self.dropout(state)))# + state\n",
    "        state = self.batch_norm(state.view(-1, state.shape[-1])).view(state.shape)\n",
    "        scores = self.metric_fc(state)\n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"embeddings\": embeds if return_embeddings else None,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import traceback\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, List, Tuple, Union, Iterable, Any, Set\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.nn.parallel import replicate, parallel_apply\n",
    "from torch.nn.parallel.scatter_gather import scatter_kwargs, gather\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from itertools import tee\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import peak_memory_mb, gpu_memory_mb\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.learning_rate_schedulers import LearningRateScheduler\n",
    "from allennlp.training.optimizers import Optimizer\n",
    "\n",
    "from AttentionSegmentation.commons.trainer_utils import is_sparse,\\\n",
    "    sparse_clip_norm, move_optimizer_to_cuda, TensorboardWriter\n",
    "# from AttentionSegmentation.visualization.visualize_attns \\\n",
    "#     import html_visualizer\n",
    "from AttentionSegmentation.model.attn2labels import BasePredictionClass\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TQDM_COLUMNS = 200\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/ytaille/deep_multilingual_normalization')\n",
    "from create_classifiers import create_classifiers\n",
    "from nlstruct.dataloaders import load_from_brat\n",
    "\n",
    "logger2 = logging.getLogger(\"nlstruct\")\n",
    "logger2.setLevel(logging.ERROR)\n",
    "\n",
    "from notebook_utils import *\n",
    "\n",
    "def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_memory_mb()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            for gpu, memory in gpu_memory_mb().items():\n",
    "                logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        from allennlp.data.fields.array_field import ArrayField\n",
    "\n",
    "        for i, td in enumerate(self._train_data):\n",
    "            td.fields['sample_id'] = ArrayField(np.array([i]))\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self._iterator(self._train_data,\n",
    "                                         num_epochs=1,\n",
    "                                         cuda_device=self._iterator_device,\n",
    "                                         shuffle=True,\n",
    "                                         )\n",
    "\n",
    "        train_generator, cp_generator, id_generator = tee(train_generator, 3)\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        for ig in id_generator:\n",
    "            ids.extend([int(sid.item()) for sid in ig['sample_id']])\n",
    "\n",
    "        shuffled_train_data = [self._train_data[i] for i in ids]\n",
    "\n",
    "#         train_predictions = self._segmenter.get_predictions(\n",
    "#                     instances=shuffled_train_data,\n",
    "#                     iterator = cp_generator,\n",
    "#                     model=self._model,\n",
    "#                     cuda_device=self._iterator_device,\n",
    "#                     verbose=True)\n",
    "\n",
    "\n",
    "        num_training_batches = self._iterator.get_num_batches(self._train_data)\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches\n",
    "                                         )\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        cpt_batch = 0\n",
    "\n",
    "        # Set the model to \"train\" mode.\n",
    "        self._model.train()\n",
    "\n",
    "        for batch in train_generator_tqdm:\n",
    "            \n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "            batch_len = len(batch['labels'])\n",
    "\n",
    "            # FOR train_predictions:\n",
    "            # pred/gold is sentence level\n",
    "            # pred_labels/gold_labels is word level\n",
    "\n",
    "\n",
    "            # FOR batch:\n",
    "            # labels is sentence level\n",
    "            # tags is word level\n",
    "\n",
    "            # print(train_texts)\n",
    "            # print(\"SENTENCE LEVEL\")\n",
    "            # print([tp['gold'] for tp in train_predictions[:10]])\n",
    "            # print(batch['labels'][:10])\n",
    "\n",
    "            # print(\"WORD LEVEL\")\n",
    "            # print([tp['gold_labels'] for tp in train_predictions[:2]])\n",
    "            # print(batch['tags'][:2])\n",
    "\n",
    "            # exit()\n",
    "            \n",
    "            if epoch <= -1:\n",
    "                trajectory_scores =  [0]\n",
    "            else:\n",
    "                output_dict = self._model(**batch)\n",
    "#                 attns_single = output_dict['attentions']\n",
    "                attns = output_dict['attentions']\n",
    "                \n",
    "                # Policy is \"attention mask\": attention scores should be higher if we want to predict CUI\n",
    "                # Only take words with attention above threshold when predicting with deep norm -> see if it's enough (reward indicates that)\n",
    "                # REINFORCE algo: (also known as Monte Carlo PG)\n",
    "                # - draw N trajectories (N attention paths?) -> discretise attentions to make them 1 / 0? -> see if it works with bernoulli first\n",
    "                # - evaluate each trajectory then sum (maybe add baseline -> subtract mean of all trajectories rewards)\n",
    "                # - Expected return is given by sum(prob(Ti | W) * reward(Ti)) -> see again if it works with bernoulli first\n",
    "                # W are WeakL weights \n",
    "                # - Gradient ascent of return / gradient descent of negative return\n",
    "\n",
    "                # Set horizon ? -> number / proportion of attention at 1 per batch\n",
    "                # Set number of trajectories ? -> maybe make trajectories number vary based on sentence length\n",
    "                # gamma = 0.9 ? -> used to simulate temporal importance of reward (multiply each step by a certain power of gamma, furthest rewards are less impactful) -> may not be possible to model here\n",
    "                \n",
    "                real_tokens = [np.array(b.fields['tokens'].tokens) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "#                     gold_labels = [np.array(b.fields['tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "                gold_norm_labels = [np.array(b.fields['chunk_tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "\n",
    "                save_to_ann(real_tokens, gold_norm_labels, '/home/ytaille/data/tmp/ws_inputs/')\n",
    "\n",
    "                # NLSTRUCT PART\n",
    "\n",
    "                bert_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "                dataset = load_from_brat(\"/home/ytaille/data/tmp/ws_inputs/\")\n",
    "\n",
    "                if len(dataset['mentions']) == 0:\n",
    "                    continue\n",
    "\n",
    "                dataset['mentions']['mention_id'] = dataset['mentions']['doc_id'] +'.'+ dataset['mentions']['mention_id'].astype(str)\n",
    "\n",
    "                batcher, vocs, mention_ids = preprocess_train(\n",
    "                    dataset,\n",
    "                    vocabularies=self.vocabularies1,\n",
    "                    bert_name=bert_name,\n",
    "                )\n",
    "\n",
    "                batch_size = len(batcher)\n",
    "                with_tqdm = True\n",
    "\n",
    "                tg.set_device('cuda:0') \n",
    "                device = tg.device\n",
    "\n",
    "                pred_batcher = predict(batcher, self.classifier1, batch_size=64)\n",
    "                \n",
    "                scores = compute_scores(pred_batcher, batcher)\n",
    "                \n",
    "                cpt_batch += batch_len\n",
    "\n",
    "#                 if any(len(tj) > 0 for tj in trajectory_scores.values()):\n",
    "#                     trajectory_scores = [t for tj in trajectory_scores.values() for t in tj]\n",
    "#                 else: policy_loss = 0\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            loss = (scores['loss'] * attns).mean() # policy_loss self._batch_loss(batch, for_training=True) + \n",
    "            loss.backward()\n",
    "\n",
    "            # Make sure Variable is on the cpu before converting to numpy.\n",
    "            # .cpu() is a no-op if you aren't using GPUs.\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            batch_grad_norm = self._rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "            if hasattr(self, \"_tf_params\") and self._tf_params is not None:\n",
    "                # We have TF logging\n",
    "                if self._batch_num_total % self._tf_params[\"log_every\"] == 0:\n",
    "                    self._tf_log(metrics, self._batch_num_total)\n",
    "\n",
    "        return self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "    \n",
    "import functools\n",
    "\n",
    "trainer._train_epoch = functools.partial(_train_epoch, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADD CUSTOM BERT IN allennlp/modules.token_embedders.bert_token_embedder.py -> BertEmbedder or PretrainedBertEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:25:13,994: INFO: Beginning training.\n",
      "2021-03-12 17:25:13,996: INFO: ==================================================\n",
      "2021-03-12 17:25:13,996: INFO: Starting Training Epoch 1/50\n",
      "2021-03-12 17:25:13,997: INFO: Peak CPU memory usage MB: 6201.82\n",
      "2021-03-12 17:25:14,093: INFO: GPU 0 memory usage MB: 5746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "CompositeMention: 0.0525, DiseaseClass: 0.2445, Modifier: 0.4071, SpecificDisease: 0.6364, accuracy: 0.3351, loss: 0.0036 ||:  98%|█████████▊| 56/57 [01:18<00:01,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-12 17:26:33,880: ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-3435b262f1ae>\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/trainer.py\", line 61, in train\n",
      "    super(Trainer, self).train(*args, **kwargs)\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\", line 588, in train\n",
      "    train_metrics = self._train_epoch(epoch)\n",
      "  File \"<ipython-input-11-9003421f0f47>\", line 176, in _train_epoch\n",
      "    bert_name=bert_name,\n",
      "  File \"/home/ytaille/AttentionSegmentation/notebook_utils.py\", line 488, in preprocess_train\n",
      "    mentions[\"text\"] = mentions[\"text\"].str.lower()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/posixpath.py\", line 395, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "2021-03-12 17:26:33,956: INFO: \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, continue_training)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9003421f0f47>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0mvocabularies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabularies1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m                     \u001b[0mbert_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m                 )\n",
      "\u001b[0;32m~/AttentionSegmentation/notebook_utils.py\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(dataset, bert_name, vocabularies, max_length, apply_unidecode, prepend_labels)\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mmentions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mentions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mention_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmentions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"mention_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2044\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2046\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2048\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2050\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1437\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1337\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1338\u001b[0m             )\n\u001b[1;32m   1339\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1194\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/yt_nlp/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Training Done.\")\n",
    "if instances_test is not None:\n",
    "    logger.info(\"Computing final Test Accuracy\")\n",
    "    trainer.test(instances_test)\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_nlp",
   "language": "python",
   "name": "yt_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
