{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.dataloaders.medic import get_raw_medic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:24,205: INFO: train_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-01 15:17:24,207: INFO: Loading Training Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-01 15:17:24,210: INFO: dataset_reader.type = WeakConll2003DatasetReader\n",
      "2021-03-01 15:17:24,213: INFO: dataset_reader.token_indexers.bert.type = bert-pretrained\n",
      "2021-03-01 15:17:24,214: INFO: dataset_reader.token_indexers.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-01 15:17:24,215: INFO: dataset_reader.token_indexers.bert.use_starting_offsets = True\n",
      "2021-03-01 15:17:24,216: INFO: dataset_reader.token_indexers.bert.do_lowercase = False\n",
      "2021-03-01 15:17:24,217: INFO: dataset_reader.token_indexers.bert.never_lowercase = None\n",
      "2021-03-01 15:17:24,218: INFO: dataset_reader.token_indexers.bert.max_pieces = 512\n",
      "2021-03-01 15:17:24,220: INFO: loading vocabulary file ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-01 15:17:24,355: INFO: dataset_reader.tag_label = ner\n",
      "2021-03-01 15:17:24,356: INFO: dataset_reader.feature_labels = ['chunk']\n",
      "2021-03-01 15:17:24,357: INFO: dataset_reader.lazy = False\n",
      "2021-03-01 15:17:24,358: INFO: dataset_reader.coding_scheme = IOB1\n",
      "2021-03-01 15:17:24,359: INFO: dataset_reader.label_indexer.label_namespace = labels\n",
      "2021-03-01 15:17:24,360: INFO: dataset_reader.label_indexer.tags = ['DiseaseClass', 'SpecificDisease', 'CompositeMention', 'Modifier']\n",
      "2021-03-01 15:17:24,361: INFO: dataset_reader.convert_numbers = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:24,367: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1803it [00:01, 1736.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:25,403: INFO: Length of Training Data: 1803\n",
      "2021-03-01 15:17:25,404: INFO: validation_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n",
      "2021-03-01 15:17:25,405: INFO: Loading Validation Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:25,408: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319it [00:00, 2051.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:25,564: INFO: Length of Validation Data: 319\n",
      "2021-03-01 15:17:25,565: INFO: test_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n",
      "2021-03-01 15:17:25,566: INFO: Loading Test Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:25,570: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [00:00, 1194.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:25,835: INFO: Length of Testing Data: 316\n",
      "2021-03-01 15:17:25,836: INFO: max_vocab_size = -1\n",
      "2021-03-01 15:17:25,837: INFO: Constructing Vocab of size: -1\n",
      "2021-03-01 15:17:25,838: INFO: Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1803/1803 [00:00<00:00, 37367.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:25,901: INFO: Saving vocab to ./trained_models/NCBI-BERT-realFT-PS/run-148/vocab\n",
      "2021-03-01 15:17:25,903: INFO: Vocab Construction Done\n",
      "2021-03-01 15:17:25,904: INFO: Constructing Data Iterators\n",
      "2021-03-01 15:17:25,906: INFO: iterator.type = bucket\n",
      "2021-03-01 15:17:25,907: INFO: iterator.sorting_keys = [['tokens', 'bert']]\n",
      "2021-03-01 15:17:25,909: INFO: iterator.padding_noise = 0.1\n",
      "2021-03-01 15:17:25,910: INFO: iterator.biggest_batch_first = False\n",
      "2021-03-01 15:17:25,911: INFO: iterator.batch_size = 32\n",
      "2021-03-01 15:17:25,912: INFO: iterator.instances_per_epoch = None\n",
      "2021-03-01 15:17:25,912: INFO: iterator.max_instances_in_memory = None\n",
      "2021-03-01 15:17:25,913: INFO: Data Iterators Done\n",
      "2021-03-01 15:17:25,914: INFO: Constructing The model\n",
      "2021-03-01 15:17:25,919: INFO: model.type = MultiClassifier\n",
      "2021-03-01 15:17:25,921: INFO: model.method = binary\n",
      "2021-03-01 15:17:25,922: INFO: model.text_field_embedder.type = basic\n",
      "2021-03-01 15:17:25,923: INFO: model.text_field_embedder.allow_unmatched_keys = True\n",
      "2021-03-01 15:17:25,924: INFO: model.text_field_embedder.token_embedders.bert.type = bert-pretrained\n",
      "2021-03-01 15:17:25,924: INFO: model.text_field_embedder.token_embedders.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased.tar.gz\n",
      "2021-03-01 15:17:25,925: INFO: model.text_field_embedder.token_embedders.bert.requires_grad = [10, 11]\n",
      "2021-03-01 15:17:25,926: INFO: model.text_field_embedder.token_embedders.bert.top_layer_only = False\n",
      "2021-03-01 15:17:25,927: INFO: loading archive file ./Data/embeddings/bert-base-multilingual-cased.tar.gz\n",
      "2021-03-01 15:17:25,929: INFO: extracting archive file ./Data/embeddings/bert-base-multilingual-cased.tar.gz to temp dir /tmp/tmp85zbjy79\n",
      "2021-03-01 15:17:31,002: INFO: Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2021-03-01 15:17:39,595: INFO: Layer encoder.layer.10.attention.self.query.weight is finetuned\n",
      "2021-03-01 15:17:39,598: INFO: Layer encoder.layer.10.attention.self.query.bias is finetuned\n",
      "2021-03-01 15:17:39,599: INFO: Layer encoder.layer.10.attention.self.key.weight is finetuned\n",
      "2021-03-01 15:17:39,600: INFO: Layer encoder.layer.10.attention.self.key.bias is finetuned\n",
      "2021-03-01 15:17:39,601: INFO: Layer encoder.layer.10.attention.self.value.weight is finetuned\n",
      "2021-03-01 15:17:39,602: INFO: Layer encoder.layer.10.attention.self.value.bias is finetuned\n",
      "2021-03-01 15:17:39,603: INFO: Layer encoder.layer.10.attention.output.dense.weight is finetuned\n",
      "2021-03-01 15:17:39,603: INFO: Layer encoder.layer.10.attention.output.dense.bias is finetuned\n",
      "2021-03-01 15:17:39,604: INFO: Layer encoder.layer.10.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:17:39,605: INFO: Layer encoder.layer.10.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:17:39,605: INFO: Layer encoder.layer.10.intermediate.dense.weight is finetuned\n",
      "2021-03-01 15:17:39,606: INFO: Layer encoder.layer.10.intermediate.dense.bias is finetuned\n",
      "2021-03-01 15:17:39,606: INFO: Layer encoder.layer.10.output.dense.weight is finetuned\n",
      "2021-03-01 15:17:39,607: INFO: Layer encoder.layer.10.output.dense.bias is finetuned\n",
      "2021-03-01 15:17:39,607: INFO: Layer encoder.layer.10.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:17:39,608: INFO: Layer encoder.layer.10.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:17:39,609: INFO: Layer encoder.layer.11.attention.self.query.weight is finetuned\n",
      "2021-03-01 15:17:39,609: INFO: Layer encoder.layer.11.attention.self.query.bias is finetuned\n",
      "2021-03-01 15:17:39,610: INFO: Layer encoder.layer.11.attention.self.key.weight is finetuned\n",
      "2021-03-01 15:17:39,610: INFO: Layer encoder.layer.11.attention.self.key.bias is finetuned\n",
      "2021-03-01 15:17:39,611: INFO: Layer encoder.layer.11.attention.self.value.weight is finetuned\n",
      "2021-03-01 15:17:39,611: INFO: Layer encoder.layer.11.attention.self.value.bias is finetuned\n",
      "2021-03-01 15:17:39,612: INFO: Layer encoder.layer.11.attention.output.dense.weight is finetuned\n",
      "2021-03-01 15:17:39,612: INFO: Layer encoder.layer.11.attention.output.dense.bias is finetuned\n",
      "2021-03-01 15:17:39,613: INFO: Layer encoder.layer.11.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:17:39,613: INFO: Layer encoder.layer.11.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:17:39,614: INFO: Layer encoder.layer.11.intermediate.dense.weight is finetuned\n",
      "2021-03-01 15:17:39,614: INFO: Layer encoder.layer.11.intermediate.dense.bias is finetuned\n",
      "2021-03-01 15:17:39,615: INFO: Layer encoder.layer.11.output.dense.weight is finetuned\n",
      "2021-03-01 15:17:39,615: INFO: Layer encoder.layer.11.output.dense.bias is finetuned\n",
      "2021-03-01 15:17:39,615: INFO: Layer encoder.layer.11.output.LayerNorm.weight is finetuned\n",
      "2021-03-01 15:17:39,616: INFO: Layer encoder.layer.11.output.LayerNorm.bias is finetuned\n",
      "2021-03-01 15:17:39,616: INFO: Layer pooler.dense.weight is finetuned\n",
      "2021-03-01 15:17:39,616: INFO: Layer pooler.dense.bias is finetuned\n",
      "2021-03-01 15:17:39,618: INFO: model.encoder_word.type = gru\n",
      "2021-03-01 15:17:39,618: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-01 15:17:39,622: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:17:39,623: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:17:39,623: INFO: model.encoder_word.bidirectional = True\n",
      "2021-03-01 15:17:39,624: INFO: model.encoder_word.dropout = 0.5\n",
      "2021-03-01 15:17:39,624: INFO: model.encoder_word.hidden_size = 150\n",
      "2021-03-01 15:17:39,624: INFO: model.encoder_word.input_size = 768\n",
      "2021-03-01 15:17:39,625: INFO: model.encoder_word.num_layers = 1\n",
      "2021-03-01 15:17:39,625: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-01 15:17:39,634: INFO: model.attention_word.type = KeyedAttention\n",
      "2021-03-01 15:17:39,635: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-01 15:17:39,635: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-01 15:17:39,636: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-01 15:17:39,637: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-01 15:17:39,637: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-01 15:17:39,639: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-01 15:17:39,640: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-01 15:17:39,640: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-01 15:17:39,641: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-01 15:17:39,642: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-01 15:17:39,643: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-01 15:17:39,644: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-01 15:17:39,644: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-01 15:17:39,645: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-01 15:17:39,645: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-01 15:17:39,647: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-01 15:17:39,647: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-01 15:17:39,648: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-01 15:17:39,648: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-01 15:17:39,649: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-01 15:17:39,650: INFO: model.threshold = 0.2\n",
      "2021-03-01 15:17:39,651: INFO: model.initializer = []\n",
      "2021-03-01 15:17:39,651: INFO: model.regularizer = []\n",
      "INIT ClassificationMetrics\n",
      "2021-03-01 15:17:39,653: INFO: Initializing parameters\n",
      "2021-03-01 15:17:39,654: INFO: Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2021-03-01 15:17:39,654: INFO:    attn_CompositeMention.key\n",
      "2021-03-01 15:17:39,654: INFO:    attn_CompositeMention.proj_ctxt.bias\n",
      "2021-03-01 15:17:39,655: INFO:    attn_CompositeMention.proj_ctxt.weight\n",
      "2021-03-01 15:17:39,655: INFO:    attn_CompositeMention.proj_ctxt_key_matrix.bias\n",
      "2021-03-01 15:17:39,656: INFO:    attn_CompositeMention.proj_ctxt_key_matrix.weight\n",
      "2021-03-01 15:17:39,656: INFO:    attn_DiseaseClass.key\n",
      "2021-03-01 15:17:39,656: INFO:    attn_DiseaseClass.proj_ctxt.bias\n",
      "2021-03-01 15:17:39,657: INFO:    attn_DiseaseClass.proj_ctxt.weight\n",
      "2021-03-01 15:17:39,657: INFO:    attn_DiseaseClass.proj_ctxt_key_matrix.bias\n",
      "2021-03-01 15:17:39,657: INFO:    attn_DiseaseClass.proj_ctxt_key_matrix.weight\n",
      "2021-03-01 15:17:39,657: INFO:    attn_Modifier.key\n",
      "2021-03-01 15:17:39,658: INFO:    attn_Modifier.proj_ctxt.bias\n",
      "2021-03-01 15:17:39,658: INFO:    attn_Modifier.proj_ctxt.weight\n",
      "2021-03-01 15:17:39,658: INFO:    attn_Modifier.proj_ctxt_key_matrix.bias\n",
      "2021-03-01 15:17:39,659: INFO:    attn_Modifier.proj_ctxt_key_matrix.weight\n",
      "2021-03-01 15:17:39,659: INFO:    attn_SpecificDisease.key\n",
      "2021-03-01 15:17:39,659: INFO:    attn_SpecificDisease.proj_ctxt.bias\n",
      "2021-03-01 15:17:39,660: INFO:    attn_SpecificDisease.proj_ctxt.weight\n",
      "2021-03-01 15:17:39,660: INFO:    attn_SpecificDisease.proj_ctxt_key_matrix.bias\n",
      "2021-03-01 15:17:39,660: INFO:    attn_SpecificDisease.proj_ctxt_key_matrix.weight\n",
      "2021-03-01 15:17:39,661: INFO:    encoder_word._module.bias_hh_l0\n",
      "2021-03-01 15:17:39,661: INFO:    encoder_word._module.bias_hh_l0_reverse\n",
      "2021-03-01 15:17:39,661: INFO:    encoder_word._module.bias_ih_l0\n",
      "2021-03-01 15:17:39,662: INFO:    encoder_word._module.bias_ih_l0_reverse\n",
      "2021-03-01 15:17:39,662: INFO:    encoder_word._module.weight_hh_l0\n",
      "2021-03-01 15:17:39,662: INFO:    encoder_word._module.weight_hh_l0_reverse\n",
      "2021-03-01 15:17:39,663: INFO:    encoder_word._module.weight_ih_l0\n",
      "2021-03-01 15:17:39,663: INFO:    encoder_word._module.weight_ih_l0_reverse\n",
      "2021-03-01 15:17:39,663: INFO:    logits_layer_CompositeMention.bias\n",
      "2021-03-01 15:17:39,663: INFO:    logits_layer_CompositeMention.weight\n",
      "2021-03-01 15:17:39,664: INFO:    logits_layer_DiseaseClass.bias\n",
      "2021-03-01 15:17:39,664: INFO:    logits_layer_DiseaseClass.weight\n",
      "2021-03-01 15:17:39,664: INFO:    logits_layer_Modifier.bias\n",
      "2021-03-01 15:17:39,665: INFO:    logits_layer_Modifier.weight\n",
      "2021-03-01 15:17:39,665: INFO:    logits_layer_O.bias\n",
      "2021-03-01 15:17:39,665: INFO:    logits_layer_O.weight\n",
      "2021-03-01 15:17:39,666: INFO:    logits_layer_SpecificDisease.bias\n",
      "2021-03-01 15:17:39,666: INFO:    logits_layer_SpecificDisease.weight\n",
      "2021-03-01 15:17:39,666: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.gamma\n",
      "2021-03-01 15:17:39,667: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
      "2021-03-01 15:17:39,667: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
      "2021-03-01 15:17:39,667: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
      "2021-03-01 15:17:39,668: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
      "2021-03-01 15:17:39,668: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
      "2021-03-01 15:17:39,668: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
      "2021-03-01 15:17:39,669: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
      "2021-03-01 15:17:39,669: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
      "2021-03-01 15:17:39,669: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
      "2021-03-01 15:17:39,669: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
      "2021-03-01 15:17:39,670: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
      "2021-03-01 15:17:39,670: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
      "2021-03-01 15:17:39,670: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
      "2021-03-01 15:17:39,671: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
      "2021-03-01 15:17:39,671: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
      "2021-03-01 15:17:39,671: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
      "2021-03-01 15:17:39,672: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
      "2021-03-01 15:17:39,672: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,672: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,673: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,673: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,673: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
      "2021-03-01 15:17:39,674: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
      "2021-03-01 15:17:39,674: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
      "2021-03-01 15:17:39,674: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
      "2021-03-01 15:17:39,675: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
      "2021-03-01 15:17:39,675: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
      "2021-03-01 15:17:39,675: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,676: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,676: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,676: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,676: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
      "2021-03-01 15:17:39,677: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
      "2021-03-01 15:17:39,677: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,677: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,678: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,678: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,678: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
      "2021-03-01 15:17:39,679: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
      "2021-03-01 15:17:39,679: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
      "2021-03-01 15:17:39,679: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
      "2021-03-01 15:17:39,680: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
      "2021-03-01 15:17:39,680: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
      "2021-03-01 15:17:39,680: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,681: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,681: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,681: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,681: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
      "2021-03-01 15:17:39,682: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
      "2021-03-01 15:17:39,682: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,682: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,683: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,683: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,683: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
      "2021-03-01 15:17:39,684: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
      "2021-03-01 15:17:39,684: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
      "2021-03-01 15:17:39,684: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
      "2021-03-01 15:17:39,685: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
      "2021-03-01 15:17:39,685: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
      "2021-03-01 15:17:39,685: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,686: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,686: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,686: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,687: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
      "2021-03-01 15:17:39,687: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
      "2021-03-01 15:17:39,687: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,688: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,688: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,688: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,688: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
      "2021-03-01 15:17:39,689: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
      "2021-03-01 15:17:39,689: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
      "2021-03-01 15:17:39,689: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
      "2021-03-01 15:17:39,690: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
      "2021-03-01 15:17:39,690: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
      "2021-03-01 15:17:39,690: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,691: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,691: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,691: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,692: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
      "2021-03-01 15:17:39,692: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
      "2021-03-01 15:17:39,692: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,693: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,693: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,693: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,694: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
      "2021-03-01 15:17:39,694: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
      "2021-03-01 15:17:39,694: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
      "2021-03-01 15:17:39,695: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
      "2021-03-01 15:17:39,695: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
      "2021-03-01 15:17:39,695: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
      "2021-03-01 15:17:39,696: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,696: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,696: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,697: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,697: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
      "2021-03-01 15:17:39,697: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
      "2021-03-01 15:17:39,697: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,698: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,698: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,698: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,699: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
      "2021-03-01 15:17:39,699: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
      "2021-03-01 15:17:39,699: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
      "2021-03-01 15:17:39,700: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
      "2021-03-01 15:17:39,700: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
      "2021-03-01 15:17:39,700: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
      "2021-03-01 15:17:39,701: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,701: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,701: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,702: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,702: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
      "2021-03-01 15:17:39,702: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
      "2021-03-01 15:17:39,703: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,703: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,703: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,704: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,704: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
      "2021-03-01 15:17:39,704: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
      "2021-03-01 15:17:39,704: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
      "2021-03-01 15:17:39,705: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
      "2021-03-01 15:17:39,705: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
      "2021-03-01 15:17:39,705: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
      "2021-03-01 15:17:39,706: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,706: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,706: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,707: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,707: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
      "2021-03-01 15:17:39,707: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
      "2021-03-01 15:17:39,708: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,708: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,708: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,709: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,709: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
      "2021-03-01 15:17:39,709: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
      "2021-03-01 15:17:39,710: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
      "2021-03-01 15:17:39,710: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
      "2021-03-01 15:17:39,710: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
      "2021-03-01 15:17:39,710: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
      "2021-03-01 15:17:39,711: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,711: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,711: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,712: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,712: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
      "2021-03-01 15:17:39,712: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
      "2021-03-01 15:17:39,713: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,713: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,713: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,714: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,714: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
      "2021-03-01 15:17:39,714: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
      "2021-03-01 15:17:39,715: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
      "2021-03-01 15:17:39,715: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
      "2021-03-01 15:17:39,715: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
      "2021-03-01 15:17:39,716: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
      "2021-03-01 15:17:39,716: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,716: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,717: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,717: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,717: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
      "2021-03-01 15:17:39,718: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
      "2021-03-01 15:17:39,718: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,718: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,719: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,719: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,719: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
      "2021-03-01 15:17:39,719: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
      "2021-03-01 15:17:39,720: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
      "2021-03-01 15:17:39,720: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
      "2021-03-01 15:17:39,720: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
      "2021-03-01 15:17:39,721: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
      "2021-03-01 15:17:39,721: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,721: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,722: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,722: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,722: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
      "2021-03-01 15:17:39,723: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
      "2021-03-01 15:17:39,723: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,723: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,724: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,724: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,724: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
      "2021-03-01 15:17:39,725: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
      "2021-03-01 15:17:39,725: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
      "2021-03-01 15:17:39,725: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
      "2021-03-01 15:17:39,726: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
      "2021-03-01 15:17:39,726: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
      "2021-03-01 15:17:39,726: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,727: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,727: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,727: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,728: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
      "2021-03-01 15:17:39,728: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
      "2021-03-01 15:17:39,728: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,728: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,729: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "2021-03-01 15:17:39,729: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "2021-03-01 15:17:39,729: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
      "2021-03-01 15:17:39,730: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n",
      "2021-03-01 15:17:39,730: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
      "2021-03-01 15:17:39,730: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
      "2021-03-01 15:17:39,731: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
      "2021-03-01 15:17:39,731: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
      "2021-03-01 15:17:39,731: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "2021-03-01 15:17:39,732: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "2021-03-01 15:17:39,732: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2021-03-01 15:17:39,732: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2021-03-01 15:17:39,733: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
      "2021-03-01 15:17:39,733: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
      "2021-03-01 15:17:39,733: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
      "2021-03-01 15:17:39,734: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n",
      "INIT MULTICLASSIFIER\n",
      "2021-03-01 15:17:39,734: INFO: Model Construction done\n",
      "2021-03-01 15:17:39,735: INFO: segmentation.type = SymbolStopwordFilteredMultiPredictions\n",
      "2021-03-01 15:17:39,735: INFO: segmentation.tol = 0.05\n",
      "2021-03-01 15:17:39,736: INFO: segmentation.visualize = True\n",
      "2021-03-01 15:17:39,736: INFO: segmentation.use_probs = True\n",
      "INIT BASE PRED CLASS\n",
      "INIT BASIC MULTI PREDICTIONS\n",
      "INIT SymbolStopwordFilteredMultiPredictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pdb\n",
    "\n",
    "sys.path.insert(0,'/home/ytaille/AttentionSegmentation')\n",
    "\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.iterators import DataIterator\n",
    "# import allennlp.data.dataset_readers as Readers\n",
    "import AttentionSegmentation.reader as Readers\n",
    "\n",
    "# import model as Models\n",
    "import AttentionSegmentation.model.classifiers as Models\n",
    "\n",
    "from AttentionSegmentation.commons.utils import \\\n",
    "    setup_output_dir, read_from_config_file\n",
    "from AttentionSegmentation.commons.model_utils import \\\n",
    "    construct_vocab, load_model_from_existing\n",
    "# from AttentionSegmentation.visualization.visualize_attns import \\\n",
    "#     html_visualizer\n",
    "import AttentionSegmentation.model.attn2labels as SegmentationModels\n",
    "\n",
    "\"\"\"The main entry point\n",
    "\n",
    "This is the main entry point for training HAN SOLO models.\n",
    "\n",
    "Usage::\n",
    "\n",
    "    ${PYTHONPATH} -m AttentionSegmentation/main\n",
    "        --config_file ${CONFIG_FILE}\n",
    "\n",
    "\"\"\"\n",
    "args = type('MyClass', (object,), {'content':{}})()\n",
    "args.config_file = 'Configs/config_ncbi.json'\n",
    "args.log = 'INFO'\n",
    "args.loglevel = 'INFO'\n",
    "args.seed = 1\n",
    "\n",
    "# Setup Experiment Directory\n",
    "config = read_from_config_file(args.config_file)\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if config.get('trainer', None) is not None and \\\n",
    "       config.get('trainer', None).get('cuda_device', -1) > 0:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "serial_dir, config = setup_output_dir(config, args.loglevel)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Training Data\n",
    "TRAIN_PATH = config.pop(\"train_data_path\")\n",
    "logger.info(\"Loading Training Data from {0}\".format(TRAIN_PATH))\n",
    "dataset_reader_params = config.pop(\"dataset_reader\")\n",
    "reader_type = dataset_reader_params.pop(\"type\", None)\n",
    "assert reader_type is not None and hasattr(Readers, reader_type),\\\n",
    "    f\"Cannot find reader {reader_type}\"\n",
    "reader = getattr(Readers, reader_type).from_params(dataset_reader_params)\n",
    "instances_train = reader.read(file_path=TRAIN_PATH)\n",
    "instances_train = instances_train\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Training Data\", len(instances_train)))\n",
    "\n",
    "# Load Validation Data\n",
    "VAL_PATH = config.pop(\"validation_data_path\")\n",
    "logger.info(\"Loading Validation Data from {0}\".format(VAL_PATH))\n",
    "instances_val = reader.read(VAL_PATH)\n",
    "instances_val = instances_val\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Validation Data\", len(instances_val)))\n",
    "\n",
    "# Load Test Data\n",
    "TEST_PATH = config.pop(\"test_data_path\", None)\n",
    "instances_test = None\n",
    "if TEST_PATH is not None:\n",
    "    logger.info(\"Loading Test Data from {0}\".format(TEST_PATH))\n",
    "    instances_test = reader.read(TEST_PATH)\n",
    "    instances_test = instances_test\n",
    "    logger.info(\"Length of {0}: {1}\".format(\n",
    "        \"Testing Data\", len(instances_test)))\n",
    "\n",
    "# # Load Pretrained Existing Model\n",
    "# load_config = config.pop(\"load_from\", None)\n",
    "\n",
    "# # Construct Vocabulary\n",
    "vocab_size = config.pop(\"max_vocab_size\", -1)\n",
    "logger.info(\"Constructing Vocab of size: {0}\".format(vocab_size))\n",
    "vocab_size = None if vocab_size == -1 else vocab_size\n",
    "vocab = Vocabulary.from_instances(instances_train,\n",
    "                                  max_vocab_size=vocab_size)\n",
    "vocab_dir = os.path.join(serial_dir, \"vocab\")\n",
    "assert os.path.exists(vocab_dir), \"Couldn't find the vocab directory\"\n",
    "vocab.save_to_files(vocab_dir)\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # modify the vocab from the source model vocab\n",
    "#     src_vocab_path = load_config.pop(\"vocab_path\", None)\n",
    "#     if src_vocab_path is not None:\n",
    "#         vocab = construct_vocab(src_vocab_path, vocab_dir)\n",
    "#         # Delete the old vocab\n",
    "#         for file in os.listdir(vocab_dir):\n",
    "#             os.remove(os.path.join(vocab_dir, file))\n",
    "#         # save the new vocab\n",
    "#         vocab.save_to_files(vocab_dir)\n",
    "logger.info(\"Saving vocab to {0}\".format(vocab_dir))\n",
    "logger.info(\"Vocab Construction Done\")\n",
    "\n",
    "# # Construct the data iterators\n",
    "logger.info(\"Constructing Data Iterators\")\n",
    "data_iterator = DataIterator.from_params(config.pop(\"iterator\"))\n",
    "data_iterator.index_with(vocab)\n",
    "\n",
    "logger.info(\"Data Iterators Done\")\n",
    "\n",
    "# Create the model\n",
    "logger.info(\"Constructing The model\")\n",
    "model_params = config.pop(\"model\")\n",
    "model_type = model_params.pop(\"type\")\n",
    "assert model_type is not None and hasattr(Models, model_type),\\\n",
    "    f\"Cannot find reader {model_type}\"\n",
    "model = getattr(Models, model_type).from_params(\n",
    "    vocab=vocab,\n",
    "    params=model_params,\n",
    "    label_indexer=reader.get_label_indexer()\n",
    ")\n",
    "logger.info(\"Model Construction done\")\n",
    "\n",
    "# visualize = config.pop(\"visualize\", False)\n",
    "# visualizer = None\n",
    "# if visualize:\n",
    "#     visualizer = html_visualizer(vocab, reader)\n",
    "segmenter_params = config.pop(\"segmentation\")\n",
    "segment_class = segmenter_params.pop(\"type\")\n",
    "segmenter = getattr(SegmentationModels, segment_class).from_params(\n",
    "    vocab=vocab,\n",
    "    reader=reader,\n",
    "    params=segmenter_params\n",
    ")\n",
    "\n",
    "# logger.info(\"Segmenter Done\")\n",
    "\n",
    "# print(\"##################################\\nAYYYYYYYYYYYYYYYYYYYYYYYY\\n\\n\\n\\n\\n\\n\\n\\n###########################\")\n",
    "\n",
    "# exit()\n",
    "\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # Load the weights, as specified by the load_config\n",
    "#     model_path = load_config.pop(\"model_path\", None)\n",
    "#     layers = load_config.pop(\"layers\", None)\n",
    "#     load_config.assert_empty(\"Load Config\")\n",
    "#     assert model_path is not None,\\\n",
    "#         \"You need to specify model path to load from\"\n",
    "#     model = load_model_from_existing(model_path, model, layers)\n",
    "#     logger.info(\"Pretrained weights loaded\")\n",
    "\n",
    "# logger.info(\"Starting the training process\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary to add unknown tag to dictionnary to avoid errors later\n",
    "data_iterator.vocab.add_token_to_namespace(\"@@UNKNOWN@@\", \"chunk_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_from_config_file(args.config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:17:40,018: INFO: PyTorch version 1.5.1 available.\n",
      "2021-03-01 15:17:43,220: INFO: TensorFlow version 2.3.1 available.\n",
      "2021-03-01 15:17:43,688: INFO: Loading faiss with AVX2 support.\n",
      "2021-03-01 15:17:43,691: INFO: Loading faiss.\n",
      "2021-03-01 15:17:44,160: INFO: trainer.patience = 10\n",
      "2021-03-01 15:17:44,162: INFO: trainer.validation_metric = +accuracy\n",
      "2021-03-01 15:17:44,163: INFO: trainer.num_epochs = 50\n",
      "2021-03-01 15:17:44,164: INFO: trainer.cuda_device = 0\n",
      "2021-03-01 15:17:44,165: INFO: trainer.grad_norm = None\n",
      "2021-03-01 15:17:44,166: INFO: trainer.grad_clipping = None\n",
      "2021-03-01 15:17:44,167: INFO: trainer.num_serialized_models_to_keep = 1\n",
      "2021-03-01 15:17:46,675: INFO: trainer.optimizer.type = adam\n",
      "2021-03-01 15:17:46,678: INFO: trainer.optimizer.parameter_groups = [[['.*bert.*'], ConfigTree([('lr', 2e-07)])], [['.*encoder_word.*', '.*attn.*', '.*logit.*'], ConfigTree([('lr', 0.001)])]]\n",
      "2021-03-01 15:17:46,679: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:17:46,680: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:17:46,681: INFO: trainer.optimizer.parameter_groups.list.list.lr = 2e-07\n",
      "2021-03-01 15:17:46,682: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:17:46,683: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:17:46,684: INFO: trainer.optimizer.parameter_groups.list.list.lr = 0.001\n",
      "2021-03-01 15:17:46,687: INFO: Done constructing parameter groups.\n",
      "2021-03-01 15:17:46,688: INFO: Group 0: ['text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7', 'text_field_embedder.token_embedder_bert._scalar_mix.gamma', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight'], {'lr': 2e-07}\n",
      "2021-03-01 15:17:46,689: INFO: Group 1: ['attn_DiseaseClass.proj_ctxt_key_matrix.weight', 'logits_layer_O.weight', 'attn_DiseaseClass.key', 'encoder_word._module.weight_ih_l0', 'encoder_word._module.weight_hh_l0', 'attn_DiseaseClass.proj_ctxt.weight', 'attn_Modifier.proj_ctxt.bias', 'logits_layer_Modifier.bias', 'attn_Modifier.key', 'logits_layer_SpecificDisease.weight', 'logits_layer_CompositeMention.bias', 'encoder_word._module.bias_ih_l0', 'encoder_word._module.bias_hh_l0_reverse', 'attn_Modifier.proj_ctxt_key_matrix.bias', 'logits_layer_O.bias', 'attn_CompositeMention.proj_ctxt_key_matrix.bias', 'encoder_word._module.weight_hh_l0_reverse', 'encoder_word._module.weight_ih_l0_reverse', 'attn_CompositeMention.key', 'logits_layer_Modifier.weight', 'attn_Modifier.proj_ctxt.weight', 'encoder_word._module.bias_hh_l0', 'logits_layer_SpecificDisease.bias', 'attn_SpecificDisease.proj_ctxt.bias', 'attn_CompositeMention.proj_ctxt.weight', 'attn_CompositeMention.proj_ctxt.bias', 'attn_CompositeMention.proj_ctxt_key_matrix.weight', 'attn_DiseaseClass.proj_ctxt.bias', 'attn_DiseaseClass.proj_ctxt_key_matrix.bias', 'logits_layer_CompositeMention.weight', 'attn_Modifier.proj_ctxt_key_matrix.weight', 'encoder_word._module.bias_ih_l0_reverse', 'logits_layer_DiseaseClass.weight', 'attn_SpecificDisease.key', 'attn_SpecificDisease.proj_ctxt_key_matrix.bias', 'attn_SpecificDisease.proj_ctxt.weight', 'logits_layer_DiseaseClass.bias', 'attn_SpecificDisease.proj_ctxt_key_matrix.weight'], {'lr': 0.001}\n",
      "2021-03-01 15:17:46,690: INFO: Group 2: [], {}\n",
      "2021-03-01 15:17:46,691: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:17:46,691: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:17:46,692: INFO: trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
      "2021-03-01 15:17:46,693: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-01 15:17:46,693: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-01 15:17:46,694: INFO: trainer.learning_rate_scheduler.factor = 0.5\n",
      "2021-03-01 15:17:46,694: INFO: trainer.learning_rate_scheduler.patience = 5\n",
      "2021-03-01 15:17:46,710: INFO: Using cache /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f\n",
      "2021-03-01 15:17:46,712: INFO: Loading /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f/output.pkl... \n",
      "2021-03-01 15:18:05,405: INFO: Quaero mentions: 16283\n",
      "2021-03-01 15:18:21,026: INFO: Using cache /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23\n",
      "2021-03-01 15:18:21,035: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23/history.yaml... \n",
      "2021-03-01 15:18:21,256: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23/checkpoint-15.pt... \n",
      "2021-03-01 15:18:22,220: INFO: Model restored to its best state: 15\n"
     ]
    }
   ],
   "source": [
    "from AttentionSegmentation.trainer import Trainer\n",
    "\n",
    "from nlstruct.utils import  torch_global as tg\n",
    "\n",
    "tg.set_device('cpu')\n",
    "\n",
    "trainer = Trainer.from_params(\n",
    "    model=model,\n",
    "    base_dir=serial_dir,\n",
    "    iterator=data_iterator,\n",
    "    train_data=instances_train,\n",
    "    validation_data=instances_val,\n",
    "    segmenter=segmenter,\n",
    "    params=config.pop(\"trainer\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = torch.Tensor([[0,1,0,1,0], [0,0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0, 0, 1]]), tensor([[1, 3, 2]]))\n",
      "(tensor([[0, 0, 0, 0, 1, 1]]), tensor([[0, 2, 2, 4, 1, 3]]))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 1.0000, 0.1000, 1.0000, 0.1000],\n",
       "        [0.0000, 0.1000, 1.0000, 0.1000, 0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_boosted = attn.clone()\n",
    "nnz = (attn>0).nonzero().t().chunk(chunks=2,dim=0)\n",
    "\n",
    "print(nnz)\n",
    "\n",
    "new_nnz = [[], []]\n",
    "\n",
    "for nz0, nz1 in zip(nnz[0][0].numpy(), nnz[1][0].numpy()):\n",
    "    new_nnz[0].extend([nz0,nz0])\n",
    "    new_nnz[1].extend([nz1-1,nz1+1])\n",
    "    \n",
    "new_nnz[0] = torch.Tensor([new_nnz[0]]).long()\n",
    "new_nnz[1] = torch.Tensor([new_nnz[1]]).long()\n",
    "new_nnz = (new_nnz[0], new_nnz[1])\n",
    "print(new_nnz)\n",
    "attn_boosted[new_nnz] += 0.1\n",
    "\n",
    "attn_boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE BIO BERT\n",
    "# TRAIN STEP 1 ONLY ON MEDIC LABELS (+ NCBI MENTIONS)\n",
    "# PREPROCESS / TRAIN / ATTEINDRE BONS SCORES\n",
    "# GET MEDIC ALTERNATIVE LABELS DANS NLSTRUCT -> TRADUIRE LABELS NCBI VERS MEDIC\n",
    "\n",
    "# USE ENTROPY INSTEAD OF CROSS ENTROPY -> not rely on labelled data only (rely on model certainty)\n",
    "\n",
    "# GROUPS : TYPE SEMANTIQUE À LA MENTION (pas utiliser)\n",
    "\n",
    "# NGRAMS FOR ENTITIES -> not possible with discontinued entities\n",
    "\n",
    "# Use \"separation token\" in phrases ?\n",
    "\n",
    "# Use a limited number of attention heads (not one per class)\n",
    "\n",
    "# Use same method as Perceval for trajectories (draw closest ones, reduce list, repeat) -> prédiction itérative\n",
    "\n",
    "# Maybe remove weakly supervised completely?\n",
    "\n",
    "# Test with Reinforce only after a few epochs\n",
    "\n",
    "# Facteur de représentation pour pondérer loss de Perceval ?\n",
    "\n",
    "# Plusieurs facteurs pour constituer la reward\n",
    "\n",
    "# Facteur de similarité mention extraite / synonyme plutôt que similarité mention / label ?\n",
    "\n",
    "# Make sure that every trajectory is different -> draw first then use Perceval\n",
    "\n",
    "# Métrique finale : Est-ce qu'on arrive à choper les CUI ? -> parce que frontières entités dures à déterminer \n",
    "\n",
    "# Use only one class ? -> simpler because all mentions are diseases -> MAKE SURE THAT SEVERAL MENTIONS ARE PREDICTABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import traceback\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, List, Tuple, Union, Iterable, Any, Set\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.nn.parallel import replicate, parallel_apply\n",
    "from torch.nn.parallel.scatter_gather import scatter_kwargs, gather\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from itertools import tee\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import peak_memory_mb, gpu_memory_mb\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.learning_rate_schedulers import LearningRateScheduler\n",
    "from allennlp.training.optimizers import Optimizer\n",
    "\n",
    "from AttentionSegmentation.commons.trainer_utils import is_sparse,\\\n",
    "    sparse_clip_norm, move_optimizer_to_cuda, TensorboardWriter\n",
    "# from AttentionSegmentation.visualization.visualize_attns \\\n",
    "#     import html_visualizer\n",
    "from AttentionSegmentation.model.attn2labels import BasePredictionClass\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TQDM_COLUMNS = 200\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/ytaille/deep_multilingual_normalization')\n",
    "from create_classifiers import create_classifiers\n",
    "from nlstruct.dataloaders import load_from_brat\n",
    "\n",
    "logger2 = logging.getLogger(\"nlstruct\")\n",
    "logger2.setLevel(logging.ERROR)\n",
    "\n",
    "from notebook_utils import *\n",
    "\n",
    "def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_memory_mb()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            for gpu, memory in gpu_memory_mb().items():\n",
    "                logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        from allennlp.data.fields.array_field import ArrayField\n",
    "\n",
    "        for i, td in enumerate(self._train_data):\n",
    "            td.fields['sample_id'] = ArrayField(np.array([i]))\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self._iterator(self._train_data,\n",
    "                                         num_epochs=1,\n",
    "                                         cuda_device=self._iterator_device,\n",
    "                                         shuffle=True,\n",
    "                                         )\n",
    "\n",
    "        train_generator, cp_generator, id_generator = tee(train_generator, 3)\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        for ig in id_generator:\n",
    "            ids.extend([int(sid.item()) for sid in ig['sample_id']])\n",
    "\n",
    "        shuffled_train_data = [self._train_data[i] for i in ids]\n",
    "\n",
    "#         train_predictions = self._segmenter.get_predictions(\n",
    "#                     instances=shuffled_train_data,\n",
    "#                     iterator = cp_generator,\n",
    "#                     model=self._model,\n",
    "#                     cuda_device=self._iterator_device,\n",
    "#                     verbose=True)\n",
    "\n",
    "        num_training_batches = self._iterator.get_num_batches(self._train_data)\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches\n",
    "                                         )\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        cpt_batch = 0\n",
    "\n",
    "        # Set the model to \"train\" mode.\n",
    "        self._model.train()\n",
    "\n",
    "        for batch in train_generator_tqdm:\n",
    "            \n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "            batch_len = len(batch['labels'])\n",
    "\n",
    "            # FOR train_predictions:\n",
    "            # pred/gold is sentence level\n",
    "            # pred_labels/gold_labels is word level\n",
    "\n",
    "\n",
    "            # FOR batch:\n",
    "            # labels is sentence level\n",
    "            # tags is word level\n",
    "\n",
    "            # print(train_texts)\n",
    "            # print(\"SENTENCE LEVEL\")\n",
    "            # print([tp['gold'] for tp in train_predictions[:10]])\n",
    "            # print(batch['labels'][:10])\n",
    "\n",
    "            # print(\"WORD LEVEL\")\n",
    "            # print([tp['gold_labels'] for tp in train_predictions[:2]])\n",
    "            # print(batch['tags'][:2])\n",
    "\n",
    "            # exit()\n",
    "            \n",
    "            if epoch <= -1:\n",
    "                trajectory_scores =  [0]\n",
    "            else:\n",
    "                output_dict = self._model(**batch)\n",
    "\n",
    "                attns = output_dict['attentions']\n",
    "\n",
    "                # Policy is \"attention mask\": attention scores should be higher if we want to predict CUI\n",
    "                # Only take words with attention above threshold when predicting with deep norm -> see if it's enough (reward indicates that)\n",
    "                # REINFORCE algo: (also known as Monte Carlo PG)\n",
    "                # - draw N trajectories (N attention paths?) -> discretise attentions to make them 1 / 0? -> see if it works with bernoulli first\n",
    "                # - evaluate each trajectory then sum (maybe add baseline -> subtract mean of all trajectories rewards)\n",
    "                # - Expected return is given by sum(prob(Ti | W) * reward(Ti)) -> see again if it works with bernoulli first\n",
    "                # W are WeakL weights \n",
    "                # - Gradient ascent of return / gradient descent of negative return\n",
    "\n",
    "                # Set horizon ? -> number / proportion of attention at 1 per batch\n",
    "                # Set number of trajectories ? -> maybe make trajectories number vary based on sentence length\n",
    "                # gamma = 0.9 ? -> used to simulate temporal importance of reward (multiply each step by a certain power of gamma, furthest rewards are less impactful) -> may not be possible to model here\n",
    "                \n",
    "                horizon = 0.2\n",
    "                n_trajectories = 10\n",
    "                gamma = 0.9\n",
    "                attn_threshold = 0.01\n",
    "\n",
    "                mask = batch['tokens']['mask']\n",
    "\n",
    "                prob_attn = attns\n",
    "                from torch.distributions import Binomial\n",
    "\n",
    "                m = Binomial(probs=prob_attn)\n",
    "                trajectory_scores = {i: [] for i in range(prob_attn.shape[-1])}\n",
    "\n",
    "                policy_loss = []\n",
    "                \n",
    "                all_samples = []\n",
    "                \n",
    "                for nb_traj in range(n_trajectories):\n",
    "                    attn_sample = m.sample()\n",
    "                    \n",
    "                    all_samples.append(attn_sample)\n",
    "                    \n",
    "                all_samples = torch.stack(all_samples)\n",
    "                \n",
    "                # logsum: probabilité qu'il y ait au moins une mention d'un type ? -> ensuite sigmoide\n",
    "                # d'abord agréger puis calculer la loss\n",
    "\n",
    "                # multi label: itération ? têtes d'attention ? tags\n",
    "\n",
    "                # match cui avec cui le plus probable -> \n",
    "\n",
    "                real_tokens = [np.array(b.fields['tokens'].tokens) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "#                     gold_labels = [np.array(b.fields['tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "                gold_norm_labels = [np.array(b.fields['chunk_tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "\n",
    "                unique_mask = [[]] * all_samples.shape[-1]\n",
    "\n",
    "                # real_tokens : batch_size * seq_len \n",
    "                # all_samples : nb_traj * batch_size * seq_len * n_class\n",
    "                # unique_mask: n_class * batch_size * nb_traj * seq_len\n",
    "                for alls_id, alls in enumerate(all_samples.permute(3,1,0,2)):\n",
    "                    for batch_id, bat in enumerate(alls):\n",
    "                        unique_mask[alls_id].append(torch.unique(bat, dim=0))\n",
    "\n",
    "                all_masked_tokens = [] \n",
    "                all_masked_gold_norm = []\n",
    "\n",
    "                for class_n in range(len(unique_mask)):\n",
    "                    list_class_n_token = []\n",
    "                    list_class_n_gold_norm = []\n",
    "                    for nb_traj in range(n_trajectories):\n",
    "                        list_batch_id_token = []\n",
    "                        list_batch_id_gold_norm = []\n",
    "                        for batch_id in range(batch_len):\n",
    "                            if nb_traj < len(unique_mask[class_n][batch_id]):\n",
    "                                current_tokens = real_tokens[batch_id]\n",
    "                                current_gold_norm = gold_norm_labels[batch_id]\n",
    "                                \n",
    "                                mask_index = unique_mask[class_n][batch_id][nb_traj][:len(current_tokens)].cpu().to(bool)\n",
    "                                \n",
    "                                list_batch_id_token.append(current_tokens[mask_index])\n",
    "                                list_batch_id_gold_norm.append(current_gold_norm[mask_index])\n",
    "                            else:\n",
    "                                list_batch_id_token.append([])\n",
    "                                list_batch_id_gold_norm.append([])\n",
    "                        list_class_n_token.append(list_batch_id_token)\n",
    "                        list_class_n_gold_norm.append(list_batch_id_gold_norm)\n",
    "                        \n",
    "                    all_masked_tokens.append(list_class_n_token)\n",
    "                    all_masked_gold_norm.append(list_class_n_gold_norm)\n",
    "                # all_masked_tokens: n_class * nb_traj * batch_size * seq_len\n",
    "                \n",
    "                all_masked_tokens = np.array(all_masked_tokens)\n",
    "                all_masked_gold_norm = np.array(all_masked_gold_norm)\n",
    "                \n",
    "                for nb_traj in range(n_trajectories):\n",
    "                    for class_n in range(attn_sample.shape[-1]):\n",
    "                        masked_tokens = all_masked_tokens[class_n,nb_traj]\n",
    "    #                     masked_gold = [rt[attn_mask[w_id,:len(rt)].cpu().to(bool)] for w_id, rt in enumerate(gold_labels)]\n",
    "                        masked_gold_norm = all_masked_gold_norm[class_n,nb_traj]\n",
    "\n",
    "                        save_to_ann(masked_tokens, masked_gold_norm, '/home/ytaille/data/tmp/ws_inputs/')\n",
    "\n",
    "                        # NLSTRUCT PART\n",
    "\n",
    "                        bert_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "                        dataset = load_from_brat(\"/home/ytaille/data/tmp/ws_inputs/\")\n",
    "\n",
    "                        if len(dataset['mentions']) == 0:\n",
    "                            continue\n",
    "\n",
    "                        dataset['mentions']['mention_id'] = dataset['mentions']['doc_id'] +'.'+ dataset['mentions']['mention_id'].astype(str)\n",
    "\n",
    "                        batcher, vocs, mention_ids = preprocess_train(\n",
    "                            dataset,\n",
    "                            vocabularies=self.vocabularies1,\n",
    "                            bert_name=bert_name,\n",
    "                        )\n",
    "\n",
    "                        batch_size = len(batcher)\n",
    "                        with_tqdm = True\n",
    "\n",
    "                        tg.set_device('cuda:0')#('cuda:0')\n",
    "                        device = tg.device\n",
    "\n",
    "                        pred_batcher = predict(batcher, self.classifier1)\n",
    "\n",
    "                        scores = compute_scores(pred_batcher, batcher)\n",
    "\n",
    "                        try:\n",
    "                            trajectory_scores[class_n].append((scores['loss'] * prob_attn).mean())\n",
    "                        except:\n",
    "                            print(trajectory_scores)\n",
    "                            raise\n",
    "                            \n",
    "\n",
    "            cpt_batch += batch_len\n",
    "\n",
    "            if any(len(tj) > 0 for tj in trajectory_scores.values()):\n",
    "                trajectory_scores = [t for tj in trajectory_scores.values() for t in tj]\n",
    "\n",
    "            else: policy_loss = 0\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            loss = self._batch_loss(batch, for_training=True) + sum(trajectory_scores) # policy_loss \n",
    "            loss.backward()\n",
    "\n",
    "            # Make sure Variable is on the cpu before converting to numpy.\n",
    "            # .cpu() is a no-op if you aren't using GPUs.\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            batch_grad_norm = self._rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "            if hasattr(self, \"_tf_params\") and self._tf_params is not None:\n",
    "                # We have TF logging\n",
    "                if self._batch_num_total % self._tf_params[\"log_every\"] == 0:\n",
    "                    self._tf_log(metrics, self._batch_num_total)\n",
    "\n",
    "        return self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "    \n",
    "import functools\n",
    "\n",
    "trainer._train_epoch = functools.partial(_train_epoch, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:18:23,260: INFO: Beginning training.\n",
      "2021-03-01 15:18:23,262: INFO: ==================================================\n",
      "2021-03-01 15:18:23,263: INFO: Starting Training Epoch 1/50\n",
      "2021-03-01 15:18:23,264: INFO: Peak CPU memory usage MB: 6867.56\n",
      "2021-03-01 15:18:23,363: INFO: GPU 0 memory usage MB: 18321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "  0%|          | 0/57 [00:00<?, ?it/s]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.0000, DiseaseClass: 0.2500, Modifier: 0.4375, SpecificDisease: 0.7188, accuracy: 0.3516, loss: 1.5015 ||:   2%|▏         | 1/57 [00:31<29:30, 31.61s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.0000, DiseaseClass: 0.2031, Modifier: 0.3281, SpecificDisease: 0.6094, accuracy: 0.2852, loss: 3.4266 ||:   4%|▎         | 2/57 [01:06<29:59, 32.71s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.2500, DiseaseClass: 0.2396, Modifier: 0.4271, SpecificDisease: 0.6667, accuracy: 0.3958, loss: 2.8853 ||:   5%|▌         | 3/57 [01:42<30:16, 33.63s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.4219, DiseaseClass: 0.2734, Modifier: 0.4531, SpecificDisease: 0.7188, accuracy: 0.4668, loss: 2.4725 ||:   7%|▋         | 4/57 [02:18<30:12, 34.21s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.5312, DiseaseClass: 0.2625, Modifier: 0.4437, SpecificDisease: 0.6562, accuracy: 0.4734, loss: 2.4676 ||:   9%|▉         | 5/57 [02:58<31:20, 36.17s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.6042, DiseaseClass: 0.2448, Modifier: 0.4167, SpecificDisease: 0.6042, accuracy: 0.4674, loss: 2.2669 ||:  11%|█         | 6/57 [03:35<30:44, 36.17s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.6562, DiseaseClass: 0.2121, Modifier: 0.3973, SpecificDisease: 0.5670, accuracy: 0.4581, loss: 3.2370 ||:  12%|█▏        | 7/57 [04:10<29:53, 35.87s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.6953, DiseaseClass: 0.2207, Modifier: 0.3984, SpecificDisease: 0.5391, accuracy: 0.4634, loss: 3.7743 ||:  14%|█▍        | 8/57 [04:49<30:10, 36.95s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.7257, DiseaseClass: 0.2830, Modifier: 0.4201, SpecificDisease: 0.5625, accuracy: 0.4978, loss: 3.4857 ||:  16%|█▌        | 9/57 [05:30<30:21, 37.96s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.7312, DiseaseClass: 0.3094, Modifier: 0.4500, SpecificDisease: 0.5875, accuracy: 0.5195, loss: 3.2971 ||:  18%|█▊        | 10/57 [06:10<30:12, 38.57s/it]/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:227: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/ipykernel_launcher.py:228: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "CompositeMention: 0.7312, DiseaseClass: 0.3094, Modifier: 0.4500, SpecificDisease: 0.5875, accuracy: 0.5195, loss: 3.2971 ||:  18%|█▊        | 10/57 [06:28<30:25, 38.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:24:53,894: ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-3435b262f1ae>\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/trainer.py\", line 61, in train\n",
      "    super(Trainer, self).train(*args, **kwargs)\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\", line 588, in train\n",
      "    train_metrics = self._train_epoch(epoch)\n",
      "  File \"<ipython-input-10-8270a012a95b>\", line 252, in _train_epoch\n",
      "    bert_name=bert_name,\n",
      "  File \"/home/ytaille/AttentionSegmentation/notebook_utils.py\", line 513, in preprocess_train\n",
      "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_auto.py\", line 200, in from_pretrained\n",
      "    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_utils.py\", line 898, in from_pretrained\n",
      "    return cls._from_pretrained(*inputs, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_utils.py\", line 1051, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_bert.py\", line 191, in __init__\n",
      "    self.vocab = load_vocab(vocab_file)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_bert.py\", line 106, in load_vocab\n",
      "    vocab[token] = index\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "2021-03-01 15:24:53,919: INFO: \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-01 15:24:54,055: ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-11-3435b262f1ae>\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/trainer.py\", line 61, in train\n",
      "    super(Trainer, self).train(*args, **kwargs)\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\", line 588, in train\n",
      "    train_metrics = self._train_epoch(epoch)\n",
      "  File \"<ipython-input-10-8270a012a95b>\", line 252, in _train_epoch\n",
      "    bert_name=bert_name,\n",
      "  File \"/home/ytaille/AttentionSegmentation/notebook_utils.py\", line 513, in preprocess_train\n",
      "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_auto.py\", line 200, in from_pretrained\n",
      "    return tokenizer_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_utils.py\", line 898, in from_pretrained\n",
      "    return cls._from_pretrained(*inputs, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_utils.py\", line 1051, in _from_pretrained\n",
      "    tokenizer = cls(*init_inputs, **init_kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_bert.py\", line 191, in __init__\n",
      "    self.vocab = load_vocab(vocab_file)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_bert.py\", line 106, in load_vocab\n",
      "    vocab[token] = index\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3360, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2047, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1436, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1336, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1193, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1150, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n",
      "2021-03-01 15:24:54,066: INFO: \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, continue_training)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-8270a012a95b>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    251\u001b[0m                             \u001b[0mvocabularies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabularies1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m                             \u001b[0mbert_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m                         )\n",
      "\u001b[0;32m~/AttentionSegmentation/notebook_utils.py\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(dataset, bert_name, vocabularies, max_length, apply_unidecode, prepend_labels)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuggingface_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_token_spans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munidecode_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    897\u001b[0m         \"\"\"\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m             )\n\u001b[0;32m--> 191\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids_to_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_bert.py\u001b[0m in \u001b[0;36mload_vocab\u001b[0;34m(vocab_file)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3262\u001b[0m                         \u001b[0masy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3263\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mawait\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0masync_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0masy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3264\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TypeError' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/async_helpers.py\u001b[0m in \u001b[0;36m_pseudo_sync_runner\u001b[0;34m(coro)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \"\"\"\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mcoro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_async\u001b[0;34m(self, raw_cell, store_history, silent, shell_futures)\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3071\u001b[0m                 has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n\u001b[0;32m-> 3072\u001b[0;31m                        interactivity=interactivity, compiler=compiler, result=result)\n\u001b[0m\u001b[1;32m   3073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3074\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_execution_succeeded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhas_raised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_ast_nodes\u001b[0;34m(self, nodelist, cell_name, interactivity, compiler, result)\u001b[0m\n\u001b[1;32m   3280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_before_exec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3282\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3283\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m             formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n\u001b[0;32m-> 1211\u001b[0;31m                                                                      chained_exceptions_tb_offset)\n\u001b[0m\u001b[1;32m   1212\u001b[0m             \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parts_of_chained_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Training Done.\")\n",
    "if instances_test is not None:\n",
    "    logger.info(\"Computing final Test Accuracy\")\n",
    "    trainer.test(instances_test)\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_mult_norm",
   "language": "python",
   "name": "deep_mult_norm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
