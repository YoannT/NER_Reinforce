{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.dataloaders.medic import get_raw_medic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/yt_nlp/lib/python3.7/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-24 16:34:35,673: INFO: train_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-24 16:34:35,678: INFO: Loading Training Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-24 16:34:35,681: INFO: dataset_reader.type = WeakConll2003DatasetReader\n",
      "2021-03-24 16:34:35,683: INFO: dataset_reader.token_indexers.bert.type = bert-pretrained\n",
      "2021-03-24 16:34:35,684: INFO: dataset_reader.token_indexers.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-24 16:34:35,686: INFO: dataset_reader.token_indexers.bert.use_starting_offsets = True\n",
      "2021-03-24 16:34:35,687: INFO: dataset_reader.token_indexers.bert.do_lowercase = False\n",
      "2021-03-24 16:34:35,688: INFO: dataset_reader.token_indexers.bert.never_lowercase = None\n",
      "2021-03-24 16:34:35,689: INFO: dataset_reader.token_indexers.bert.max_pieces = 512\n",
      "2021-03-24 16:34:35,692: INFO: loading vocabulary file ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pdb\n",
    "\n",
    "sys.path.insert(0,'/home/ytaille/AttentionSegmentation')\n",
    "\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.iterators import DataIterator\n",
    "# import allennlp.data.dataset_readers as Readers\n",
    "import AttentionSegmentation.reader as Readers\n",
    "\n",
    "# import model as Models\n",
    "import AttentionSegmentation.model.classifiers as Models\n",
    "\n",
    "from AttentionSegmentation.commons.utils import \\\n",
    "    setup_output_dir, read_from_config_file\n",
    "from AttentionSegmentation.commons.model_utils import \\\n",
    "    construct_vocab, load_model_from_existing\n",
    "# from AttentionSegmentation.visualization.visualize_attns import \\\n",
    "#     html_visualizer\n",
    "import AttentionSegmentation.model.attn2labels as SegmentationModels\n",
    "\n",
    "\"\"\"The main entry point\n",
    "\n",
    "This is the main entry point for training HAN SOLO models.\n",
    "\n",
    "Usage::\n",
    "\n",
    "    ${PYTHONPATH} -m AttentionSegmentation/main\n",
    "        --config_file ${CONFIG_FILE}\n",
    "\n",
    "\"\"\"\n",
    "args = type('MyClass', (object,), {'content':{}})()\n",
    "args.config_file = 'Configs/config_ncbi.json'\n",
    "args.log = 'INFO'\n",
    "args.loglevel = 'INFO'\n",
    "args.seed = 1\n",
    "\n",
    "# Setup Experiment Directory\n",
    "config = read_from_config_file(args.config_file)\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if config.get('trainer', None) is not None and \\\n",
    "       config.get('trainer', None).get('cuda_device', -1) > 0:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "serial_dir, config = setup_output_dir(config, args.loglevel)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Training Data\n",
    "TRAIN_PATH = config.pop(\"train_data_path\")\n",
    "logger.info(\"Loading Training Data from {0}\".format(TRAIN_PATH))\n",
    "dataset_reader_params = config.pop(\"dataset_reader\")\n",
    "reader_type = dataset_reader_params.pop(\"type\", None)\n",
    "assert reader_type is not None and hasattr(Readers, reader_type),\\\n",
    "    f\"Cannot find reader {reader_type}\"\n",
    "reader = getattr(Readers, reader_type).from_params(dataset_reader_params)\n",
    "instances_train = reader.read(file_path=TRAIN_PATH)\n",
    "instances_train = instances_train\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Training Data\", len(instances_train)))\n",
    "\n",
    "# Load Validation Data\n",
    "VAL_PATH = config.pop(\"validation_data_path\")\n",
    "logger.info(\"Loading Validation Data from {0}\".format(VAL_PATH))\n",
    "instances_val = reader.read(VAL_PATH)\n",
    "instances_val = instances_val\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Validation Data\", len(instances_val)))\n",
    "\n",
    "# Load Test Data\n",
    "TEST_PATH = config.pop(\"test_data_path\", None)\n",
    "instances_test = None\n",
    "if TEST_PATH is not None:\n",
    "    logger.info(\"Loading Test Data from {0}\".format(TEST_PATH))\n",
    "    instances_test = reader.read(TEST_PATH)\n",
    "    instances_test = instances_test\n",
    "    logger.info(\"Length of {0}: {1}\".format(\n",
    "        \"Testing Data\", len(instances_test)))\n",
    "\n",
    "# # Load Pretrained Existing Model\n",
    "# load_config = config.pop(\"load_from\", None)\n",
    "\n",
    "# # Construct Vocabulary\n",
    "vocab_size = config.pop(\"max_vocab_size\", -1)\n",
    "logger.info(\"Constructing Vocab of size: {0}\".format(vocab_size))\n",
    "vocab_size = None if vocab_size == -1 else vocab_size\n",
    "vocab = Vocabulary.from_instances(instances_train,\n",
    "                                  max_vocab_size=vocab_size)\n",
    "vocab_dir = os.path.join(serial_dir, \"vocab\")\n",
    "assert os.path.exists(vocab_dir), \"Couldn't find the vocab directory\"\n",
    "vocab.save_to_files(vocab_dir)\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # modify the vocab from the source model vocab\n",
    "#     src_vocab_path = load_config.pop(\"vocab_path\", None)\n",
    "#     if src_vocab_path is not None:\n",
    "#         vocab = construct_vocab(src_vocab_path, vocab_dir)\n",
    "#         # Delete the old vocab\n",
    "#         for file in os.listdir(vocab_dir):\n",
    "#             os.remove(os.path.join(vocab_dir, file))\n",
    "#         # save the new vocab\n",
    "#         vocab.save_to_files(vocab_dir)\n",
    "logger.info(\"Saving vocab to {0}\".format(vocab_dir))\n",
    "logger.info(\"Vocab Construction Done\")\n",
    "\n",
    "# # Construct the data iterators\n",
    "logger.info(\"Constructing Data Iterators\")\n",
    "data_iterator = DataIterator.from_params(config.pop(\"iterator\"))\n",
    "data_iterator.index_with(vocab)\n",
    "\n",
    "logger.info(\"Data Iterators Done\")\n",
    "\n",
    "# Create the model\n",
    "logger.info(\"Constructing The model\")\n",
    "model_params = config.pop(\"model\")\n",
    "model_type = model_params.pop(\"type\")\n",
    "assert model_type is not None and hasattr(Models, model_type),\\\n",
    "    f\"Cannot find reader {model_type}\"\n",
    "model = getattr(Models, model_type).from_params(\n",
    "    vocab=vocab,\n",
    "    params=model_params,\n",
    "    label_indexer=reader.get_label_indexer()\n",
    ")\n",
    "logger.info(\"Model Construction done\")\n",
    "\n",
    "# visualize = config.pop(\"visualize\", False)\n",
    "# visualizer = None\n",
    "# if visualize:\n",
    "#     visualizer = html_visualizer(vocab, reader)\n",
    "segmenter_params = config.pop(\"segmentation\")\n",
    "segment_class = segmenter_params.pop(\"type\")\n",
    "segmenter = getattr(SegmentationModels, segment_class).from_params(\n",
    "    vocab=vocab,\n",
    "    reader=reader,\n",
    "    params=segmenter_params\n",
    ")\n",
    "\n",
    "# logger.info(\"Segmenter Done\")\n",
    "\n",
    "# print(\"##################################\\nAYYYYYYYYYYYYYYYYYYYYYYYY\\n\\n\\n\\n\\n\\n\\n\\n###########################\")\n",
    "\n",
    "# exit()\n",
    "\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # Load the weights, as specified by the load_config\n",
    "#     model_path = load_config.pop(\"model_path\", None)\n",
    "#     layers = load_config.pop(\"layers\", None)\n",
    "#     load_config.assert_empty(\"Load Config\")\n",
    "#     assert model_path is not None,\\\n",
    "#         \"You need to specify model path to load from\"\n",
    "#     model = load_model_from_existing(model_path, model, layers)\n",
    "#     logger.info(\"Pretrained weights loaded\")\n",
    "\n",
    "# logger.info(\"Starting the training process\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers.models.bert.modeling_bert\n",
    "from transformers.models.bert.modeling_bert import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary to add unknown tag to dictionnary to avoid errors later\n",
    "data_iterator.vocab.add_token_to_namespace(\"@@UNKNOWN@@\", \"chunk_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_from_config_file(args.config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AttentionSegmentation.trainer import Trainer\n",
    "\n",
    "from nlstruct.utils import  torch_global as tg\n",
    "\n",
    "trainer = Trainer.from_params(\n",
    "    model=model,\n",
    "    base_dir=serial_dir,\n",
    "    iterator=data_iterator,\n",
    "    train_data=instances_train,\n",
    "    validation_data=instances_val,\n",
    "    segmenter=segmenter,\n",
    "    params=config.pop(\"trainer\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIT FOR BOOSTING SURROUNDING ATTENTIONS\n",
    "\n",
    "# attn = torch.Tensor([[0,1,0,1,0], [0,0,1,0,0]])\n",
    "\n",
    "# attn_boosted = attn.clone()\n",
    "# nnz = (attn>0).nonzero().t().chunk(chunks=2,dim=0)\n",
    "\n",
    "# print(nnz)\n",
    "\n",
    "# new_nnz = [[], []]\n",
    "\n",
    "# for nz0, nz1 in zip(nnz[0][0].numpy(), nnz[1][0].numpy()):\n",
    "#     new_nnz[0].extend([nz0,nz0])\n",
    "#     new_nnz[1].extend([nz1-1,nz1+1])\n",
    "    \n",
    "# new_nnz[0] = torch.Tensor([new_nnz[0]]).long()\n",
    "# new_nnz[1] = torch.Tensor([new_nnz[1]]).long()\n",
    "# new_nnz = (new_nnz[0], new_nnz[1])\n",
    "# print(new_nnz)\n",
    "# attn_boosted[new_nnz] += 0.1\n",
    "\n",
    "# attn_boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE BIO BERT\n",
    "# TRAIN STEP 1 ONLY ON MEDIC LABELS (+ NCBI MENTIONS)\n",
    "# PREPROCESS / TRAIN / ATTEINDRE BONS SCORES\n",
    "# GET MEDIC ALTERNATIVE LABELS DANS NLSTRUCT -> TRADUIRE LABELS NCBI VERS MEDIC\n",
    "\n",
    "# USE ENTROPY INSTEAD OF CROSS ENTROPY -> not rely on labelled data only (rely on model certainty)\n",
    "\n",
    "# GROUPS : TYPE SEMANTIQUE À LA MENTION (pas utiliser)\n",
    "\n",
    "# NGRAMS FOR ENTITIES -> not possible with discontinued entities\n",
    "\n",
    "# Use \"separation token\" in phrases ?\n",
    "\n",
    "# Use a limited number of attention heads (not one per class)\n",
    "\n",
    "# Use same method as Perceval for trajectories (draw closest ones, reduce list, repeat) -> prédiction itérative\n",
    "\n",
    "# Maybe remove weakly supervised completely?\n",
    "\n",
    "# Test with Reinforce only after a few epochs\n",
    "\n",
    "# Facteur de représentation pour pondérer loss de Perceval ?\n",
    "\n",
    "# Plusieurs facteurs pour constituer la reward\n",
    "\n",
    "# Facteur de similarité mention extraite / synonyme plutôt que similarité mention / label ?\n",
    "\n",
    "# Make sure that every trajectory is different -> draw first then use Perceval\n",
    "\n",
    "# Métrique finale : Est-ce qu'on arrive à choper les CUI ? -> parce que frontières entités dures à déterminer \n",
    "\n",
    "# Use only one class ? -> simpler because all mentions are diseases -> MAKE SURE THAT SEVERAL MENTIONS ARE PREDICTABLE\n",
    "\n",
    "# maybe problem with reinforce comes from hyperparameters??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import traceback\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, List, Tuple, Union, Iterable, Any, Set\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.nn.parallel import replicate, parallel_apply\n",
    "from torch.nn.parallel.scatter_gather import scatter_kwargs, gather\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from itertools import tee\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import peak_memory_mb, gpu_memory_mb\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.learning_rate_schedulers import LearningRateScheduler\n",
    "from allennlp.training.optimizers import Optimizer\n",
    "\n",
    "from AttentionSegmentation.commons.trainer_utils import is_sparse,\\\n",
    "    sparse_clip_norm, move_optimizer_to_cuda, TensorboardWriter\n",
    "# from AttentionSegmentation.visualization.visualize_attns \\\n",
    "#     import html_visualizer\n",
    "from AttentionSegmentation.model.attn2labels import BasePredictionClass\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TQDM_COLUMNS = 200\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/ytaille/deep_multilingual_normalization')\n",
    "from create_classifiers import create_classifiers\n",
    "from nlstruct.dataloaders import load_from_brat\n",
    "\n",
    "logger2 = logging.getLogger(\"nlstruct\")\n",
    "logger2.setLevel(logging.ERROR)\n",
    "\n",
    "from notebook_utils import *\n",
    "\n",
    "def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_memory_mb()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            for gpu, memory in gpu_memory_mb().items():\n",
    "                logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        from allennlp.data.fields.array_field import ArrayField\n",
    "\n",
    "        for i, td in enumerate(self._train_data):\n",
    "            td.fields['sample_id'] = ArrayField(np.array([i]))\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self._iterator(self._train_data,\n",
    "                                         num_epochs=1,\n",
    "                                         cuda_device=self._iterator_device,\n",
    "                                         shuffle=True,\n",
    "                                         )\n",
    "\n",
    "        train_generator, cp_generator, id_generator = tee(train_generator, 3)\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        for ig in id_generator:\n",
    "            ids.extend([int(sid.item()) for sid in ig['sample_id']])\n",
    "\n",
    "        shuffled_train_data = [self._train_data[i] for i in ids]\n",
    "\n",
    "#         train_predictions = self._segmenter.get_predictions(\n",
    "#                     instances=shuffled_train_data,\n",
    "#                     iterator = cp_generator,\n",
    "#                     model=self._model,\n",
    "#                     cuda_device=self._iterator_device,\n",
    "#                     verbose=True)\n",
    "\n",
    "        num_training_batches = self._iterator.get_num_batches(self._train_data)\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches\n",
    "                                         )\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        cpt_batch = 0\n",
    "\n",
    "        # Set the model to \"train\" mode.\n",
    "        self._model.train()\n",
    "\n",
    "        for batch in train_generator_tqdm:\n",
    "            \n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "            batch_len = len(batch['labels'])\n",
    "\n",
    "            # FOR train_predictions:\n",
    "            # pred/gold is sentence level\n",
    "            # pred_labels/gold_labels is word level\n",
    "\n",
    "\n",
    "            # FOR batch:\n",
    "            # labels is sentence level\n",
    "            # tags is word level\n",
    "\n",
    "            # print(train_texts)\n",
    "            # print(\"SENTENCE LEVEL\")\n",
    "            # print([tp['gold'] for tp in train_predictions[:10]])\n",
    "            # print(batch['labels'][:10])\n",
    "\n",
    "            # print(\"WORD LEVEL\")\n",
    "            # print([tp['gold_labels'] for tp in train_predictions[:2]])\n",
    "            # print(batch['tags'][:2])\n",
    "\n",
    "            # exit()\n",
    "            \n",
    "            if epoch <= -1:\n",
    "                trajectory_scores =  [0]\n",
    "            else:\n",
    "                output_dict = self._model(**batch)\n",
    "                \n",
    "                attns = output_dict['attentions']\n",
    "#                 attns = output_dict['attentions_rl'].permute(0,2,1)\n",
    "                \n",
    "                # Policy is \"attention mask\": attention scores should be higher if we want to predict CUI\n",
    "                # Only take words with attention above threshold when predicting with deep norm -> see if it's enough (reward indicates that)\n",
    "                # REINFORCE algo: (also known as Monte Carlo PG)\n",
    "                # - draw N trajectories (N attention paths?) -> discretise attentions to make them 1 / 0? -> see if it works with bernoulli first\n",
    "                # - evaluate each trajectory then sum (maybe add baseline -> subtract mean of all trajectories rewards)\n",
    "                # - Expected return is given by sum(prob(Ti | W) * reward(Ti)) -> see again if it works with bernoulli first\n",
    "                # W are WeakL weights \n",
    "                # - Gradient ascent of return / gradient descent of negative return\n",
    "\n",
    "                # Set horizon ? -> number / proportion of attention at 1 per batch\n",
    "                # Set number of trajectories ? -> maybe make trajectories number vary based on sentence length\n",
    "                # gamma = 0.9 ? -> used to simulate temporal importance of reward (multiply each step by a certain power of gamma, furthest rewards are less impactful) -> may not be possible to model here\n",
    "                \n",
    "                horizon = 0.2\n",
    "                n_trajectories = 10\n",
    "                gamma = 0.9\n",
    "                attn_threshold = 0.01\n",
    "\n",
    "                mask = batch['tokens']['mask']\n",
    "        \n",
    "                prob_attn = attns\n",
    "                from torch.distributions import Binomial\n",
    "\n",
    "                m = Binomial(probs=prob_attn)\n",
    "                trajectory_scores = []#{i: [] for i in range(prob_attn.shape[-1])}\n",
    "\n",
    "                policy_loss = []\n",
    "                \n",
    "#                 all_samples = []\n",
    "                \n",
    "                for nb_traj in range(n_trajectories):\n",
    "                    attn_sample = m.sample()\n",
    "                    \n",
    "#                     all_samples.append(attn_sample)\n",
    "                    \n",
    "#                 all_samples = torch.stack(all_samples)\n",
    "                \n",
    "                    # logsum: probabilité qu'il y ait au moins une mention d'un type ? -> ensuite sigmoide\n",
    "                    # d'abord agréger puis calculer la loss\n",
    "\n",
    "                    # multi label: itération ? têtes d'attention ? tags\n",
    "\n",
    "                    # match cui avec cui le plus probable -> \n",
    "\n",
    "                    real_tokens = [np.array(b.fields['tokens'].tokens) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "    #                     gold_labels = [np.array(b.fields['tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "                    gold_norm_labels = [np.array(b.fields['chunk_tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "\n",
    "                    # unique_mask: n_class * batch_size * nb_traj * seq_len\n",
    "                    attn_mask = attn_sample\n",
    "\n",
    "                    from datetime import datetime\n",
    "                    now = datetime.now\n",
    "                    \n",
    "                    dnow = now()\n",
    "#                     for class_n in range(attn_sample.shape[-1]):\n",
    "                        \n",
    "#                     masked_tokens = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]\n",
    "#                                      if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]] \n",
    "#                                      for w_id, rt in enumerate(real_tokens)] # weird behaviour for len == 1\n",
    "# #                     masked_gold = [rt[attn_mask[w_id,:len(rt)].cpu().to(bool)] for w_id, rt in enumerate(gold_labels)]\n",
    "#                     masked_gold_norm = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)] \n",
    "#                                         if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]]\n",
    "#                                         for w_id, rt in enumerate(gold_norm_labels)]\n",
    "\n",
    "                    masked_tokens = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]\n",
    "                                     if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]] \n",
    "                                     for w_id, rt in enumerate(real_tokens) for class_n in range(attn_sample.shape[-1])] # weird behaviour for len == 1\n",
    "#                     masked_gold = [rt[attn_mask[w_id,:len(rt)].cpu().to(bool)] for w_id, rt in enumerate(gold_labels)]\n",
    "                    masked_gold_norm = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)] \n",
    "                                        if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]]\n",
    "                                        for w_id, rt in enumerate(gold_norm_labels) for class_n in range(attn_sample.shape[-1])]\n",
    "    \n",
    "                    save_to_ann(masked_tokens, masked_gold_norm, '/home/ytaille/data/tmp/ws_inputs/')\n",
    "\n",
    "                    # NLSTRUCT PART\n",
    "\n",
    "                    bert_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "                    dataset = load_from_brat(\"/home/ytaille/data/tmp/ws_inputs/\")\n",
    "\n",
    "                    if len(dataset['mentions']) == 0:\n",
    "                        continue\n",
    "\n",
    "                    dataset['mentions']['mention_id'] = dataset['mentions']['doc_id'] +'.'+ dataset['mentions']['mention_id'].astype(str)\n",
    "\n",
    "                    batcher, vocs, mention_ids = preprocess_train(\n",
    "                        dataset,\n",
    "                        vocabularies=self.vocabularies1,\n",
    "                        bert_name=bert_name,\n",
    "                    )\n",
    "\n",
    "                    batch_size = len(batcher)\n",
    "                    with_tqdm = True\n",
    "\n",
    "                    tg.set_device('cuda:0')#('cuda:0')\n",
    "                    device = tg.device\n",
    "\n",
    "                    pred_batcher = predict(batcher, self.classifier1)\n",
    "\n",
    "                    scores = compute_scores(pred_batcher, batcher)\n",
    "\n",
    "                    try:\n",
    "                        trajectory_scores.append((scores['loss'] * prob_attn).mean())\n",
    "                    except:\n",
    "                        print(trajectory_scores)\n",
    "                        raise\n",
    "\n",
    "                cpt_batch += batch_len\n",
    "\n",
    "#                 if any(len(tj) > 0 for tj in trajectory_scores.values()):\n",
    "#                     trajectory_scores = [t for tj in trajectory_scores.values() for t in tj]\n",
    "#                 else: policy_loss = 0\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            loss = self._batch_loss(batch, for_training=True) + sum(trajectory_scores) # policy_loss \n",
    "            loss.backward()\n",
    "\n",
    "            # Make sure Variable is on the cpu before converting to numpy.\n",
    "            # .cpu() is a no-op if you aren't using GPUs.\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            batch_grad_norm = self._rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "            if hasattr(self, \"_tf_params\") and self._tf_params is not None:\n",
    "                # We have TF logging\n",
    "                if self._batch_num_total % self._tf_params[\"log_every\"] == 0:\n",
    "                    self._tf_log(metrics, self._batch_num_total)\n",
    "\n",
    "        return self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "    \n",
    "import functools\n",
    "\n",
    "trainer._train_epoch = functools.partial(_train_epoch, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Training Done.\")\n",
    "if instances_test is not None:\n",
    "    logger.info(\"Computing final Test Accuracy\")\n",
    "    trainer.test(instances_test)\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yt_nlp",
   "language": "python",
   "name": "yt_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
