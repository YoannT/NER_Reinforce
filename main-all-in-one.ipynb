{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlstruct.dataloaders.medic import get_raw_medic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/sklearn/utils/linear_assignment_.py:22: FutureWarning: The linear_assignment_ module is deprecated in 0.21 and will be removed from 0.23. Use scipy.optimize.linear_sum_assignment instead.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:21,017: INFO: train_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-08 11:07:21,019: INFO: Loading Training Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n",
      "2021-03-08 11:07:21,022: INFO: dataset_reader.type = WeakConll2003DatasetReader\n",
      "2021-03-08 11:07:21,026: INFO: dataset_reader.token_indexers.bert.type = bert-pretrained\n",
      "2021-03-08 11:07:21,028: INFO: dataset_reader.token_indexers.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-08 11:07:21,029: INFO: dataset_reader.token_indexers.bert.use_starting_offsets = True\n",
      "2021-03-08 11:07:21,030: INFO: dataset_reader.token_indexers.bert.do_lowercase = False\n",
      "2021-03-08 11:07:21,031: INFO: dataset_reader.token_indexers.bert.never_lowercase = None\n",
      "2021-03-08 11:07:21,031: INFO: dataset_reader.token_indexers.bert.max_pieces = 512\n",
      "2021-03-08 11:07:21,034: INFO: loading vocabulary file ./Data/embeddings/bert-base-multilingual-cased-vocab.txt\n",
      "2021-03-08 11:07:21,174: INFO: dataset_reader.tag_label = ner\n",
      "2021-03-08 11:07:21,175: INFO: dataset_reader.feature_labels = ['chunk']\n",
      "2021-03-08 11:07:21,176: INFO: dataset_reader.lazy = False\n",
      "2021-03-08 11:07:21,177: INFO: dataset_reader.coding_scheme = IOB1\n",
      "2021-03-08 11:07:21,179: INFO: dataset_reader.label_indexer.label_namespace = labels\n",
      "2021-03-08 11:07:21,180: INFO: dataset_reader.label_indexer.tags = ['DiseaseClass', 'SpecificDisease', 'CompositeMention', 'Modifier']\n",
      "2021-03-08 11:07:21,182: INFO: dataset_reader.convert_numbers = True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:21,187: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_train.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1803it [00:01, 1717.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:22,237: INFO: Length of Training Data: 1803\n",
      "2021-03-08 11:07:22,239: INFO: validation_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n",
      "2021-03-08 11:07:22,240: INFO: Loading Validation Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:22,244: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_dev.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "319it [00:00, 1997.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:22,405: INFO: Length of Validation Data: 319\n",
      "2021-03-08 11:07:22,406: INFO: test_data_path = /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n",
      "2021-03-08 11:07:22,407: INFO: Loading Test Data from /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:22,411: INFO: Reading instances from lines in file at: /home/ytaille/data/resources/medic/ncbi_conll_ner_test.conll\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "316it [00:00, 1198.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:22,675: INFO: Length of Testing Data: 316\n",
      "2021-03-08 11:07:22,677: INFO: max_vocab_size = -1\n",
      "2021-03-08 11:07:22,678: INFO: Constructing Vocab of size: -1\n",
      "2021-03-08 11:07:22,680: INFO: Fitting token dictionary from dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1803/1803 [00:00<00:00, 37312.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:22,742: INFO: Saving vocab to ./trained_models/NCBI-BERT-realFT-PS/run-180/vocab\n",
      "2021-03-08 11:07:22,743: INFO: Vocab Construction Done\n",
      "2021-03-08 11:07:22,745: INFO: Constructing Data Iterators\n",
      "2021-03-08 11:07:22,746: INFO: iterator.type = bucket\n",
      "2021-03-08 11:07:22,747: INFO: iterator.sorting_keys = [['tokens', 'bert']]\n",
      "2021-03-08 11:07:22,749: INFO: iterator.padding_noise = 0.1\n",
      "2021-03-08 11:07:22,750: INFO: iterator.biggest_batch_first = False\n",
      "2021-03-08 11:07:22,751: INFO: iterator.batch_size = 32\n",
      "2021-03-08 11:07:22,751: INFO: iterator.instances_per_epoch = None\n",
      "2021-03-08 11:07:22,752: INFO: iterator.max_instances_in_memory = None\n",
      "2021-03-08 11:07:22,753: INFO: Data Iterators Done\n",
      "2021-03-08 11:07:22,753: INFO: Constructing The model\n",
      "2021-03-08 11:07:22,755: INFO: model.type = MultiClassifier\n",
      "2021-03-08 11:07:22,756: INFO: model.method = binary\n",
      "2021-03-08 11:07:22,757: INFO: model.text_field_embedder.type = basic\n",
      "2021-03-08 11:07:22,758: INFO: model.text_field_embedder.allow_unmatched_keys = True\n",
      "2021-03-08 11:07:22,759: INFO: model.text_field_embedder.token_embedders.bert.type = bert-pretrained\n",
      "2021-03-08 11:07:22,760: INFO: model.text_field_embedder.token_embedders.bert.pretrained_model = ./Data/embeddings/bert-base-multilingual-cased.tar.gz\n",
      "2021-03-08 11:07:22,760: INFO: model.text_field_embedder.token_embedders.bert.requires_grad = [10, 11]\n",
      "2021-03-08 11:07:22,761: INFO: model.text_field_embedder.token_embedders.bert.top_layer_only = False\n",
      "2021-03-08 11:07:22,768: INFO: loading archive file ./Data/embeddings/bert-base-multilingual-cased.tar.gz\n",
      "2021-03-08 11:07:22,769: INFO: extracting archive file ./Data/embeddings/bert-base-multilingual-cased.tar.gz to temp dir /tmp/tmpfp2960ri\n",
      "2021-03-08 11:07:27,853: INFO: Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 119547\n",
      "}\n",
      "\n",
      "2021-03-08 11:07:36,384: INFO: Layer encoder.layer.10.attention.self.query.weight is finetuned\n",
      "2021-03-08 11:07:36,386: INFO: Layer encoder.layer.10.attention.self.query.bias is finetuned\n",
      "2021-03-08 11:07:36,387: INFO: Layer encoder.layer.10.attention.self.key.weight is finetuned\n",
      "2021-03-08 11:07:36,387: INFO: Layer encoder.layer.10.attention.self.key.bias is finetuned\n",
      "2021-03-08 11:07:36,388: INFO: Layer encoder.layer.10.attention.self.value.weight is finetuned\n",
      "2021-03-08 11:07:36,389: INFO: Layer encoder.layer.10.attention.self.value.bias is finetuned\n",
      "2021-03-08 11:07:36,389: INFO: Layer encoder.layer.10.attention.output.dense.weight is finetuned\n",
      "2021-03-08 11:07:36,390: INFO: Layer encoder.layer.10.attention.output.dense.bias is finetuned\n",
      "2021-03-08 11:07:36,391: INFO: Layer encoder.layer.10.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-08 11:07:36,391: INFO: Layer encoder.layer.10.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-08 11:07:36,392: INFO: Layer encoder.layer.10.intermediate.dense.weight is finetuned\n",
      "2021-03-08 11:07:36,392: INFO: Layer encoder.layer.10.intermediate.dense.bias is finetuned\n",
      "2021-03-08 11:07:36,393: INFO: Layer encoder.layer.10.output.dense.weight is finetuned\n",
      "2021-03-08 11:07:36,393: INFO: Layer encoder.layer.10.output.dense.bias is finetuned\n",
      "2021-03-08 11:07:36,394: INFO: Layer encoder.layer.10.output.LayerNorm.weight is finetuned\n",
      "2021-03-08 11:07:36,394: INFO: Layer encoder.layer.10.output.LayerNorm.bias is finetuned\n",
      "2021-03-08 11:07:36,394: INFO: Layer encoder.layer.11.attention.self.query.weight is finetuned\n",
      "2021-03-08 11:07:36,395: INFO: Layer encoder.layer.11.attention.self.query.bias is finetuned\n",
      "2021-03-08 11:07:36,395: INFO: Layer encoder.layer.11.attention.self.key.weight is finetuned\n",
      "2021-03-08 11:07:36,396: INFO: Layer encoder.layer.11.attention.self.key.bias is finetuned\n",
      "2021-03-08 11:07:36,396: INFO: Layer encoder.layer.11.attention.self.value.weight is finetuned\n",
      "2021-03-08 11:07:36,400: INFO: Layer encoder.layer.11.attention.self.value.bias is finetuned\n",
      "2021-03-08 11:07:36,400: INFO: Layer encoder.layer.11.attention.output.dense.weight is finetuned\n",
      "2021-03-08 11:07:36,401: INFO: Layer encoder.layer.11.attention.output.dense.bias is finetuned\n",
      "2021-03-08 11:07:36,401: INFO: Layer encoder.layer.11.attention.output.LayerNorm.weight is finetuned\n",
      "2021-03-08 11:07:36,402: INFO: Layer encoder.layer.11.attention.output.LayerNorm.bias is finetuned\n",
      "2021-03-08 11:07:36,402: INFO: Layer encoder.layer.11.intermediate.dense.weight is finetuned\n",
      "2021-03-08 11:07:36,403: INFO: Layer encoder.layer.11.intermediate.dense.bias is finetuned\n",
      "2021-03-08 11:07:36,403: INFO: Layer encoder.layer.11.output.dense.weight is finetuned\n",
      "2021-03-08 11:07:36,403: INFO: Layer encoder.layer.11.output.dense.bias is finetuned\n",
      "2021-03-08 11:07:36,404: INFO: Layer encoder.layer.11.output.LayerNorm.weight is finetuned\n",
      "2021-03-08 11:07:36,404: INFO: Layer encoder.layer.11.output.LayerNorm.bias is finetuned\n",
      "2021-03-08 11:07:36,404: INFO: Layer pooler.dense.weight is finetuned\n",
      "2021-03-08 11:07:36,405: INFO: Layer pooler.dense.bias is finetuned\n",
      "2021-03-08 11:07:36,406: INFO: model.encoder_word.type = gru\n",
      "2021-03-08 11:07:36,408: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-08 11:07:36,409: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-08 11:07:36,409: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-08 11:07:36,409: INFO: model.encoder_word.bidirectional = True\n",
      "2021-03-08 11:07:36,410: INFO: model.encoder_word.dropout = 0.5\n",
      "2021-03-08 11:07:36,410: INFO: model.encoder_word.hidden_size = 150\n",
      "2021-03-08 11:07:36,411: INFO: model.encoder_word.input_size = 768\n",
      "2021-03-08 11:07:36,411: INFO: model.encoder_word.num_layers = 1\n",
      "2021-03-08 11:07:36,411: INFO: model.encoder_word.batch_first = True\n",
      "2021-03-08 11:07:36,420: INFO: model.attention_word.type = KeyedAttention\n",
      "2021-03-08 11:07:36,421: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-08 11:07:36,422: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-08 11:07:36,422: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-08 11:07:36,422: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-08 11:07:36,423: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-08 11:07:36,425: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-08 11:07:36,426: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-08 11:07:36,426: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-08 11:07:36,427: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-08 11:07:36,427: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-08 11:07:36,430: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-08 11:07:36,430: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-08 11:07:36,431: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-08 11:07:36,431: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-08 11:07:36,431: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-08 11:07:36,434: INFO: model.attention_word.key_emb_size = 300\n",
      "2021-03-08 11:07:36,434: INFO: model.attention_word.ctxt_emb_size = 300\n",
      "2021-03-08 11:07:36,434: INFO: model.attention_word.attn_type = sum\n",
      "2021-03-08 11:07:36,435: INFO: model.attention_word.dropout = 0.0\n",
      "2021-03-08 11:07:36,435: INFO: model.attention_word.temperature = 1.0\n",
      "INIT BaseAttention \n",
      "INIT KEYED ATTENTION\n",
      "2021-03-08 11:07:36,436: INFO: model.threshold = 0.2\n",
      "2021-03-08 11:07:36,437: INFO: model.initializer = []\n",
      "2021-03-08 11:07:36,437: INFO: model.regularizer = []\n",
      "INIT ClassificationMetrics\n",
      "2021-03-08 11:07:36,439: INFO: Initializing parameters\n",
      "2021-03-08 11:07:36,440: INFO: Done initializing parameters; the following parameters are using their default initialization from their code\n",
      "2021-03-08 11:07:36,440: INFO:    attn_CompositeMention.key\n",
      "2021-03-08 11:07:36,441: INFO:    attn_CompositeMention.proj_ctxt.bias\n",
      "2021-03-08 11:07:36,441: INFO:    attn_CompositeMention.proj_ctxt.weight\n",
      "2021-03-08 11:07:36,441: INFO:    attn_CompositeMention.proj_ctxt_key_matrix.bias\n",
      "2021-03-08 11:07:36,442: INFO:    attn_CompositeMention.proj_ctxt_key_matrix.weight\n",
      "2021-03-08 11:07:36,442: INFO:    attn_DiseaseClass.key\n",
      "2021-03-08 11:07:36,442: INFO:    attn_DiseaseClass.proj_ctxt.bias\n",
      "2021-03-08 11:07:36,442: INFO:    attn_DiseaseClass.proj_ctxt.weight\n",
      "2021-03-08 11:07:36,443: INFO:    attn_DiseaseClass.proj_ctxt_key_matrix.bias\n",
      "2021-03-08 11:07:36,443: INFO:    attn_DiseaseClass.proj_ctxt_key_matrix.weight\n",
      "2021-03-08 11:07:36,443: INFO:    attn_Modifier.key\n",
      "2021-03-08 11:07:36,444: INFO:    attn_Modifier.proj_ctxt.bias\n",
      "2021-03-08 11:07:36,444: INFO:    attn_Modifier.proj_ctxt.weight\n",
      "2021-03-08 11:07:36,444: INFO:    attn_Modifier.proj_ctxt_key_matrix.bias\n",
      "2021-03-08 11:07:36,445: INFO:    attn_Modifier.proj_ctxt_key_matrix.weight\n",
      "2021-03-08 11:07:36,445: INFO:    attn_SpecificDisease.key\n",
      "2021-03-08 11:07:36,445: INFO:    attn_SpecificDisease.proj_ctxt.bias\n",
      "2021-03-08 11:07:36,446: INFO:    attn_SpecificDisease.proj_ctxt.weight\n",
      "2021-03-08 11:07:36,446: INFO:    attn_SpecificDisease.proj_ctxt_key_matrix.bias\n",
      "2021-03-08 11:07:36,446: INFO:    attn_SpecificDisease.proj_ctxt_key_matrix.weight\n",
      "2021-03-08 11:07:36,447: INFO:    encoder_word._module.bias_hh_l0\n",
      "2021-03-08 11:07:36,447: INFO:    encoder_word._module.bias_hh_l0_reverse\n",
      "2021-03-08 11:07:36,447: INFO:    encoder_word._module.bias_ih_l0\n",
      "2021-03-08 11:07:36,448: INFO:    encoder_word._module.bias_ih_l0_reverse\n",
      "2021-03-08 11:07:36,448: INFO:    encoder_word._module.weight_hh_l0\n",
      "2021-03-08 11:07:36,448: INFO:    encoder_word._module.weight_hh_l0_reverse\n",
      "2021-03-08 11:07:36,449: INFO:    encoder_word._module.weight_ih_l0\n",
      "2021-03-08 11:07:36,449: INFO:    encoder_word._module.weight_ih_l0_reverse\n",
      "2021-03-08 11:07:36,449: INFO:    logits_layer_CompositeMention.bias\n",
      "2021-03-08 11:07:36,449: INFO:    logits_layer_CompositeMention.weight\n",
      "2021-03-08 11:07:36,450: INFO:    logits_layer_DiseaseClass.bias\n",
      "2021-03-08 11:07:36,450: INFO:    logits_layer_DiseaseClass.weight\n",
      "2021-03-08 11:07:36,450: INFO:    logits_layer_Modifier.bias\n",
      "2021-03-08 11:07:36,451: INFO:    logits_layer_Modifier.weight\n",
      "2021-03-08 11:07:36,451: INFO:    logits_layer_O.bias\n",
      "2021-03-08 11:07:36,451: INFO:    logits_layer_O.weight\n",
      "2021-03-08 11:07:36,452: INFO:    logits_layer_SpecificDisease.bias\n",
      "2021-03-08 11:07:36,452: INFO:    logits_layer_SpecificDisease.weight\n",
      "2021-03-08 11:07:36,452: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.gamma\n",
      "2021-03-08 11:07:36,453: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0\n",
      "2021-03-08 11:07:36,453: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1\n",
      "2021-03-08 11:07:36,453: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10\n",
      "2021-03-08 11:07:36,454: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11\n",
      "2021-03-08 11:07:36,454: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2\n",
      "2021-03-08 11:07:36,454: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3\n",
      "2021-03-08 11:07:36,455: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4\n",
      "2021-03-08 11:07:36,455: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5\n",
      "2021-03-08 11:07:36,455: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6\n",
      "2021-03-08 11:07:36,456: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7\n",
      "2021-03-08 11:07:36,456: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8\n",
      "2021-03-08 11:07:36,456: INFO:    text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9\n",
      "2021-03-08 11:07:36,456: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.bias\n",
      "2021-03-08 11:07:36,457: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.LayerNorm.weight\n",
      "2021-03-08 11:07:36,457: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.position_embeddings.weight\n",
      "2021-03-08 11:07:36,457: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.token_type_embeddings.weight\n",
      "2021-03-08 11:07:36,458: INFO:    text_field_embedder.token_embedder_bert.bert_model.embeddings.word_embeddings.weight\n",
      "2021-03-08 11:07:36,458: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,458: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,459: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,459: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,459: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.bias\n",
      "2021-03-08 11:07:36,460: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.key.weight\n",
      "2021-03-08 11:07:36,460: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.bias\n",
      "2021-03-08 11:07:36,460: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.query.weight\n",
      "2021-03-08 11:07:36,461: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.bias\n",
      "2021-03-08 11:07:36,461: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.attention.self.value.weight\n",
      "2021-03-08 11:07:36,461: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,462: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,462: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,462: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,462: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.bias\n",
      "2021-03-08 11:07:36,463: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.0.output.dense.weight\n",
      "2021-03-08 11:07:36,463: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,463: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,464: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,464: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,464: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.bias\n",
      "2021-03-08 11:07:36,465: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.key.weight\n",
      "2021-03-08 11:07:36,465: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.bias\n",
      "2021-03-08 11:07:36,465: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.query.weight\n",
      "2021-03-08 11:07:36,466: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.bias\n",
      "2021-03-08 11:07:36,466: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.attention.self.value.weight\n",
      "2021-03-08 11:07:36,466: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,467: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,467: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,467: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,467: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.bias\n",
      "2021-03-08 11:07:36,468: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.1.output.dense.weight\n",
      "2021-03-08 11:07:36,468: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,468: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,469: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,469: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,469: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias\n",
      "2021-03-08 11:07:36,470: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight\n",
      "2021-03-08 11:07:36,470: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias\n",
      "2021-03-08 11:07:36,470: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight\n",
      "2021-03-08 11:07:36,471: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias\n",
      "2021-03-08 11:07:36,471: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight\n",
      "2021-03-08 11:07:36,471: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,472: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,472: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,472: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,472: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias\n",
      "2021-03-08 11:07:36,473: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight\n",
      "2021-03-08 11:07:36,473: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,473: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,474: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,474: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,474: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias\n",
      "2021-03-08 11:07:36,475: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight\n",
      "2021-03-08 11:07:36,475: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias\n",
      "2021-03-08 11:07:36,475: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight\n",
      "2021-03-08 11:07:36,476: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias\n",
      "2021-03-08 11:07:36,476: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight\n",
      "2021-03-08 11:07:36,476: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,477: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,477: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,477: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,478: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias\n",
      "2021-03-08 11:07:36,478: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight\n",
      "2021-03-08 11:07:36,478: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,479: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,479: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,479: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,479: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.bias\n",
      "2021-03-08 11:07:36,480: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.key.weight\n",
      "2021-03-08 11:07:36,480: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.bias\n",
      "2021-03-08 11:07:36,480: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.query.weight\n",
      "2021-03-08 11:07:36,481: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.bias\n",
      "2021-03-08 11:07:36,481: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.attention.self.value.weight\n",
      "2021-03-08 11:07:36,481: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,482: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,482: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,482: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,483: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.bias\n",
      "2021-03-08 11:07:36,483: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.2.output.dense.weight\n",
      "2021-03-08 11:07:36,483: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,484: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,484: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,484: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,485: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.bias\n",
      "2021-03-08 11:07:36,485: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.key.weight\n",
      "2021-03-08 11:07:36,485: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.bias\n",
      "2021-03-08 11:07:36,485: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.query.weight\n",
      "2021-03-08 11:07:36,486: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.bias\n",
      "2021-03-08 11:07:36,486: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.attention.self.value.weight\n",
      "2021-03-08 11:07:36,486: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,487: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,487: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,487: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,488: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.bias\n",
      "2021-03-08 11:07:36,488: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.3.output.dense.weight\n",
      "2021-03-08 11:07:36,488: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,489: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,489: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,489: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,490: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.bias\n",
      "2021-03-08 11:07:36,490: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.key.weight\n",
      "2021-03-08 11:07:36,490: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.bias\n",
      "2021-03-08 11:07:36,490: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.query.weight\n",
      "2021-03-08 11:07:36,491: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.bias\n",
      "2021-03-08 11:07:36,491: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.attention.self.value.weight\n",
      "2021-03-08 11:07:36,491: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,492: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,492: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,492: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,493: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.bias\n",
      "2021-03-08 11:07:36,493: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.4.output.dense.weight\n",
      "2021-03-08 11:07:36,493: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,494: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,494: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,494: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,495: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.bias\n",
      "2021-03-08 11:07:36,495: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.key.weight\n",
      "2021-03-08 11:07:36,495: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.bias\n",
      "2021-03-08 11:07:36,496: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.query.weight\n",
      "2021-03-08 11:07:36,496: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.bias\n",
      "2021-03-08 11:07:36,496: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.attention.self.value.weight\n",
      "2021-03-08 11:07:36,496: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,497: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,497: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,497: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,498: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.bias\n",
      "2021-03-08 11:07:36,498: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.5.output.dense.weight\n",
      "2021-03-08 11:07:36,498: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,499: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,499: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,499: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,500: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.bias\n",
      "2021-03-08 11:07:36,500: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.key.weight\n",
      "2021-03-08 11:07:36,500: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.bias\n",
      "2021-03-08 11:07:36,501: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.query.weight\n",
      "2021-03-08 11:07:36,501: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.bias\n",
      "2021-03-08 11:07:36,501: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.attention.self.value.weight\n",
      "2021-03-08 11:07:36,501: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,502: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,502: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,502: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,503: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.bias\n",
      "2021-03-08 11:07:36,503: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.6.output.dense.weight\n",
      "2021-03-08 11:07:36,503: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,504: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,504: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,504: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,505: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.bias\n",
      "2021-03-08 11:07:36,505: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.key.weight\n",
      "2021-03-08 11:07:36,505: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.bias\n",
      "2021-03-08 11:07:36,506: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.query.weight\n",
      "2021-03-08 11:07:36,506: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.bias\n",
      "2021-03-08 11:07:36,506: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.attention.self.value.weight\n",
      "2021-03-08 11:07:36,507: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,507: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,507: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,507: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,508: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.bias\n",
      "2021-03-08 11:07:36,508: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.7.output.dense.weight\n",
      "2021-03-08 11:07:36,508: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,509: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,509: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,509: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,510: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.bias\n",
      "2021-03-08 11:07:36,510: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.key.weight\n",
      "2021-03-08 11:07:36,510: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.bias\n",
      "2021-03-08 11:07:36,511: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.query.weight\n",
      "2021-03-08 11:07:36,511: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.bias\n",
      "2021-03-08 11:07:36,511: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.attention.self.value.weight\n",
      "2021-03-08 11:07:36,512: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,512: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,512: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,513: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,513: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.bias\n",
      "2021-03-08 11:07:36,513: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.8.output.dense.weight\n",
      "2021-03-08 11:07:36,514: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,514: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,514: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.bias\n",
      "2021-03-08 11:07:36,515: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.output.dense.weight\n",
      "2021-03-08 11:07:36,515: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.bias\n",
      "2021-03-08 11:07:36,515: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.key.weight\n",
      "2021-03-08 11:07:36,515: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.bias\n",
      "2021-03-08 11:07:36,516: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.query.weight\n",
      "2021-03-08 11:07:36,516: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.bias\n",
      "2021-03-08 11:07:36,516: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.attention.self.value.weight\n",
      "2021-03-08 11:07:36,517: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.bias\n",
      "2021-03-08 11:07:36,517: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.intermediate.dense.weight\n",
      "2021-03-08 11:07:36,517: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.bias\n",
      "2021-03-08 11:07:36,518: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.LayerNorm.weight\n",
      "2021-03-08 11:07:36,518: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.bias\n",
      "2021-03-08 11:07:36,518: INFO:    text_field_embedder.token_embedder_bert.bert_model.encoder.layer.9.output.dense.weight\n",
      "2021-03-08 11:07:36,519: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias\n",
      "2021-03-08 11:07:36,519: INFO:    text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight\n",
      "INIT MULTICLASSIFIER\n",
      "2021-03-08 11:07:36,520: INFO: Model Construction done\n",
      "2021-03-08 11:07:36,520: INFO: segmentation.type = SymbolStopwordFilteredMultiPredictions\n",
      "2021-03-08 11:07:36,521: INFO: segmentation.tol = 0.05\n",
      "2021-03-08 11:07:36,521: INFO: segmentation.visualize = True\n",
      "2021-03-08 11:07:36,522: INFO: segmentation.use_probs = True\n",
      "INIT BASE PRED CLASS\n",
      "INIT BASIC MULTI PREDICTIONS\n",
      "INIT SymbolStopwordFilteredMultiPredictions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/nn/modules/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import pdb\n",
    "\n",
    "sys.path.insert(0,'/home/ytaille/AttentionSegmentation')\n",
    "\n",
    "from allennlp.data import Vocabulary\n",
    "from allennlp.data.iterators import DataIterator\n",
    "# import allennlp.data.dataset_readers as Readers\n",
    "import AttentionSegmentation.reader as Readers\n",
    "\n",
    "# import model as Models\n",
    "import AttentionSegmentation.model.classifiers as Models\n",
    "\n",
    "from AttentionSegmentation.commons.utils import \\\n",
    "    setup_output_dir, read_from_config_file\n",
    "from AttentionSegmentation.commons.model_utils import \\\n",
    "    construct_vocab, load_model_from_existing\n",
    "# from AttentionSegmentation.visualization.visualize_attns import \\\n",
    "#     html_visualizer\n",
    "import AttentionSegmentation.model.attn2labels as SegmentationModels\n",
    "\n",
    "\"\"\"The main entry point\n",
    "\n",
    "This is the main entry point for training HAN SOLO models.\n",
    "\n",
    "Usage::\n",
    "\n",
    "    ${PYTHONPATH} -m AttentionSegmentation/main\n",
    "        --config_file ${CONFIG_FILE}\n",
    "\n",
    "\"\"\"\n",
    "args = type('MyClass', (object,), {'content':{}})()\n",
    "args.config_file = 'Configs/config_ncbi.json'\n",
    "args.log = 'INFO'\n",
    "args.loglevel = 'INFO'\n",
    "args.seed = 1\n",
    "\n",
    "# Setup Experiment Directory\n",
    "config = read_from_config_file(args.config_file)\n",
    "if args.seed > 0:\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if config.get('trainer', None) is not None and \\\n",
    "       config.get('trainer', None).get('cuda_device', -1) > 0:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "serial_dir, config = setup_output_dir(config, args.loglevel)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load Training Data\n",
    "TRAIN_PATH = config.pop(\"train_data_path\")\n",
    "logger.info(\"Loading Training Data from {0}\".format(TRAIN_PATH))\n",
    "dataset_reader_params = config.pop(\"dataset_reader\")\n",
    "reader_type = dataset_reader_params.pop(\"type\", None)\n",
    "assert reader_type is not None and hasattr(Readers, reader_type),\\\n",
    "    f\"Cannot find reader {reader_type}\"\n",
    "reader = getattr(Readers, reader_type).from_params(dataset_reader_params)\n",
    "instances_train = reader.read(file_path=TRAIN_PATH)\n",
    "instances_train = instances_train\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Training Data\", len(instances_train)))\n",
    "\n",
    "# Load Validation Data\n",
    "VAL_PATH = config.pop(\"validation_data_path\")\n",
    "logger.info(\"Loading Validation Data from {0}\".format(VAL_PATH))\n",
    "instances_val = reader.read(VAL_PATH)\n",
    "instances_val = instances_val\n",
    "logger.info(\"Length of {0}: {1}\".format(\n",
    "    \"Validation Data\", len(instances_val)))\n",
    "\n",
    "# Load Test Data\n",
    "TEST_PATH = config.pop(\"test_data_path\", None)\n",
    "instances_test = None\n",
    "if TEST_PATH is not None:\n",
    "    logger.info(\"Loading Test Data from {0}\".format(TEST_PATH))\n",
    "    instances_test = reader.read(TEST_PATH)\n",
    "    instances_test = instances_test\n",
    "    logger.info(\"Length of {0}: {1}\".format(\n",
    "        \"Testing Data\", len(instances_test)))\n",
    "\n",
    "# # Load Pretrained Existing Model\n",
    "# load_config = config.pop(\"load_from\", None)\n",
    "\n",
    "# # Construct Vocabulary\n",
    "vocab_size = config.pop(\"max_vocab_size\", -1)\n",
    "logger.info(\"Constructing Vocab of size: {0}\".format(vocab_size))\n",
    "vocab_size = None if vocab_size == -1 else vocab_size\n",
    "vocab = Vocabulary.from_instances(instances_train,\n",
    "                                  max_vocab_size=vocab_size)\n",
    "vocab_dir = os.path.join(serial_dir, \"vocab\")\n",
    "assert os.path.exists(vocab_dir), \"Couldn't find the vocab directory\"\n",
    "vocab.save_to_files(vocab_dir)\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # modify the vocab from the source model vocab\n",
    "#     src_vocab_path = load_config.pop(\"vocab_path\", None)\n",
    "#     if src_vocab_path is not None:\n",
    "#         vocab = construct_vocab(src_vocab_path, vocab_dir)\n",
    "#         # Delete the old vocab\n",
    "#         for file in os.listdir(vocab_dir):\n",
    "#             os.remove(os.path.join(vocab_dir, file))\n",
    "#         # save the new vocab\n",
    "#         vocab.save_to_files(vocab_dir)\n",
    "logger.info(\"Saving vocab to {0}\".format(vocab_dir))\n",
    "logger.info(\"Vocab Construction Done\")\n",
    "\n",
    "# # Construct the data iterators\n",
    "logger.info(\"Constructing Data Iterators\")\n",
    "data_iterator = DataIterator.from_params(config.pop(\"iterator\"))\n",
    "data_iterator.index_with(vocab)\n",
    "\n",
    "logger.info(\"Data Iterators Done\")\n",
    "\n",
    "# Create the model\n",
    "logger.info(\"Constructing The model\")\n",
    "model_params = config.pop(\"model\")\n",
    "model_type = model_params.pop(\"type\")\n",
    "assert model_type is not None and hasattr(Models, model_type),\\\n",
    "    f\"Cannot find reader {model_type}\"\n",
    "model = getattr(Models, model_type).from_params(\n",
    "    vocab=vocab,\n",
    "    params=model_params,\n",
    "    label_indexer=reader.get_label_indexer()\n",
    ")\n",
    "logger.info(\"Model Construction done\")\n",
    "\n",
    "# visualize = config.pop(\"visualize\", False)\n",
    "# visualizer = None\n",
    "# if visualize:\n",
    "#     visualizer = html_visualizer(vocab, reader)\n",
    "segmenter_params = config.pop(\"segmentation\")\n",
    "segment_class = segmenter_params.pop(\"type\")\n",
    "segmenter = getattr(SegmentationModels, segment_class).from_params(\n",
    "    vocab=vocab,\n",
    "    reader=reader,\n",
    "    params=segmenter_params\n",
    ")\n",
    "\n",
    "# logger.info(\"Segmenter Done\")\n",
    "\n",
    "# print(\"##################################\\nAYYYYYYYYYYYYYYYYYYYYYYYY\\n\\n\\n\\n\\n\\n\\n\\n###########################\")\n",
    "\n",
    "# exit()\n",
    "\n",
    "\n",
    "# if load_config is not None:\n",
    "#     # Load the weights, as specified by the load_config\n",
    "#     model_path = load_config.pop(\"model_path\", None)\n",
    "#     layers = load_config.pop(\"layers\", None)\n",
    "#     load_config.assert_empty(\"Load Config\")\n",
    "#     assert model_path is not None,\\\n",
    "#         \"You need to specify model path to load from\"\n",
    "#     model = load_model_from_existing(model_path, model, layers)\n",
    "#     logger.info(\"Pretrained weights loaded\")\n",
    "\n",
    "# logger.info(\"Starting the training process\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Necessary to add unknown tag to dictionnary to avoid errors later\n",
    "data_iterator.vocab.add_token_to_namespace(\"@@UNKNOWN@@\", \"chunk_tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_from_config_file(args.config_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:07:36,816: INFO: PyTorch version 1.5.1 available.\n",
      "2021-03-08 11:07:40,103: INFO: TensorFlow version 2.3.1 available.\n",
      "2021-03-08 11:07:40,598: INFO: Loading faiss with AVX2 support.\n",
      "2021-03-08 11:07:40,602: INFO: Loading faiss.\n",
      "2021-03-08 11:07:41,051: INFO: trainer.patience = 10\n",
      "2021-03-08 11:07:41,053: INFO: trainer.validation_metric = +accuracy\n",
      "2021-03-08 11:07:41,054: INFO: trainer.num_epochs = 50\n",
      "2021-03-08 11:07:41,055: INFO: trainer.cuda_device = 0\n",
      "2021-03-08 11:07:41,056: INFO: trainer.grad_norm = None\n",
      "2021-03-08 11:07:41,057: INFO: trainer.grad_clipping = None\n",
      "2021-03-08 11:07:41,059: INFO: trainer.num_serialized_models_to_keep = 1\n",
      "2021-03-08 11:07:43,769: INFO: trainer.optimizer.type = adam\n",
      "2021-03-08 11:07:43,771: INFO: trainer.optimizer.parameter_groups = [[['.*bert.*'], ConfigTree([('lr', 2e-07)])], [['.*encoder_word.*', '.*attn.*', '.*logit.*'], ConfigTree([('lr', 0.001)])]]\n",
      "2021-03-08 11:07:43,773: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-08 11:07:43,774: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-08 11:07:43,775: INFO: trainer.optimizer.parameter_groups.list.list.lr = 2e-07\n",
      "2021-03-08 11:07:43,775: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-08 11:07:43,776: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-08 11:07:43,777: INFO: trainer.optimizer.parameter_groups.list.list.lr = 0.001\n",
      "2021-03-08 11:07:43,781: INFO: Done constructing parameter groups.\n",
      "2021-03-08 11:07:43,782: INFO: Group 0: ['text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.10', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.5', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.6', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.11', 'text_field_embedder.token_embedder_bert._scalar_mix.gamma', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.7', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.pooler.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.1', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.2', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.dense.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.value.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.query.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.3', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.weight', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.0', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.4', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.8', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.query.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.intermediate.dense.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert._scalar_mix.scalar_parameters.9', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.output.LayerNorm.bias', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.10.attention.self.key.weight', 'text_field_embedder.token_embedder_bert.bert_model.encoder.layer.11.attention.self.key.bias'], {'lr': 2e-07}\n",
      "2021-03-08 11:07:43,782: INFO: Group 1: ['encoder_word._module.bias_ih_l0_reverse', 'attn_Modifier.key', 'encoder_word._module.bias_hh_l0_reverse', 'logits_layer_O.weight', 'logits_layer_CompositeMention.weight', 'attn_CompositeMention.proj_ctxt.weight', 'encoder_word._module.weight_hh_l0', 'attn_SpecificDisease.proj_ctxt.weight', 'logits_layer_SpecificDisease.bias', 'attn_CompositeMention.proj_ctxt.bias', 'attn_CompositeMention.proj_ctxt_key_matrix.bias', 'attn_Modifier.proj_ctxt_key_matrix.bias', 'logits_layer_DiseaseClass.weight', 'logits_layer_Modifier.weight', 'attn_DiseaseClass.proj_ctxt_key_matrix.bias', 'logits_layer_SpecificDisease.weight', 'encoder_word._module.bias_ih_l0', 'encoder_word._module.bias_hh_l0', 'attn_SpecificDisease.key', 'attn_SpecificDisease.proj_ctxt_key_matrix.weight', 'logits_layer_CompositeMention.bias', 'attn_DiseaseClass.proj_ctxt.weight', 'attn_DiseaseClass.proj_ctxt_key_matrix.weight', 'attn_DiseaseClass.key', 'encoder_word._module.weight_ih_l0', 'attn_SpecificDisease.proj_ctxt_key_matrix.bias', 'encoder_word._module.weight_ih_l0_reverse', 'attn_SpecificDisease.proj_ctxt.bias', 'attn_Modifier.proj_ctxt_key_matrix.weight', 'attn_DiseaseClass.proj_ctxt.bias', 'logits_layer_Modifier.bias', 'logits_layer_DiseaseClass.bias', 'attn_CompositeMention.key', 'logits_layer_O.bias', 'encoder_word._module.weight_hh_l0_reverse', 'attn_CompositeMention.proj_ctxt_key_matrix.weight', 'attn_Modifier.proj_ctxt.weight', 'attn_Modifier.proj_ctxt.bias'], {'lr': 0.001}\n",
      "2021-03-08 11:07:43,783: INFO: Group 2: [], {}\n",
      "2021-03-08 11:07:43,784: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-08 11:07:43,784: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-08 11:07:43,785: INFO: trainer.learning_rate_scheduler.type = reduce_on_plateau\n",
      "2021-03-08 11:07:43,786: INFO: Converting Params object to dict; logging of default values will not occur when dictionary parameters are used subsequently.\n",
      "2021-03-08 11:07:43,786: INFO: CURRENTLY DEFINED PARAMETERS: \n",
      "2021-03-08 11:07:43,787: INFO: trainer.learning_rate_scheduler.factor = 0.5\n",
      "2021-03-08 11:07:43,787: INFO: trainer.learning_rate_scheduler.patience = 5\n",
      "2021-03-08 11:07:43,801: INFO: Using cache /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f\n",
      "2021-03-08 11:07:43,802: INFO: Loading /home/ytaille/data/cache/preprocess_training_data/579ec808c912b49f/output.pkl... \n",
      "2021-03-08 11:08:02,974: INFO: Quaero mentions: 16283\n",
      "2021-03-08 11:08:18,409: INFO: Using cache /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23\n",
      "2021-03-08 11:08:18,417: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23/history.yaml... \n",
      "2021-03-08 11:08:18,634: INFO: Loading /home/ytaille/data/cache/norm/paper/train_step1/34ef6f8317449a23/checkpoint-15.pt... \n",
      "2021-03-08 11:08:19,462: INFO: Model restored to its best state: 15\n"
     ]
    }
   ],
   "source": [
    "from AttentionSegmentation.trainer import Trainer\n",
    "\n",
    "from nlstruct.utils import  torch_global as tg\n",
    "\n",
    "trainer = Trainer.from_params(\n",
    "    model=model,\n",
    "    base_dir=serial_dir,\n",
    "    iterator=data_iterator,\n",
    "    train_data=instances_train,\n",
    "    validation_data=instances_val,\n",
    "    segmenter=segmenter,\n",
    "    params=config.pop(\"trainer\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIT FOR BOOSTING SURROUNDING ATTENTIONS\n",
    "\n",
    "# attn = torch.Tensor([[0,1,0,1,0], [0,0,1,0,0]])\n",
    "\n",
    "# attn_boosted = attn.clone()\n",
    "# nnz = (attn>0).nonzero().t().chunk(chunks=2,dim=0)\n",
    "\n",
    "# print(nnz)\n",
    "\n",
    "# new_nnz = [[], []]\n",
    "\n",
    "# for nz0, nz1 in zip(nnz[0][0].numpy(), nnz[1][0].numpy()):\n",
    "#     new_nnz[0].extend([nz0,nz0])\n",
    "#     new_nnz[1].extend([nz1-1,nz1+1])\n",
    "    \n",
    "# new_nnz[0] = torch.Tensor([new_nnz[0]]).long()\n",
    "# new_nnz[1] = torch.Tensor([new_nnz[1]]).long()\n",
    "# new_nnz = (new_nnz[0], new_nnz[1])\n",
    "# print(new_nnz)\n",
    "# attn_boosted[new_nnz] += 0.1\n",
    "\n",
    "# attn_boosted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE BIO BERT\n",
    "# TRAIN STEP 1 ONLY ON MEDIC LABELS (+ NCBI MENTIONS)\n",
    "# PREPROCESS / TRAIN / ATTEINDRE BONS SCORES\n",
    "# GET MEDIC ALTERNATIVE LABELS DANS NLSTRUCT -> TRADUIRE LABELS NCBI VERS MEDIC\n",
    "\n",
    "# USE ENTROPY INSTEAD OF CROSS ENTROPY -> not rely on labelled data only (rely on model certainty)\n",
    "\n",
    "# GROUPS : TYPE SEMANTIQUE À LA MENTION (pas utiliser)\n",
    "\n",
    "# NGRAMS FOR ENTITIES -> not possible with discontinued entities\n",
    "\n",
    "# Use \"separation token\" in phrases ?\n",
    "\n",
    "# Use a limited number of attention heads (not one per class)\n",
    "\n",
    "# Use same method as Perceval for trajectories (draw closest ones, reduce list, repeat) -> prédiction itérative\n",
    "\n",
    "# Maybe remove weakly supervised completely?\n",
    "\n",
    "# Test with Reinforce only after a few epochs\n",
    "\n",
    "# Facteur de représentation pour pondérer loss de Perceval ?\n",
    "\n",
    "# Plusieurs facteurs pour constituer la reward\n",
    "\n",
    "# Facteur de similarité mention extraite / synonyme plutôt que similarité mention / label ?\n",
    "\n",
    "# Make sure that every trajectory is different -> draw first then use Perceval\n",
    "\n",
    "# Métrique finale : Est-ce qu'on arrive à choper les CUI ? -> parce que frontières entités dures à déterminer \n",
    "\n",
    "# Use only one class ? -> simpler because all mentions are diseases -> MAKE SURE THAT SEVERAL MENTIONS ARE PREDICTABLE\n",
    "\n",
    "# maybe problem with reinforce algo comes from hyperparameters?? -> USE OPTIMIZER PARAMETER SPECIFICATION\n",
    "\n",
    "# change objective: instead of WL use RL metrics -> measure on CUI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE PERCEVAL WAY OF PREDICTING:\n",
    "\n",
    "# (Faire tirage attention ?)\n",
    "\n",
    "# Entrée: Embeddings tokens + embeddings labels \n",
    "\n",
    "# -> le masque d'attention sert à déterminer quels tokens fournir\n",
    "# Pour un CUI prédit: récupérer loss Perceval, comparer avec CUI le plus proche?\n",
    "# DEFT aide les attentions, mais on abandonne le masque d'attention?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERNet(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 n_labels,\n",
    "                 hidden_dim,\n",
    "                 dropout,\n",
    "                 n_tokens=None,\n",
    "                 token_dim=None,\n",
    "                 embeddings=None,\n",
    "                 tag_scheme=\"bio\",\n",
    "                 metric='linear',\n",
    "                 metric_fc_kwargs=None,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        if embeddings is not None:\n",
    "            self.embeddings = embeddings\n",
    "            if n_tokens is None or token_dim is None:\n",
    "                if hasattr(embeddings, 'weight'):\n",
    "                    n_tokens, token_dim = embeddings.weight.shape\n",
    "                else:\n",
    "                    n_tokens, token_dim = embeddings.embeddings.weight.shape\n",
    "        else:\n",
    "            self.embeddings = torch.nn.Embedding(n_tokens, token_dim) if n_tokens > 0 else None\n",
    "        assert token_dim is not None, \"Provide token_dim or embeddings\"\n",
    "        assert self.embeddings is not None\n",
    "\n",
    "        dim = (token_dim if n_tokens > 0 else 0)\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        if tag_scheme == \"bio\":\n",
    "            self.crf = BIODecoder(n_labels)\n",
    "        elif tag_scheme == \"bioul\":\n",
    "            self.crf = BIOULDecoder(n_labels)\n",
    "        else:\n",
    "            raise Exception()\n",
    "        if hidden_dim is None:\n",
    "            hidden_dim = dim\n",
    "        self.linear = torch.nn.Linear(dim, hidden_dim)\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(dim)\n",
    "\n",
    "        n_tags = self.crf.num_tags\n",
    "        metric_fc_kwargs = metric_fc_kwargs if metric_fc_kwargs is not None else {}\n",
    "        if metric == \"linear\":\n",
    "            self.metric_fc = torch.nn.Linear(dim, n_tags)\n",
    "        elif metric == \"cosine\":\n",
    "            self.metric_fc = CosineSimilarity(dim, n_tags, rescale=rescale, **metric_fc_kwargs)\n",
    "        elif metric == \"ema_cosine\":\n",
    "            self.metric_fc = EMACosineSimilarity(dim, n_tags, rescale=rescale, **metric_fc_kwargs)\n",
    "        else:\n",
    "            raise Exception()\n",
    "    \n",
    "    def extended_embeddings(self, tokens, mask, **kwargs):\n",
    "        # Default case here, size <= 512\n",
    "        # Small ugly check to see if self.embeddings is Bert-like, then we need to pass a mask\n",
    "        if hasattr(self.embeddings, 'encoder') or hasattr(self.embeddings, 'transformer'):\n",
    "            return self.embeddings(tokens, mask, **kwargs)[0]\n",
    "        else:\n",
    "            return self.embeddings(tokens)\n",
    "\n",
    "    def forward(self, tokens, mask, tag_embeds=None, return_embeddings=False):\n",
    "        # Embed the tokens\n",
    "        scores = None\n",
    "        # shape: n_batch * sequence * 768\n",
    "        embeds = self.extended_embeddings(tokens, mask, custom_embeds=tag_embeds)\n",
    "        state = embeds.masked_fill(~mask.unsqueeze(-1), 0)\n",
    "        state = torch.relu(self.linear(self.dropout(state)))# + state\n",
    "        state = self.batch_norm(state.view(-1, state.shape[-1])).view(state.shape)\n",
    "        scores = self.metric_fc(state)\n",
    "        return {\n",
    "            \"scores\": scores,\n",
    "            \"embeddings\": embeds if return_embeddings else None,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import deque\n",
    "import time\n",
    "import re\n",
    "import datetime\n",
    "import traceback\n",
    "import numpy as np\n",
    "from typing import Dict, Optional, List, Tuple, Union, Iterable, Any, Set\n",
    "import pdb\n",
    "\n",
    "import torch\n",
    "import torch.optim.lr_scheduler\n",
    "from torch.nn.parallel import replicate, parallel_apply\n",
    "from torch.nn.parallel.scatter_gather import scatter_kwargs, gather\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from itertools import tee\n",
    "\n",
    "from allennlp.common import Params\n",
    "from allennlp.common.checks import ConfigurationError\n",
    "from allennlp.common.util import peak_memory_mb, gpu_memory_mb\n",
    "from allennlp.common.tqdm import Tqdm\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.iterators.data_iterator import DataIterator\n",
    "from allennlp.models.model import Model\n",
    "from allennlp.nn import util\n",
    "from allennlp.training.learning_rate_schedulers import LearningRateScheduler\n",
    "from allennlp.training.optimizers import Optimizer\n",
    "\n",
    "from AttentionSegmentation.commons.trainer_utils import is_sparse,\\\n",
    "    sparse_clip_norm, move_optimizer_to_cuda, TensorboardWriter\n",
    "# from AttentionSegmentation.visualization.visualize_attns \\\n",
    "#     import html_visualizer\n",
    "from AttentionSegmentation.model.attn2labels import BasePredictionClass\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "TQDM_COLUMNS = 200\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0,'/home/ytaille/deep_multilingual_normalization')\n",
    "from create_classifiers import create_classifiers\n",
    "from nlstruct.dataloaders import load_from_brat\n",
    "\n",
    "logger2 = logging.getLogger(\"nlstruct\")\n",
    "logger2.setLevel(logging.ERROR)\n",
    "\n",
    "from notebook_utils import *\n",
    "\n",
    "def _train_epoch(self, epoch: int) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Trains one epoch and returns metrics.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Peak CPU memory usage MB: {peak_memory_mb()}\")\n",
    "        if torch.cuda.is_available():\n",
    "            for gpu, memory in gpu_memory_mb().items():\n",
    "                logger.info(f\"GPU {gpu} memory usage MB: {memory}\")\n",
    "\n",
    "        train_loss = 0.0\n",
    "\n",
    "        from allennlp.data.fields.array_field import ArrayField\n",
    "\n",
    "        for i, td in enumerate(self._train_data):\n",
    "            td.fields['sample_id'] = ArrayField(np.array([i]))\n",
    "\n",
    "        # Get tqdm for the training batches\n",
    "        train_generator = self._iterator(self._train_data,\n",
    "                                         num_epochs=1,\n",
    "                                         cuda_device=self._iterator_device,\n",
    "                                         shuffle=True,\n",
    "                                         )\n",
    "\n",
    "        train_generator, cp_generator, id_generator = tee(train_generator, 3)\n",
    "\n",
    "        ids = []\n",
    "\n",
    "        for ig in id_generator:\n",
    "            ids.extend([int(sid.item()) for sid in ig['sample_id']])\n",
    "\n",
    "        shuffled_train_data = [self._train_data[i] for i in ids]\n",
    "\n",
    "#         train_predictions = self._segmenter.get_predictions(\n",
    "#                     instances=shuffled_train_data,\n",
    "#                     iterator = cp_generator,\n",
    "#                     model=self._model,\n",
    "#                     cuda_device=self._iterator_device,\n",
    "#                     verbose=True)\n",
    "\n",
    "\n",
    "        num_training_batches = self._iterator.get_num_batches(self._train_data)\n",
    "        train_generator_tqdm = Tqdm.tqdm(train_generator,\n",
    "                                         total=num_training_batches\n",
    "                                         )\n",
    "        self._last_log = time.time()\n",
    "        last_save_time = time.time()\n",
    "\n",
    "        batches_this_epoch = 0\n",
    "        if self._batch_num_total is None:\n",
    "            self._batch_num_total = 0\n",
    "\n",
    "        cpt_batch = 0\n",
    "\n",
    "        # Set the model to \"train\" mode.\n",
    "        self._model.train()\n",
    "\n",
    "        for batch in train_generator_tqdm:\n",
    "            \n",
    "            batches_this_epoch += 1\n",
    "            self._batch_num_total += 1\n",
    "            batch_num_total = self._batch_num_total\n",
    "            batch_len = len(batch['labels'])\n",
    "\n",
    "            # FOR train_predictions:\n",
    "            # pred/gold is sentence level\n",
    "            # pred_labels/gold_labels is word level\n",
    "\n",
    "\n",
    "            # FOR batch:\n",
    "            # labels is sentence level\n",
    "            # tags is word level\n",
    "\n",
    "            # print(train_texts)\n",
    "            # print(\"SENTENCE LEVEL\")\n",
    "            # print([tp['gold'] for tp in train_predictions[:10]])\n",
    "            # print(batch['labels'][:10])\n",
    "\n",
    "            # print(\"WORD LEVEL\")\n",
    "            # print([tp['gold_labels'] for tp in train_predictions[:2]])\n",
    "            # print(batch['tags'][:2])\n",
    "\n",
    "            # exit()\n",
    "            \n",
    "            if epoch <= -1:\n",
    "                trajectory_scores =  [0]\n",
    "            else:\n",
    "                output_dict = self._model(**batch)\n",
    "                \n",
    "#                 attns_single = output_dict['attentions']\n",
    "                attns = output_dict['attentions']\n",
    "                \n",
    "                # Policy is \"attention mask\": attention scores should be higher if we want to predict CUI\n",
    "                # Only take words with attention above threshold when predicting with deep norm -> see if it's enough (reward indicates that)\n",
    "                # REINFORCE algo: (also known as Monte Carlo PG)\n",
    "                # - draw N trajectories (N attention paths?) -> discretise attentions to make them 1 / 0? -> see if it works with bernoulli first\n",
    "                # - evaluate each trajectory then sum (maybe add baseline -> subtract mean of all trajectories rewards)\n",
    "                # - Expected return is given by sum(prob(Ti | W) * reward(Ti)) -> see again if it works with bernoulli first\n",
    "                # W are WeakL weights \n",
    "                # - Gradient ascent of return / gradient descent of negative return\n",
    "\n",
    "                # Set horizon ? -> number / proportion of attention at 1 per batch\n",
    "                # Set number of trajectories ? -> maybe make trajectories number vary based on sentence length\n",
    "                # gamma = 0.9 ? -> used to simulate temporal importance of reward (multiply each step by a certain power of gamma, furthest rewards are less impactful) -> may not be possible to model here\n",
    "                \n",
    "                horizon = 0.2\n",
    "                n_trajectories = 10\n",
    "                gamma = 0.9\n",
    "                attn_threshold = 0.01\n",
    "\n",
    "                mask = batch['tokens']['mask']\n",
    "        \n",
    "                prob_attn = attns\n",
    "                from torch.distributions import Binomial\n",
    "\n",
    "                m = Binomial(probs=prob_attn)\n",
    "                trajectory_scores = []#{i: [] for i in range(prob_attn.shape[-1])}\n",
    "                \n",
    "                real_tokens = [np.array(b.fields['tokens'].tokens) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "#                     gold_labels = [np.array(b.fields['tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "                gold_norm_labels = [np.array(b.fields['chunk_tags'].labels) for b in shuffled_train_data[cpt_batch:cpt_batch+batch_len]]\n",
    "\n",
    "                policy_loss = []\n",
    "                \n",
    "                all_samples = []\n",
    "    \n",
    "                for nb_traj in range(n_trajectories):\n",
    "                    attn_sample = m.sample()\n",
    "                    \n",
    "                    all_samples.append(attn_sample)\n",
    "                    \n",
    "#                     for class_n in range(attn_sample.shape[-1]):\n",
    "                        \n",
    "#                     masked_tokens = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]\n",
    "#                                      if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]] \n",
    "#                                      for w_id, rt in enumerate(real_tokens)] # weird behaviour for len == 1\n",
    "# #                     masked_gold = [rt[attn_mask[w_id,:len(rt)].cpu().to(bool)] for w_id, rt in enumerate(gold_labels)]\n",
    "#                     masked_gold_norm = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)] \n",
    "#                                         if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]]\n",
    "#                                         for w_id, rt in enumerate(gold_norm_labels)]\n",
    "                try:\n",
    "                    masked_tokens = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]\n",
    "                                     if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]] \n",
    "                                     for w_id, rt in enumerate(real_tokens) \n",
    "                                     for class_n in range(attn_sample.shape[-1])\n",
    "                                     for attn_mask in all_samples\n",
    "                                    ] # weird behaviour for len == 1\n",
    "    #                     masked_gold = [rt[attn_mask[w_id,:len(rt)].cpu().to(bool)] for w_id, rt in enumerate(gold_labels)]\n",
    "                    masked_gold_norm = [rt[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)] \n",
    "                                        if len(rt) > 1 else rt[[attn_mask[w_id,:len(rt),class_n].cpu().to(bool)]]\n",
    "                                        for w_id, rt in enumerate(gold_norm_labels) \n",
    "                                        for class_n in range(attn_sample.shape[-1])\n",
    "                                        for attn_mask in all_samples\n",
    "                                       ]\n",
    "                except:\n",
    "                    # IGNORE FOR NOW BUT ERROR SOMETIMES\n",
    "                    pass\n",
    "\n",
    "                save_to_ann(masked_tokens, masked_gold_norm, '/home/ytaille/data/tmp/ws_inputs/')\n",
    "\n",
    "                # NLSTRUCT PART\n",
    "\n",
    "                bert_name = \"bert-base-multilingual-uncased\"\n",
    "\n",
    "                dataset = load_from_brat(\"/home/ytaille/data/tmp/ws_inputs/\")\n",
    "\n",
    "                if len(dataset['mentions']) == 0:\n",
    "                    continue\n",
    "\n",
    "                dataset['mentions']['mention_id'] = dataset['mentions']['doc_id'] +'.'+ dataset['mentions']['mention_id'].astype(str)\n",
    "\n",
    "                batcher, vocs, mention_ids = preprocess_train(\n",
    "                    dataset,\n",
    "                    vocabularies=self.vocabularies1,\n",
    "                    bert_name=bert_name,\n",
    "                )\n",
    "\n",
    "                batch_size = len(batcher)\n",
    "                with_tqdm = True\n",
    "\n",
    "                tg.set_device('cuda:0') #('cuda:0')\n",
    "                device = tg.device\n",
    "\n",
    "                pred_batcher = predict(batcher, self.classifier1, batch_size=64)\n",
    "                \n",
    "                scores = compute_scores(pred_batcher, batcher)\n",
    "\n",
    "                try:\n",
    "                    trajectory_scores.append((scores['loss'] * prob_attn).mean())\n",
    "                except:\n",
    "                    print(trajectory_scores)\n",
    "                    raise\n",
    "\n",
    "                cpt_batch += batch_len\n",
    "\n",
    "#                 if any(len(tj) > 0 for tj in trajectory_scores.values()):\n",
    "#                     trajectory_scores = [t for tj in trajectory_scores.values() for t in tj]\n",
    "#                 else: policy_loss = 0\n",
    "\n",
    "            self._optimizer.zero_grad()\n",
    "            loss = sum(trajectory_scores) # policy_loss self._batch_loss(batch, for_training=True) + \n",
    "            loss.backward()\n",
    "\n",
    "            # Make sure Variable is on the cpu before converting to numpy.\n",
    "            # .cpu() is a no-op if you aren't using GPUs.\n",
    "            train_loss += loss.data.cpu().numpy()\n",
    "            batch_grad_norm = self._rescale_gradients()\n",
    "\n",
    "            # This does nothing if batch_num_total is None or you are using an\n",
    "            # LRScheduler which doesn't update per batch.\n",
    "            if self._learning_rate_scheduler:\n",
    "                self._learning_rate_scheduler.step_batch(batch_num_total)\n",
    "                \n",
    "            self._optimizer.step()\n",
    "\n",
    "            # Update the description with the latest metrics\n",
    "            metrics = self._get_metrics(train_loss, batches_this_epoch)\n",
    "            description = self._description_from_metrics(metrics)\n",
    "\n",
    "            train_generator_tqdm.set_description(description, refresh=False)\n",
    "            if hasattr(self, \"_tf_params\") and self._tf_params is not None:\n",
    "                # We have TF logging\n",
    "                if self._batch_num_total % self._tf_params[\"log_every\"] == 0:\n",
    "                    self._tf_log(metrics, self._batch_num_total)\n",
    "\n",
    "        return self._get_metrics(train_loss, batches_this_epoch, reset=True)\n",
    "    \n",
    "import functools\n",
    "\n",
    "trainer._train_epoch = functools.partial(_train_epoch, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:08:20,564: INFO: Beginning training.\n",
      "2021-03-08 11:08:20,565: INFO: ==================================================\n",
      "2021-03-08 11:08:20,567: INFO: Starting Training Epoch 1/50\n",
      "2021-03-08 11:08:20,569: INFO: Peak CPU memory usage MB: 6870.372\n",
      "2021-03-08 11:08:20,674: INFO: GPU 0 memory usage MB: 5750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "CompositeMention: 0.0479, DiseaseClass: 0.1958, Modifier: 0.4208, SpecificDisease: 0.5646, accuracy: 0.3073, loss: 0.1034 ||: 100%|██████████| 57/57 [02:16<00:00,  2.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:10:38,805: INFO: Starting with Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "CompositeMention: 0.0878, DiseaseClass: 0.2382, Modifier: 0.3981, SpecificDisease: 0.5611, accuracy: 0.3213, loss: 0.7400 ||: 100%|██████████| 10/10 [00:01<00:00,  6.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:10:40,289: INFO: Validation done. (350.0 / 1276) zero predicted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:10:48,580: INFO: Best validation performance so far. Copying weights to './trained_models/NCBI-BERT-realFT-PS/run-180/models/best.th'.\n",
      "2021-03-08 11:10:55,462: INFO: Metrics:\n",
      "                        Training DiseaseClass      : 0.241  Validation DiseaseClass      : 0.238\n",
      "                        Training CompositeMention  : 0.052  Validation CompositeMention  : 0.088\n",
      "                        Training SpecificDisease   : 0.631  Validation SpecificDisease   : 0.561\n",
      "                        Training accuracy          : 0.332  Validation accuracy          : 0.321\n",
      "                        Training Modifier          : 0.404  Validation Modifier          : 0.398\n",
      "                        Training loss              : 0.027  Validation loss              : 0.740\n",
      "\n",
      "2021-03-08 11:10:55,463: INFO: Writing validation visualization at ./trained_models/NCBI-BERT-realFT-PS/run-180/visualization/validation.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:02<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:10:58,779: INFO: Tag: CompositeMention, Acc: 8.78\n",
      "2021-03-08 11:10:58,781: INFO: Tag: DiseaseClass, Acc: 23.82\n",
      "2021-03-08 11:10:58,783: INFO: Tag: Modifier, Acc: 39.81\n",
      "2021-03-08 11:10:58,784: INFO: Tag: SpecificDisease, Acc: 56.11\n",
      "2021-03-08 11:10:58,785: INFO: Average ACC: 32.13\n",
      "2021-03-08 11:10:58,834: INFO: processed 22501 tokens with 119 phrases; \n",
      "2021-03-08 11:10:58,835: INFO: found: 756 phrases; correct: 0.\n",
      "\n",
      "2021-03-08 11:10:58,836: INFO: accuracy:  92.78%; \n",
      "2021-03-08 11:10:58,837: INFO: precision:   0.00%; recall:   0.00%; FB1:   0.00\n",
      "2021-03-08 11:10:58,838: INFO:               TAG  precision   recall      FB1\n",
      "2021-03-08 11:10:58,839: INFO:  CompositeMention      0.00%    0.00%    0.00%\n",
      "2021-03-08 11:10:58,840: INFO:      DiseaseClass      0.00%    0.00%    0.00%\n",
      "2021-03-08 11:10:58,841: INFO:          Modifier      0.00%    0.00%    0.00%\n",
      "2021-03-08 11:10:58,842: INFO:   SpecificDisease      0.00%    0.00%    0.00%\n",
      "2021-03-08 11:10:58,845: INFO: Writing predictions to ./trained_models/NCBI-BERT-realFT-PS/run-180/predictions.json\n",
      "2021-03-08 11:10:58,848: INFO: Reducing LR: 2.00e-07 -> 2.00e-07\n",
      "2021-03-08 11:10:58,849: INFO: Epoch duration: 00:02:38\n",
      "2021-03-08 11:10:58,850: INFO: Estimated training time remaining: 02:09:15\n",
      "2021-03-08 11:10:58,850: INFO: ==================================================\n",
      "2021-03-08 11:10:58,851: INFO: ==================================================\n",
      "2021-03-08 11:10:58,851: INFO: Starting Training Epoch 2/50\n",
      "2021-03-08 11:10:58,852: INFO: Peak CPU memory usage MB: 6870.372\n",
      "2021-03-08 11:10:58,950: INFO: GPU 0 memory usage MB: 10768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:603: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "/home/ytaille/AttentionSegmentation/allennlp/data/fields/array_field.py:42: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return_array[slices] = self.array\n",
      "CompositeMention: 0.0802, DiseaseClass: 0.2290, Modifier: 0.3953, SpecificDisease: 0.6027, accuracy: 0.3268, loss: 0.0932 ||:  11%|█         | 6/57 [00:34<04:55,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-08 11:11:34,707: ERROR: Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-12-3435b262f1ae>\", line 1, in <module>\n",
      "    trainer.train()\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/trainer.py\", line 61, in train\n",
      "    super(Trainer, self).train(*args, **kwargs)\n",
      "  File \"/home/ytaille/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\", line 588, in train\n",
      "    train_metrics = self._train_epoch(epoch)\n",
      "  File \"<ipython-input-11-6104dadf68b1>\", line 226, in _train_epoch\n",
      "    bert_name=bert_name,\n",
      "  File \"/home/ytaille/AttentionSegmentation/notebook_utils.py\", line 513, in preprocess_train\n",
      "    tokenizer = AutoTokenizer.from_pretrained(bert_name)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_auto.py\", line 189, in from_pretrained\n",
      "    config = AutoConfig.from_pretrained(pretrained_model_name_or_path, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/configuration_auto.py\", line 192, in from_pretrained\n",
      "    pretrained_model_name_or_path, pretrained_config_archive_map=ALL_PRETRAINED_CONFIG_ARCHIVE_MAP, **kwargs\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/configuration_utils.py\", line 249, in get_config_dict\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/file_utils.py\", line 267, in cached_path\n",
      "    local_files_only=local_files_only,\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/file_utils.py\", line 374, in get_from_cache\n",
      "    response = requests.head(url, allow_redirects=True, proxies=proxies, timeout=etag_timeout)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/api.py\", line 104, in head\n",
      "    return request('head', url, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/api.py\", line 61, in request\n",
      "    return session.request(method=method, url=url, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/sessions.py\", line 542, in request\n",
      "    resp = self.send(prep, **send_kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/sessions.py\", line 655, in send\n",
      "    r = adapter.send(request, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/adapters.py\", line 449, in send\n",
      "    timeout=timeout\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 706, in urlopen\n",
      "    chunked=chunked,\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 382, in _make_request\n",
      "    self._validate_conn(conn)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\", line 1010, in _validate_conn\n",
      "    conn.connect()\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connection.py\", line 353, in connect\n",
      "    conn = self._new_conn()\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connection.py\", line 170, in _new_conn\n",
      "    (self._dns_host, self.port), self.timeout, **extra_kw\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/util/connection.py\", line 86, in create_connection\n",
      "    sock.connect(sa)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/ytaille/.conda/envs/deep_multilingual_normalization/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "2021-03-08 11:11:34,734: INFO: \n",
      "Unfortunately, your original traceback can not be constructed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/AttentionSegmentation/AttentionSegmentation/commons/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, continue_training)\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mepoch_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             \u001b[0mtrain_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-6104dadf68b1>\u001b[0m in \u001b[0;36m_train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0mvocabularies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabularies1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m                     \u001b[0mbert_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m                 )\n",
      "\u001b[0;32m~/AttentionSegmentation/notebook_utils.py\u001b[0m in \u001b[0;36mpreprocess_train\u001b[0;34m(dataset, bert_name, vocabularies, max_length, apply_unidecode, prepend_labels)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[0;31m# Tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhuggingface_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmentions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_token_spans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munidecode_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         config_dict, _ = PretrainedConfig.get_config_dict(\n\u001b[0;32m--> 192\u001b[0;31m             \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_config_archive_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mALL_PRETRAINED_CONFIG_ARCHIVE_MAP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         )\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, pretrained_config_archive_map, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mresume_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, local_files_only)\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m             \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         )\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 374\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproxies\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0metag_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mhead\u001b[0;34m(url, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'head'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1009\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    169\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             )\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/deep_multilingual_normalization/lib/python3.6/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(\"Training Done.\")\n",
    "if instances_test is not None:\n",
    "    logger.info(\"Computing final Test Accuracy\")\n",
    "    trainer.test(instances_test)\n",
    "logger.info(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_mult_norm",
   "language": "python",
   "name": "deep_mult_norm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
